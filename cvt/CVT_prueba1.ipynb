{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate, Lambda, Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Conv2D, MaxPooling2D,\n",
    "    AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization, ReLU, Input\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import optimizers, regularizers, backend as K\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report,\n",
    "    cohen_kappa_score, hamming_loss, log_loss, zero_one_loss, matthews_corrcoef, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from time import time\n",
    "import time as tm\n",
    "import datetime\n",
    "from skimage.util.shape import view_as_blocks\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import ntpath\n",
    "import copy\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tfkan.layers import DenseKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters and Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('../SRM_Kernels.npy') \n",
    "biasSRM=np.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################\n",
    "def thtanh(x,t):\n",
    "    th=K.tanh(x)*t\n",
    "    return th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-UNIWARD BOSSbase 1.01 PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 256, 256, 1)\n",
      "(12000, 2)\n",
      "(4000, 256, 256, 1)\n",
      "(4000, 2)\n",
      "(4000, 256, 256, 1)\n",
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "X_train = np.load('../data_gbras/X_train.npy')\n",
    "y_train = np.load('../data_gbras/y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load('../data_gbras/X_valid.npy')\n",
    "y_valid = np.load('../data_gbras/y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load('../data_gbras/X_test.npy')\n",
    "y_test = np.load('../data_gbras/y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions arquitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excitation_layer(input_layer, out_dim, ratio, conv):\n",
    "  squeeze = tf.keras.layers.GlobalAveragePooling2D()(input_layer)\n",
    "  excitation = tf.keras.layers.Dense(units=out_dim / ratio, activation='relu')(squeeze)\n",
    "  excitation = tf.keras.layers.Dense(out_dim,activation='sigmoid')(excitation)\n",
    "  excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "  scale = tf.keras.layers.multiply([input_layer, excitation])\n",
    "  if conv:\n",
    "    shortcut = tf.keras.layers.Conv2D(out_dim,kernel_size=1,strides=1,\n",
    "                                      padding='same',kernel_initializer='he_normal')(input_layer)\n",
    "    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "  else:\n",
    "    shortcut = input_layer\n",
    "  out = tf.keras.layers.add([shortcut, scale])\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "def sreLu (input):\n",
    "  return ReLU(negative_slope=0.1, threshold=0)(input)\n",
    "\n",
    "def sConv(input,parameters,size,nstrides):\n",
    "  return Conv2D(parameters, (size,size), strides=(nstrides,nstrides),padding=\"same\", kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(input)\n",
    "\n",
    "def sBN (input):\n",
    "  return tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=True, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(input)\n",
    "\n",
    "def sGlobal_Avg_Pooling (input):\n",
    "  return tf.keras.layers.GlobalAveragePooling2D()(input)\n",
    "\n",
    "def sDense (input, n_units, activate_c):\n",
    "  return tf.keras.layers.Dense(n_units,activation=activate_c)(input)\n",
    "\n",
    "def smultiply (input_1, input_2):\n",
    "  return tf.keras.layers.multiply([input_1, input_2])\n",
    "\n",
    "def sadd (input_1, input_2):\n",
    "  return tf.keras.layers.add([input_1, input_2])\n",
    "\n",
    "# Initial learning rate for the optimizer\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "# Learning rate schedule with exponential decay\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Block_1 (input, parameter):\n",
    "  output = sConv(input, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  output = sreLu(output)\n",
    "  return output\n",
    "  \n",
    "\n",
    "\n",
    "def SE_Block(input, out_dim, ratio):\n",
    "  output = sGlobal_Avg_Pooling(input)\n",
    "  output = sDense(output, out_dim/ratio, 'relu')\n",
    "  output = sDense(output, out_dim, 'sigmoid')\n",
    "  return output\n",
    "  \n",
    "  \n",
    "  \n",
    "def Block_2 (input, parameter):\n",
    "  output = Block_1(input, parameter)\n",
    "  output = sConv(output, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  multiplier = SE_Block(output,  parameter, parameter)\n",
    "  # output = smultiply(output, output)\n",
    "  output = smultiply(multiplier, output)\n",
    "  output = sadd(output, input)\n",
    "  return output\n",
    "  \n",
    "  \n",
    "\n",
    "def Block_3 (input, parameter):\n",
    "  addition = sConv(input, parameter, 1, 2)\n",
    "  addition = sBN(addition)\n",
    "  output = sConv(input, parameter, 3, 2)\n",
    "  output = sBN(output)\n",
    "  output = sreLu(output)\n",
    "  output = sConv(output, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  multiplier = SE_Block(output,  parameter, parameter)\n",
    "  output = smultiply(multiplier, output)\n",
    "  output = sadd(output, addition)\n",
    "  return output  \n",
    "  \n",
    "def Block_4 (input, parameter):\n",
    "  output = Block_1(input, parameter)\n",
    "  output = sConv(input, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  \n",
    "  return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS_1 = 1e-6\n",
    "PROJECTION_DIM_1 = 16\n",
    "NUM_HEADS_1 = 4\n",
    "NUM_LAYERS_1 = 4\n",
    "MLP_UNITS_1 = [\n",
    "    PROJECTION_DIM_1 * 2,\n",
    "    PROJECTION_DIM_1,\n",
    "]\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE_2 = 1e-3\n",
    "WEIGHT_DECAY_2 = 1e-4\n",
    "\n",
    "IMAGE_SIZE_2 =  16# We will resize input images to this size.\n",
    "PATCH_SIZE_2 = 4  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES_2 = (IMAGE_SIZE_2 // PATCH_SIZE_2) ** 2\n",
    "print(NUM_PATCHES_2)\n",
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS_2 = 1e-6\n",
    "PROJECTION_DIM_2 = 128\n",
    "NUM_HEADS_2 = 4\n",
    "NUM_LAYERS_2 = 4\n",
    "MLP_UNITS_2 = [\n",
    "    PROJECTION_DIM_2 * 2,\n",
    "    PROJECTION_DIM_2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(projected_patches, num_patches=NUM_PATCHES_2, projection_dim=PROJECTION_DIM_2):\n",
    "    # Build the positions: create a range of position indices from 0 to num_patches - 1\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "\n",
    "    # Encode the positions with an Embedding layer\n",
    "    encoded_positions = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim\n",
    "    )(positions)\n",
    "\n",
    "    # Add encoded positions to the projected patches\n",
    "    return projected_patches + encoded_positions\n",
    "\n",
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and add Dense => Dropout layers\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)  # Dense layer with GELU activation\n",
    "        x = layers.Dropout(dropout_rate)(x)  # Dropout layer\n",
    "    return x\n",
    "\n",
    "def transformer_2(encoded_patches):\n",
    "    # Apply layer normalization\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(encoded_patches)\n",
    "\n",
    "    # Multi-Head Self Attention layer\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS_2, key_dim=PROJECTION_DIM_2, dropout=0.1\n",
    "    )(x1, x1)\n",
    "\n",
    "    # Skip connection\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Apply layer normalization again\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(x2)\n",
    "\n",
    "    # Apply MLP layer\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS_2, dropout_rate=0.1)\n",
    "\n",
    "    # Second skip connection\n",
    "    encoded_patches = layers.Add()([x4, x2])\n",
    "    return encoded_patches\n",
    "\n",
    "def Transform_sh_2(inputs):\n",
    "    # Apply squeeze and excitation layer\n",
    "    inputs1 = squeeze_excitation_layer(inputs, out_dim=512, ratio=32.0, conv=False)\n",
    "    print(inputs1.shape)\n",
    "\n",
    "    # Project input patches using a Conv2D layer\n",
    "    projected_patches = layers.Conv2D(\n",
    "        filters=PROJECTION_DIM_2,\n",
    "        kernel_size=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
    "        strides=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
    "        padding=\"VALID\",\n",
    "    )(inputs1)\n",
    "\n",
    "    # Get the shape of the projected patches\n",
    "    _, h, w, c = projected_patches.shape\n",
    "\n",
    "    # Reshape the projected patches\n",
    "    projected_patches = layers.Reshape((h * w, c))(projected_patches)  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Add positional embeddings to the projected patches\n",
    "    encoded_patches = position_embedding(projected_patches)  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Apply dropout\n",
    "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
    "\n",
    "    # Iterate over the number of layers and stack transformer blocks\n",
    "    for i in range(NUM_LAYERS_2):\n",
    "        # Add a transformer block\n",
    "        encoded_patches = transformer_2(encoded_patches)\n",
    "\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch = 0, model_name=\"\", num_test=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=\"D:/testing_\"+num_test+\"/\"+model_name+\"_\"+\"{}\".format(time())\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "    filepath = log_dir+\"/saved-model.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath, \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    model.reset_states()\n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                        callbacks=[tensorboard,  checkpoint], \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        initial_epoch=initial_epoch)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results_dir=\"D:/testing_\"+num_test+\"/\"+model_name+\"/\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "      \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVT-h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output last layer before transformer (None, 16, 16, 512)\n",
      "(None, 16, 16, 512)\n",
      "Transformer_create\n"
     ]
    }
   ],
   "source": [
    "def new_arch():\n",
    "  tf.keras.backend.clear_session()\n",
    "  inputs = tf.keras.Input(shape=(256,256,1), name=\"input_1\")\n",
    "  #Layer 1\n",
    "  layers_ty = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "  layers_tn = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=True, activation=Tanh3, use_bias=True)(inputs)\n",
    "\n",
    "  layers = tf.keras.layers.add([layers_ty, layers_tn])\n",
    "  layers1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "  #Layer 2\n",
    "  \n",
    "  # L1\n",
    "  layers = Block_1(layers1,64)\n",
    "\n",
    "  # L2\n",
    "  layers = Block_1(layers,64)\n",
    "\n",
    "  # L3 - L7\n",
    "  for i in range(5):\n",
    "    layers = Block_2(layers,64)\n",
    "\n",
    "  # L8 - L11\n",
    "  for i in [64, 64, 128, 256]:\n",
    "    layers = Block_3(layers,i)\n",
    "\n",
    "  # L12\n",
    "  layers = Block_4(layers,512)\n",
    "  print('output last layer before transformer', layers.shape)\n",
    "  #CVT=Transform_sh_1(layers)\n",
    "  #CVT_2=Transform_sh_1(CVT)\n",
    "  CVT1=Transform_sh_2(layers)\n",
    "\n",
    "  representation = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(CVT1)\n",
    "  representation = tf.keras.layers.GlobalAvgPool1D()(representation)\n",
    "  #---------------------------------------------------Fin de Transformer 2------------------------------------------------------------------------#\n",
    "  # Classify outputs.\n",
    "      #FC\n",
    "  layers = Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(representation)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "  layers = Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "  layers = Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "\n",
    "  #Softmax\n",
    "  predictions = Dense(2, activation=\"softmax\", name=\"output_1\",kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  model =tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    " \n",
    "  # Compile the model if the compile flag is set\n",
    "\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.95)\n",
    "  if compile:\n",
    "      model.compile(optimizer= optimizer,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "      \n",
    "      print (\"Transformer_create\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "model = new_arch()  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.4864\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50225, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 719s 471ms/step - loss: 0.9567 - accuracy: 0.4864 - val_loss: 0.9519 - val_accuracy: 0.5023\n",
      "Epoch 2/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9508 - accuracy: 0.4990\n",
      "Epoch 2: val_accuracy improved from 0.50225 to 0.53375, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 725s 483ms/step - loss: 0.9508 - accuracy: 0.4990 - val_loss: 0.9438 - val_accuracy: 0.5337\n",
      "Epoch 3/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.5309\n",
      "Epoch 3: val_accuracy improved from 0.53375 to 0.55350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 728s 485ms/step - loss: 0.9437 - accuracy: 0.5309 - val_loss: 0.9344 - val_accuracy: 0.5535\n",
      "Epoch 4/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9332 - accuracy: 0.5458\n",
      "Epoch 4: val_accuracy improved from 0.55350 to 0.63100, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 728s 486ms/step - loss: 0.9332 - accuracy: 0.5458 - val_loss: 0.8824 - val_accuracy: 0.6310\n",
      "Epoch 5/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9192 - accuracy: 0.5634\n",
      "Epoch 5: val_accuracy improved from 0.63100 to 0.66850, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 727s 485ms/step - loss: 0.9192 - accuracy: 0.5634 - val_loss: 0.8367 - val_accuracy: 0.6685\n",
      "Epoch 6/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.5885\n",
      "Epoch 6: val_accuracy improved from 0.66850 to 0.67175, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.9045 - accuracy: 0.5885 - val_loss: 0.8318 - val_accuracy: 0.6718\n",
      "Epoch 7/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.5980\n",
      "Epoch 7: val_accuracy improved from 0.67175 to 0.70350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.8882 - accuracy: 0.5980 - val_loss: 0.7703 - val_accuracy: 0.7035\n",
      "Epoch 8/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8762 - accuracy: 0.6071\n",
      "Epoch 8: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.8762 - accuracy: 0.6071 - val_loss: 0.7814 - val_accuracy: 0.6930\n",
      "Epoch 9/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.6203\n",
      "Epoch 9: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 733s 488ms/step - loss: 0.8621 - accuracy: 0.6203 - val_loss: 0.7741 - val_accuracy: 0.6995\n",
      "Epoch 10/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8524 - accuracy: 0.6285\n",
      "Epoch 10: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.8524 - accuracy: 0.6285 - val_loss: 0.7664 - val_accuracy: 0.6980\n",
      "Epoch 11/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.6296\n",
      "Epoch 11: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.8390 - accuracy: 0.6296 - val_loss: 0.7767 - val_accuracy: 0.6902\n",
      "Epoch 12/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.6398\n",
      "Epoch 12: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.8359 - accuracy: 0.6398 - val_loss: 0.7615 - val_accuracy: 0.7020\n",
      "Epoch 13/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.6423\n",
      "Epoch 13: val_accuracy improved from 0.70350 to 0.71125, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.8291 - accuracy: 0.6423 - val_loss: 0.7526 - val_accuracy: 0.7113\n",
      "Epoch 14/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.6506\n",
      "Epoch 14: val_accuracy improved from 0.71125 to 0.75000, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 731s 487ms/step - loss: 0.8117 - accuracy: 0.6506 - val_loss: 0.6696 - val_accuracy: 0.7500\n",
      "Epoch 15/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8033 - accuracy: 0.6561\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 745s 496ms/step - loss: 0.8033 - accuracy: 0.6561 - val_loss: 0.7872 - val_accuracy: 0.7290\n",
      "Epoch 16/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.6603\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.8005 - accuracy: 0.6603 - val_loss: 0.7240 - val_accuracy: 0.7212\n",
      "Epoch 17/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7895 - accuracy: 0.6629\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.7895 - accuracy: 0.6629 - val_loss: 0.7291 - val_accuracy: 0.7225\n",
      "Epoch 18/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7788 - accuracy: 0.6664\n",
      "Epoch 18: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 734s 490ms/step - loss: 0.7788 - accuracy: 0.6664 - val_loss: 0.7096 - val_accuracy: 0.7230\n",
      "Epoch 19/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7748 - accuracy: 0.6744\n",
      "Epoch 19: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.7748 - accuracy: 0.6744 - val_loss: 0.6625 - val_accuracy: 0.7498\n",
      "Epoch 20/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.6818\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.7604 - accuracy: 0.6818 - val_loss: 0.7019 - val_accuracy: 0.7335\n",
      "Epoch 21/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7570 - accuracy: 0.6772\n",
      "Epoch 21: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.7570 - accuracy: 0.6772 - val_loss: 0.6745 - val_accuracy: 0.7330\n",
      "Epoch 22/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7475 - accuracy: 0.6809\n",
      "Epoch 22: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.7475 - accuracy: 0.6809 - val_loss: 0.7112 - val_accuracy: 0.7168\n",
      "Epoch 23/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.6957\n",
      "Epoch 23: val_accuracy improved from 0.75000 to 0.76700, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.7391 - accuracy: 0.6957 - val_loss: 0.6568 - val_accuracy: 0.7670\n",
      "Epoch 24/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7357 - accuracy: 0.6938\n",
      "Epoch 24: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.7357 - accuracy: 0.6938 - val_loss: 0.7120 - val_accuracy: 0.7437\n",
      "Epoch 25/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.6973\n",
      "Epoch 25: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.7272 - accuracy: 0.6973 - val_loss: 0.7198 - val_accuracy: 0.7477\n",
      "Epoch 26/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.6992\n",
      "Epoch 26: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 734s 490ms/step - loss: 0.7191 - accuracy: 0.6992 - val_loss: 0.6966 - val_accuracy: 0.7570\n",
      "Epoch 27/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.7078\n",
      "Epoch 27: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 737s 491ms/step - loss: 0.7086 - accuracy: 0.7078 - val_loss: 0.6937 - val_accuracy: 0.7433\n",
      "Epoch 28/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.7080\n",
      "Epoch 28: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.7073 - accuracy: 0.7080 - val_loss: 0.7187 - val_accuracy: 0.7495\n",
      "Epoch 29/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.7164\n",
      "Epoch 29: val_accuracy improved from 0.76700 to 0.77075, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6917 - accuracy: 0.7164 - val_loss: 0.6557 - val_accuracy: 0.7707\n",
      "Epoch 30/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.7167\n",
      "Epoch 30: val_accuracy did not improve from 0.77075\n",
      "1500/1500 [==============================] - 736s 491ms/step - loss: 0.6901 - accuracy: 0.7167 - val_loss: 0.6293 - val_accuracy: 0.7685\n",
      "Epoch 31/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.7225\n",
      "Epoch 31: val_accuracy improved from 0.77075 to 0.78625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.6797 - accuracy: 0.7225 - val_loss: 0.6509 - val_accuracy: 0.7862\n",
      "Epoch 32/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7243\n",
      "Epoch 32: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 727s 484ms/step - loss: 0.6764 - accuracy: 0.7243 - val_loss: 0.7232 - val_accuracy: 0.7475\n",
      "Epoch 33/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6703 - accuracy: 0.7330\n",
      "Epoch 33: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 726s 484ms/step - loss: 0.6703 - accuracy: 0.7330 - val_loss: 0.6426 - val_accuracy: 0.7540\n",
      "Epoch 34/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6615 - accuracy: 0.7322\n",
      "Epoch 34: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 727s 484ms/step - loss: 0.6615 - accuracy: 0.7322 - val_loss: 0.6363 - val_accuracy: 0.7697\n",
      "Epoch 35/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7371\n",
      "Epoch 35: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.6571 - accuracy: 0.7371 - val_loss: 0.6141 - val_accuracy: 0.7807\n",
      "Epoch 36/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7397\n",
      "Epoch 36: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6531 - accuracy: 0.7397 - val_loss: 0.7280 - val_accuracy: 0.7513\n",
      "Epoch 37/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.7441\n",
      "Epoch 37: val_accuracy improved from 0.78625 to 0.79650, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.6436 - accuracy: 0.7441 - val_loss: 0.5980 - val_accuracy: 0.7965\n",
      "Epoch 38/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.7498\n",
      "Epoch 38: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6336 - accuracy: 0.7498 - val_loss: 0.5873 - val_accuracy: 0.7878\n",
      "Epoch 39/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.7552\n",
      "Epoch 39: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.6254 - accuracy: 0.7552 - val_loss: 0.7391 - val_accuracy: 0.7640\n",
      "Epoch 40/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.7553\n",
      "Epoch 40: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6238 - accuracy: 0.7553 - val_loss: 0.6414 - val_accuracy: 0.7617\n",
      "Epoch 41/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7617\n",
      "Epoch 41: val_accuracy improved from 0.79650 to 0.80475, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6093 - accuracy: 0.7617 - val_loss: 0.5609 - val_accuracy: 0.8048\n",
      "Epoch 42/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7642\n",
      "Epoch 42: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6065 - accuracy: 0.7642 - val_loss: 0.5898 - val_accuracy: 0.7995\n",
      "Epoch 43/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.7655\n",
      "Epoch 43: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.6023 - accuracy: 0.7655 - val_loss: 0.6141 - val_accuracy: 0.7958\n",
      "Epoch 44/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.7686\n",
      "Epoch 44: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.5971 - accuracy: 0.7686 - val_loss: 0.7490 - val_accuracy: 0.7778\n",
      "Epoch 45/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.7746\n",
      "Epoch 45: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 738s 492ms/step - loss: 0.5881 - accuracy: 0.7746 - val_loss: 0.6979 - val_accuracy: 0.7845\n",
      "Epoch 46/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7736\n",
      "Epoch 46: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.5833 - accuracy: 0.7736 - val_loss: 0.6287 - val_accuracy: 0.7815\n",
      "Epoch 47/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7747\n",
      "Epoch 47: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.5843 - accuracy: 0.7747 - val_loss: 0.5917 - val_accuracy: 0.7843\n",
      "Epoch 48/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.7841\n",
      "Epoch 48: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.5687 - accuracy: 0.7841 - val_loss: 0.6671 - val_accuracy: 0.7952\n",
      "Epoch 49/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.7798\n",
      "Epoch 49: val_accuracy improved from 0.80475 to 0.81350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.5712 - accuracy: 0.7798 - val_loss: 0.5863 - val_accuracy: 0.8135\n",
      "Epoch 50/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7857\n",
      "Epoch 50: val_accuracy did not improve from 0.81350\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.5653 - accuracy: 0.7857 - val_loss: 0.5616 - val_accuracy: 0.8117\n",
      "Epoch 51/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.7877\n",
      "Epoch 51: val_accuracy improved from 0.81350 to 0.81575, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.5599 - accuracy: 0.7877 - val_loss: 0.6071 - val_accuracy: 0.8158\n",
      "Epoch 52/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.7947\n",
      "Epoch 52: val_accuracy did not improve from 0.81575\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.5548 - accuracy: 0.7947 - val_loss: 0.6255 - val_accuracy: 0.8045\n",
      "Epoch 53/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.7912\n",
      "Epoch 53: val_accuracy did not improve from 0.81575\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.5467 - accuracy: 0.7912 - val_loss: 0.6471 - val_accuracy: 0.7993\n",
      "Epoch 54/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.7937\n",
      "Epoch 54: val_accuracy improved from 0.81575 to 0.82775, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.5438 - accuracy: 0.7937 - val_loss: 0.5188 - val_accuracy: 0.8278\n",
      "Epoch 55/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.8020\n",
      "Epoch 55: val_accuracy did not improve from 0.82775\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.5362 - accuracy: 0.8020 - val_loss: 0.5787 - val_accuracy: 0.7990\n",
      "Epoch 56/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.8008\n",
      "Epoch 56: val_accuracy improved from 0.82775 to 0.83275, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 764s 510ms/step - loss: 0.5328 - accuracy: 0.8008 - val_loss: 0.5255 - val_accuracy: 0.8328\n",
      "Epoch 57/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.8030\n",
      "Epoch 57: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.5289 - accuracy: 0.8030 - val_loss: 0.8143 - val_accuracy: 0.7295\n",
      "Epoch 58/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8020\n",
      "Epoch 58: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.5258 - accuracy: 0.8020 - val_loss: 0.5573 - val_accuracy: 0.8130\n",
      "Epoch 59/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.8117\n",
      "Epoch 59: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.5094 - accuracy: 0.8117 - val_loss: 0.5561 - val_accuracy: 0.8285\n",
      "Epoch 60/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.8148\n",
      "Epoch 60: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 757s 504ms/step - loss: 0.5074 - accuracy: 0.8148 - val_loss: 0.5645 - val_accuracy: 0.8207\n",
      "Epoch 61/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8123\n",
      "Epoch 61: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.5084 - accuracy: 0.8123 - val_loss: 0.5686 - val_accuracy: 0.8310\n",
      "Epoch 62/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.8158\n",
      "Epoch 62: val_accuracy improved from 0.83275 to 0.83525, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.5032 - accuracy: 0.8158 - val_loss: 0.5154 - val_accuracy: 0.8353\n",
      "Epoch 63/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.8191\n",
      "Epoch 63: val_accuracy improved from 0.83525 to 0.83625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4987 - accuracy: 0.8191 - val_loss: 0.5229 - val_accuracy: 0.8363\n",
      "Epoch 64/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.8187\n",
      "Epoch 64: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.4944 - accuracy: 0.8187 - val_loss: 0.5820 - val_accuracy: 0.8225\n",
      "Epoch 65/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8172\n",
      "Epoch 65: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.5020 - accuracy: 0.8172 - val_loss: 0.5226 - val_accuracy: 0.8292\n",
      "Epoch 66/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.8224\n",
      "Epoch 66: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4869 - accuracy: 0.8224 - val_loss: 0.5649 - val_accuracy: 0.8288\n",
      "Epoch 67/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.8263\n",
      "Epoch 67: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4812 - accuracy: 0.8263 - val_loss: 0.6090 - val_accuracy: 0.8347\n",
      "Epoch 68/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.8332\n",
      "Epoch 68: val_accuracy improved from 0.83625 to 0.84500, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4669 - accuracy: 0.8332 - val_loss: 0.5678 - val_accuracy: 0.8450\n",
      "Epoch 69/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.8332\n",
      "Epoch 69: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4710 - accuracy: 0.8332 - val_loss: 0.6529 - val_accuracy: 0.8320\n",
      "Epoch 70/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.8287\n",
      "Epoch 70: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.4743 - accuracy: 0.8287 - val_loss: 0.5901 - val_accuracy: 0.8278\n",
      "Epoch 71/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.8367\n",
      "Epoch 71: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.4680 - accuracy: 0.8367 - val_loss: 0.6101 - val_accuracy: 0.8305\n",
      "Epoch 72/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.8321\n",
      "Epoch 72: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.4642 - accuracy: 0.8321 - val_loss: 0.6540 - val_accuracy: 0.8303\n",
      "Epoch 73/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.8368\n",
      "Epoch 73: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.4612 - accuracy: 0.8368 - val_loss: 0.5910 - val_accuracy: 0.8205\n",
      "Epoch 74/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4573 - accuracy: 0.8391\n",
      "Epoch 74: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.4573 - accuracy: 0.8391 - val_loss: 0.5450 - val_accuracy: 0.8440\n",
      "Epoch 75/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.8424\n",
      "Epoch 75: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.4531 - accuracy: 0.8424 - val_loss: 0.6260 - val_accuracy: 0.8257\n",
      "Epoch 76/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8398\n",
      "Epoch 76: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4508 - accuracy: 0.8398 - val_loss: 0.6273 - val_accuracy: 0.8303\n",
      "Epoch 77/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8427\n",
      "Epoch 77: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4451 - accuracy: 0.8427 - val_loss: 0.5127 - val_accuracy: 0.8422\n",
      "Epoch 78/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8450\n",
      "Epoch 78: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4410 - accuracy: 0.8450 - val_loss: 0.5476 - val_accuracy: 0.8397\n",
      "Epoch 79/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8404\n",
      "Epoch 79: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4475 - accuracy: 0.8404 - val_loss: 0.5336 - val_accuracy: 0.8235\n",
      "Epoch 80/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8497\n",
      "Epoch 80: val_accuracy improved from 0.84500 to 0.84675, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4302 - accuracy: 0.8497 - val_loss: 0.5888 - val_accuracy: 0.8468\n",
      "Epoch 81/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4434 - accuracy: 0.8421\n",
      "Epoch 81: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.4434 - accuracy: 0.8421 - val_loss: 0.5270 - val_accuracy: 0.8393\n",
      "Epoch 82/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8561\n",
      "Epoch 82: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4263 - accuracy: 0.8561 - val_loss: 0.9185 - val_accuracy: 0.7782\n",
      "Epoch 83/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8515\n",
      "Epoch 83: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.4281 - accuracy: 0.8515 - val_loss: 0.5874 - val_accuracy: 0.8462\n",
      "Epoch 84/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8523\n",
      "Epoch 84: val_accuracy improved from 0.84675 to 0.85350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4223 - accuracy: 0.8523 - val_loss: 0.4755 - val_accuracy: 0.8535\n",
      "Epoch 85/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8564\n",
      "Epoch 85: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4235 - accuracy: 0.8564 - val_loss: 0.4876 - val_accuracy: 0.8505\n",
      "Epoch 86/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8580\n",
      "Epoch 86: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4162 - accuracy: 0.8580 - val_loss: 0.5609 - val_accuracy: 0.8347\n",
      "Epoch 87/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8520\n",
      "Epoch 87: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4246 - accuracy: 0.8520 - val_loss: 0.5541 - val_accuracy: 0.8510\n",
      "Epoch 88/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8623\n",
      "Epoch 88: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4128 - accuracy: 0.8623 - val_loss: 0.5537 - val_accuracy: 0.8432\n",
      "Epoch 89/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.8581\n",
      "Epoch 89: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.4173 - accuracy: 0.8581 - val_loss: 0.4995 - val_accuracy: 0.8447\n",
      "Epoch 90/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8601\n",
      "Epoch 90: val_accuracy improved from 0.85350 to 0.85500, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.4080 - accuracy: 0.8601 - val_loss: 0.5557 - val_accuracy: 0.8550\n",
      "Epoch 91/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8617\n",
      "Epoch 91: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4084 - accuracy: 0.8617 - val_loss: 0.5751 - val_accuracy: 0.8375\n",
      "Epoch 92/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8612\n",
      "Epoch 92: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4101 - accuracy: 0.8612 - val_loss: 0.4743 - val_accuracy: 0.8543\n",
      "Epoch 93/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.8608\n",
      "Epoch 93: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4023 - accuracy: 0.8608 - val_loss: 0.6251 - val_accuracy: 0.8422\n",
      "Epoch 94/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8645\n",
      "Epoch 94: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4021 - accuracy: 0.8645 - val_loss: 0.6131 - val_accuracy: 0.8435\n",
      "Epoch 95/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8664\n",
      "Epoch 95: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4003 - accuracy: 0.8664 - val_loss: 0.5612 - val_accuracy: 0.8545\n",
      "Epoch 96/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8668\n",
      "Epoch 96: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.3941 - accuracy: 0.8668 - val_loss: 0.8340 - val_accuracy: 0.8008\n",
      "Epoch 97/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8677\n",
      "Epoch 97: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.3985 - accuracy: 0.8677 - val_loss: 0.4928 - val_accuracy: 0.8533\n",
      "Epoch 98/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8688\n",
      "Epoch 98: val_accuracy improved from 0.85500 to 0.85650, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3914 - accuracy: 0.8688 - val_loss: 0.5252 - val_accuracy: 0.8565\n",
      "Epoch 99/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8697\n",
      "Epoch 99: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.3973 - accuracy: 0.8697 - val_loss: 0.5896 - val_accuracy: 0.8520\n",
      "Epoch 100/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8723\n",
      "Epoch 100: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3855 - accuracy: 0.8723 - val_loss: 0.6307 - val_accuracy: 0.8345\n",
      "Epoch 101/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8710\n",
      "Epoch 101: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3894 - accuracy: 0.8710 - val_loss: 0.6345 - val_accuracy: 0.8320\n",
      "Epoch 102/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8684\n",
      "Epoch 102: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.3891 - accuracy: 0.8684 - val_loss: 0.7340 - val_accuracy: 0.8288\n",
      "Epoch 103/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8738\n",
      "Epoch 103: val_accuracy improved from 0.85650 to 0.86275, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3830 - accuracy: 0.8738 - val_loss: 0.5133 - val_accuracy: 0.8627\n",
      "Epoch 104/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.8748\n",
      "Epoch 104: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3823 - accuracy: 0.8748 - val_loss: 0.5505 - val_accuracy: 0.8535\n",
      "Epoch 105/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8796\n",
      "Epoch 105: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.3718 - accuracy: 0.8796 - val_loss: 0.5561 - val_accuracy: 0.8515\n",
      "Epoch 106/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.8758\n",
      "Epoch 106: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.3762 - accuracy: 0.8758 - val_loss: 0.6139 - val_accuracy: 0.8485\n",
      "Epoch 107/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.8766\n",
      "Epoch 107: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3737 - accuracy: 0.8766 - val_loss: 0.6620 - val_accuracy: 0.8393\n",
      "Epoch 108/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8739\n",
      "Epoch 108: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3779 - accuracy: 0.8739 - val_loss: 0.4906 - val_accuracy: 0.8610\n",
      "Epoch 109/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8783\n",
      "Epoch 109: val_accuracy improved from 0.86275 to 0.86575, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3731 - accuracy: 0.8783 - val_loss: 0.5566 - val_accuracy: 0.8658\n",
      "Epoch 110/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8743\n",
      "Epoch 110: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3759 - accuracy: 0.8743 - val_loss: 0.5314 - val_accuracy: 0.8643\n",
      "Epoch 111/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8813\n",
      "Epoch 111: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3678 - accuracy: 0.8813 - val_loss: 0.8752 - val_accuracy: 0.7822\n",
      "Epoch 112/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8753\n",
      "Epoch 112: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3703 - accuracy: 0.8753 - val_loss: 0.5261 - val_accuracy: 0.8533\n",
      "Epoch 113/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.8864\n",
      "Epoch 113: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3528 - accuracy: 0.8864 - val_loss: 0.6082 - val_accuracy: 0.8540\n",
      "Epoch 114/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8746\n",
      "Epoch 114: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 760s 507ms/step - loss: 0.3689 - accuracy: 0.8746 - val_loss: 0.7334 - val_accuracy: 0.8328\n",
      "Epoch 115/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8814\n",
      "Epoch 115: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.3636 - accuracy: 0.8814 - val_loss: 0.5768 - val_accuracy: 0.8565\n",
      "Epoch 116/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8788\n",
      "Epoch 116: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.3662 - accuracy: 0.8788 - val_loss: 0.5324 - val_accuracy: 0.8472\n",
      "Epoch 117/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.8817\n",
      "Epoch 117: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3598 - accuracy: 0.8817 - val_loss: 0.5813 - val_accuracy: 0.8397\n",
      "Epoch 118/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8800\n",
      "Epoch 118: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3602 - accuracy: 0.8800 - val_loss: 0.5183 - val_accuracy: 0.8500\n",
      "Epoch 119/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8844\n",
      "Epoch 119: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.3566 - accuracy: 0.8844 - val_loss: 0.7469 - val_accuracy: 0.8332\n",
      "Epoch 120/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8857\n",
      "Epoch 120: val_accuracy improved from 0.86575 to 0.86625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.3539 - accuracy: 0.8857 - val_loss: 0.5423 - val_accuracy: 0.8662\n",
      "Epoch 121/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.8834\n",
      "Epoch 121: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3535 - accuracy: 0.8834 - val_loss: 0.5976 - val_accuracy: 0.8528\n",
      "Epoch 122/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8915\n",
      "Epoch 122: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3464 - accuracy: 0.8915 - val_loss: 0.5434 - val_accuracy: 0.8572\n",
      "Epoch 123/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8877\n",
      "Epoch 123: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 768s 512ms/step - loss: 0.3461 - accuracy: 0.8877 - val_loss: 0.5654 - val_accuracy: 0.8597\n",
      "Epoch 124/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8827\n",
      "Epoch 124: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.3558 - accuracy: 0.8827 - val_loss: 0.6053 - val_accuracy: 0.8450\n",
      "Epoch 125/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8877\n",
      "Epoch 125: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.3508 - accuracy: 0.8877 - val_loss: 0.5532 - val_accuracy: 0.8500\n",
      "Epoch 126/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8898\n",
      "Epoch 126: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3446 - accuracy: 0.8898 - val_loss: 0.4910 - val_accuracy: 0.8637\n",
      "Epoch 127/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8921\n",
      "Epoch 127: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3400 - accuracy: 0.8921 - val_loss: 0.5339 - val_accuracy: 0.8633\n",
      "Epoch 128/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8902\n",
      "Epoch 128: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3409 - accuracy: 0.8902 - val_loss: 0.5272 - val_accuracy: 0.8635\n",
      "Epoch 129/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8913\n",
      "Epoch 129: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.3441 - accuracy: 0.8913 - val_loss: 0.5396 - val_accuracy: 0.8595\n",
      "Epoch 130/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8903\n",
      "Epoch 130: val_accuracy improved from 0.86625 to 0.86675, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.3441 - accuracy: 0.8903 - val_loss: 0.5354 - val_accuracy: 0.8668\n",
      "Epoch 131/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8900\n",
      "Epoch 131: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3425 - accuracy: 0.8900 - val_loss: 0.5735 - val_accuracy: 0.8487\n",
      "Epoch 132/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.8901\n",
      "Epoch 132: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 761s 508ms/step - loss: 0.3435 - accuracy: 0.8901 - val_loss: 0.5628 - val_accuracy: 0.8515\n",
      "Epoch 133/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.8945\n",
      "Epoch 133: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.3362 - accuracy: 0.8945 - val_loss: 0.6370 - val_accuracy: 0.8510\n",
      "Epoch 134/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.8928\n",
      "Epoch 134: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.3298 - accuracy: 0.8928 - val_loss: 0.6382 - val_accuracy: 0.8512\n",
      "Epoch 135/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8957\n",
      "Epoch 135: val_accuracy improved from 0.86675 to 0.87025, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.3329 - accuracy: 0.8957 - val_loss: 0.5651 - val_accuracy: 0.8702\n",
      "Epoch 136/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8947\n",
      "Epoch 136: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3280 - accuracy: 0.8947 - val_loss: 0.6832 - val_accuracy: 0.8320\n",
      "Epoch 137/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8964\n",
      "Epoch 137: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3296 - accuracy: 0.8964 - val_loss: 0.5452 - val_accuracy: 0.8618\n",
      "Epoch 138/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8962\n",
      "Epoch 138: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3270 - accuracy: 0.8962 - val_loss: 0.6319 - val_accuracy: 0.8570\n",
      "Epoch 139/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8953\n",
      "Epoch 139: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.3288 - accuracy: 0.8953 - val_loss: 0.5578 - val_accuracy: 0.8503\n",
      "Epoch 140/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8952\n",
      "Epoch 140: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 762s 508ms/step - loss: 0.3329 - accuracy: 0.8952 - val_loss: 0.4714 - val_accuracy: 0.8655\n",
      "Epoch 141/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9030\n",
      "Epoch 141: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 761s 508ms/step - loss: 0.3201 - accuracy: 0.9030 - val_loss: 0.6045 - val_accuracy: 0.8615\n",
      "Epoch 142/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9040\n",
      "Epoch 142: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3142 - accuracy: 0.9040 - val_loss: 0.6268 - val_accuracy: 0.8665\n",
      "Epoch 143/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8986\n",
      "Epoch 143: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3220 - accuracy: 0.8986 - val_loss: 0.5623 - val_accuracy: 0.8537\n",
      "Epoch 144/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9013\n",
      "Epoch 144: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.3185 - accuracy: 0.9013 - val_loss: 0.5865 - val_accuracy: 0.8590\n",
      "Epoch 145/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.9001\n",
      "Epoch 145: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3180 - accuracy: 0.9001 - val_loss: 0.4967 - val_accuracy: 0.8625\n",
      "Epoch 146/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.9033\n",
      "Epoch 146: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3178 - accuracy: 0.9033 - val_loss: 0.6036 - val_accuracy: 0.8630\n",
      "Epoch 147/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9017\n",
      "Epoch 147: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.3207 - accuracy: 0.9017 - val_loss: 0.5571 - val_accuracy: 0.8627\n",
      "Epoch 148/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9062\n",
      "Epoch 148: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.3125 - accuracy: 0.9062 - val_loss: 0.5739 - val_accuracy: 0.8625\n",
      "Epoch 149/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.9039\n",
      "Epoch 149: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.3160 - accuracy: 0.9039 - val_loss: 0.6418 - val_accuracy: 0.8553\n",
      "Epoch 150/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.9041\n",
      "Epoch 150: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3174 - accuracy: 0.9041 - val_loss: 0.5952 - val_accuracy: 0.8470\n",
      "Epoch 151/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9047\n",
      "Epoch 151: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3110 - accuracy: 0.9047 - val_loss: 0.6212 - val_accuracy: 0.8615\n",
      "Epoch 152/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.9022\n",
      "Epoch 152: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3144 - accuracy: 0.9022 - val_loss: 0.8678 - val_accuracy: 0.8100\n",
      "Epoch 153/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.9030\n",
      "Epoch 153: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3155 - accuracy: 0.9030 - val_loss: 0.5589 - val_accuracy: 0.8505\n",
      "Epoch 154/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9057\n",
      "Epoch 154: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3111 - accuracy: 0.9057 - val_loss: 0.6499 - val_accuracy: 0.8435\n",
      "Epoch 155/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9067\n",
      "Epoch 155: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.3108 - accuracy: 0.9067 - val_loss: 0.5307 - val_accuracy: 0.8618\n",
      "Epoch 156/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.9073\n",
      "Epoch 156: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.3052 - accuracy: 0.9073 - val_loss: 0.7724 - val_accuracy: 0.8167\n",
      "Epoch 157/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.9103\n",
      "Epoch 157: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.3043 - accuracy: 0.9103 - val_loss: 0.6471 - val_accuracy: 0.8493\n",
      "Epoch 158/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9043\n",
      "Epoch 158: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3127 - accuracy: 0.9043 - val_loss: 0.5510 - val_accuracy: 0.8468\n",
      "Epoch 159/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9097\n",
      "Epoch 159: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3030 - accuracy: 0.9097 - val_loss: 0.6106 - val_accuracy: 0.8575\n",
      "Epoch 160/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.9057\n",
      "Epoch 160: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3060 - accuracy: 0.9057 - val_loss: 0.5879 - val_accuracy: 0.8608\n",
      "Epoch 161/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9083\n",
      "Epoch 161: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3047 - accuracy: 0.9083 - val_loss: 0.5434 - val_accuracy: 0.8618\n",
      "Epoch 162/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9093\n",
      "Epoch 162: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3017 - accuracy: 0.9093 - val_loss: 0.6106 - val_accuracy: 0.8553\n",
      "Epoch 163/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9105\n",
      "Epoch 163: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2990 - accuracy: 0.9105 - val_loss: 0.6778 - val_accuracy: 0.8587\n",
      "Epoch 164/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9078\n",
      "Epoch 164: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3019 - accuracy: 0.9078 - val_loss: 0.6376 - val_accuracy: 0.8405\n",
      "Epoch 165/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.9067\n",
      "Epoch 165: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3090 - accuracy: 0.9067 - val_loss: 0.5399 - val_accuracy: 0.8602\n",
      "Epoch 166/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9088\n",
      "Epoch 166: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2985 - accuracy: 0.9088 - val_loss: 0.6354 - val_accuracy: 0.8537\n",
      "Epoch 167/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.9116\n",
      "Epoch 167: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3014 - accuracy: 0.9116 - val_loss: 0.6144 - val_accuracy: 0.8615\n",
      "Epoch 168/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9149\n",
      "Epoch 168: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.2941 - accuracy: 0.9149 - val_loss: 0.5417 - val_accuracy: 0.8675\n",
      "Epoch 169/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9121\n",
      "Epoch 169: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.2905 - accuracy: 0.9121 - val_loss: 0.5251 - val_accuracy: 0.8553\n",
      "Epoch 170/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9073\n",
      "Epoch 170: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2962 - accuracy: 0.9073 - val_loss: 0.5392 - val_accuracy: 0.8620\n",
      "Epoch 171/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9146\n",
      "Epoch 171: val_accuracy improved from 0.87025 to 0.87150, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2932 - accuracy: 0.9146 - val_loss: 0.5480 - val_accuracy: 0.8715\n",
      "Epoch 172/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9087\n",
      "Epoch 172: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2996 - accuracy: 0.9087 - val_loss: 0.6608 - val_accuracy: 0.8497\n",
      "Epoch 173/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9109\n",
      "Epoch 173: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 745s 496ms/step - loss: 0.2990 - accuracy: 0.9109 - val_loss: 0.5804 - val_accuracy: 0.8675\n",
      "Epoch 174/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9130\n",
      "Epoch 174: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.2914 - accuracy: 0.9130 - val_loss: 0.6666 - val_accuracy: 0.8522\n",
      "Epoch 175/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9149\n",
      "Epoch 175: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2894 - accuracy: 0.9149 - val_loss: 0.6381 - val_accuracy: 0.8455\n",
      "Epoch 176/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9112\n",
      "Epoch 176: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 743s 496ms/step - loss: 0.2942 - accuracy: 0.9112 - val_loss: 0.6315 - val_accuracy: 0.8270\n",
      "Epoch 177/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9146\n",
      "Epoch 177: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2889 - accuracy: 0.9146 - val_loss: 0.7251 - val_accuracy: 0.8280\n",
      "Epoch 178/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9187\n",
      "Epoch 178: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.2884 - accuracy: 0.9187 - val_loss: 0.6678 - val_accuracy: 0.8677\n",
      "Epoch 179/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9131\n",
      "Epoch 179: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.2971 - accuracy: 0.9131 - val_loss: 0.5331 - val_accuracy: 0.8620\n",
      "Epoch 180/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9161\n",
      "Epoch 180: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2878 - accuracy: 0.9161 - val_loss: 0.6271 - val_accuracy: 0.8370\n",
      "Epoch 181/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9137\n",
      "Epoch 181: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2909 - accuracy: 0.9137 - val_loss: 0.9455 - val_accuracy: 0.7930\n",
      "Epoch 182/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9178\n",
      "Epoch 182: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2858 - accuracy: 0.9178 - val_loss: 0.5681 - val_accuracy: 0.8640\n",
      "Epoch 183/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9141\n",
      "Epoch 183: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 770s 513ms/step - loss: 0.2874 - accuracy: 0.9141 - val_loss: 0.6032 - val_accuracy: 0.8572\n",
      "Epoch 184/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.9124\n",
      "Epoch 184: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2951 - accuracy: 0.9124 - val_loss: 0.5316 - val_accuracy: 0.8673\n",
      "Epoch 185/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9182\n",
      "Epoch 185: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2881 - accuracy: 0.9182 - val_loss: 0.5979 - val_accuracy: 0.8683\n",
      "Epoch 186/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9205\n",
      "Epoch 186: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2823 - accuracy: 0.9205 - val_loss: 0.6136 - val_accuracy: 0.8610\n",
      "Epoch 187/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.9125\n",
      "Epoch 187: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2921 - accuracy: 0.9125 - val_loss: 0.6200 - val_accuracy: 0.8540\n",
      "Epoch 188/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.9187\n",
      "Epoch 188: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.2826 - accuracy: 0.9187 - val_loss: 0.5010 - val_accuracy: 0.8597\n",
      "Epoch 189/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9184\n",
      "Epoch 189: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2858 - accuracy: 0.9184 - val_loss: 0.6183 - val_accuracy: 0.8677\n",
      "Epoch 190/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9170\n",
      "Epoch 190: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.2819 - accuracy: 0.9170 - val_loss: 0.5665 - val_accuracy: 0.8712\n",
      "Epoch 191/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9158\n",
      "Epoch 191: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.2834 - accuracy: 0.9158 - val_loss: 0.6570 - val_accuracy: 0.8353\n",
      "Epoch 192/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9224\n",
      "Epoch 192: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2750 - accuracy: 0.9224 - val_loss: 0.6368 - val_accuracy: 0.8530\n",
      "Epoch 193/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9216\n",
      "Epoch 193: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.2771 - accuracy: 0.9216 - val_loss: 0.5601 - val_accuracy: 0.8555\n",
      "Epoch 194/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9218\n",
      "Epoch 194: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.2771 - accuracy: 0.9218 - val_loss: 0.6327 - val_accuracy: 0.8587\n",
      "Epoch 195/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9204\n",
      "Epoch 195: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.2797 - accuracy: 0.9204 - val_loss: 0.6723 - val_accuracy: 0.8353\n",
      "Epoch 196/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9232\n",
      "Epoch 196: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2786 - accuracy: 0.9232 - val_loss: 0.5733 - val_accuracy: 0.8658\n",
      "Epoch 197/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9217\n",
      "Epoch 197: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2798 - accuracy: 0.9217 - val_loss: 0.6187 - val_accuracy: 0.8472\n",
      "Epoch 198/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9242\n",
      "Epoch 198: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.2739 - accuracy: 0.9242 - val_loss: 0.5745 - val_accuracy: 0.8633\n",
      "Epoch 199/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9182\n",
      "Epoch 199: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2813 - accuracy: 0.9182 - val_loss: 0.5253 - val_accuracy: 0.8515\n",
      "Epoch 200/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.9211\n",
      "Epoch 200: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2756 - accuracy: 0.9211 - val_loss: 0.6415 - val_accuracy: 0.8612\n",
      "Epoch 201/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9220\n",
      "Epoch 201: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2736 - accuracy: 0.9220 - val_loss: 0.5672 - val_accuracy: 0.8605\n",
      "Epoch 202/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9277\n",
      "Epoch 202: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2697 - accuracy: 0.9277 - val_loss: 0.6030 - val_accuracy: 0.8627\n",
      "Epoch 203/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9258\n",
      "Epoch 203: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2661 - accuracy: 0.9258 - val_loss: 0.6926 - val_accuracy: 0.8522\n",
      "Epoch 204/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9295\n",
      "Epoch 204: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2618 - accuracy: 0.9295 - val_loss: 0.6071 - val_accuracy: 0.8627\n",
      "Epoch 205/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9227\n",
      "Epoch 205: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 769s 513ms/step - loss: 0.2763 - accuracy: 0.9227 - val_loss: 0.5368 - val_accuracy: 0.8618\n",
      "Epoch 206/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9241\n",
      "Epoch 206: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2676 - accuracy: 0.9241 - val_loss: 0.6840 - val_accuracy: 0.8570\n",
      "Epoch 207/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9248\n",
      "Epoch 207: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2697 - accuracy: 0.9248 - val_loss: 0.5905 - val_accuracy: 0.8597\n",
      "Epoch 208/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9258\n",
      "Epoch 208: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.2654 - accuracy: 0.9258 - val_loss: 0.5460 - val_accuracy: 0.8698\n",
      "Epoch 209/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9261\n",
      "Epoch 209: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2654 - accuracy: 0.9261 - val_loss: 0.6548 - val_accuracy: 0.8595\n",
      "Epoch 210/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9239\n",
      "Epoch 210: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2665 - accuracy: 0.9239 - val_loss: 0.6374 - val_accuracy: 0.8482\n",
      "Epoch 211/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9265\n",
      "Epoch 211: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2691 - accuracy: 0.9265 - val_loss: 0.5450 - val_accuracy: 0.8620\n",
      "Epoch 212/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9273\n",
      "Epoch 212: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2667 - accuracy: 0.9273 - val_loss: 0.5865 - val_accuracy: 0.8660\n",
      "Epoch 213/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9263\n",
      "Epoch 213: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2702 - accuracy: 0.9263 - val_loss: 0.6127 - val_accuracy: 0.8503\n",
      "Epoch 214/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9269\n",
      "Epoch 214: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2625 - accuracy: 0.9269 - val_loss: 0.6540 - val_accuracy: 0.8382\n",
      "Epoch 215/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9291\n",
      "Epoch 215: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2610 - accuracy: 0.9291 - val_loss: 0.5695 - val_accuracy: 0.8630\n",
      "Epoch 216/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9251\n",
      "Epoch 216: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2655 - accuracy: 0.9251 - val_loss: 0.6897 - val_accuracy: 0.8505\n",
      "Epoch 217/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9307\n",
      "Epoch 217: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2577 - accuracy: 0.9307 - val_loss: 0.5701 - val_accuracy: 0.8622\n",
      "Epoch 218/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9279\n",
      "Epoch 218: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2648 - accuracy: 0.9279 - val_loss: 0.6189 - val_accuracy: 0.8512\n",
      "Epoch 219/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9305\n",
      "Epoch 219: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.2607 - accuracy: 0.9305 - val_loss: 0.5832 - val_accuracy: 0.8577\n",
      "Epoch 220/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9300\n",
      "Epoch 220: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.2614 - accuracy: 0.9300 - val_loss: 0.6382 - val_accuracy: 0.8555\n",
      "Epoch 221/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9280\n",
      "Epoch 221: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 749s 500ms/step - loss: 0.2619 - accuracy: 0.9280 - val_loss: 0.6007 - val_accuracy: 0.8568\n",
      "Epoch 222/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9314\n",
      "Epoch 222: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2599 - accuracy: 0.9314 - val_loss: 0.6310 - val_accuracy: 0.8447\n",
      "Epoch 223/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9307\n",
      "Epoch 223: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2598 - accuracy: 0.9307 - val_loss: 0.6310 - val_accuracy: 0.8700\n",
      "Epoch 224/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9290\n",
      "Epoch 224: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2567 - accuracy: 0.9290 - val_loss: 0.8502 - val_accuracy: 0.8325\n",
      "Epoch 225/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9299\n",
      "Epoch 225: val_accuracy improved from 0.87150 to 0.87350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2576 - accuracy: 0.9299 - val_loss: 0.5694 - val_accuracy: 0.8735\n",
      "Epoch 226/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9308\n",
      "Epoch 226: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2578 - accuracy: 0.9308 - val_loss: 0.7478 - val_accuracy: 0.8325\n",
      "Epoch 227/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9303\n",
      "Epoch 227: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 760s 507ms/step - loss: 0.2592 - accuracy: 0.9303 - val_loss: 0.6790 - val_accuracy: 0.8508\n",
      "Epoch 228/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9324\n",
      "Epoch 228: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2559 - accuracy: 0.9324 - val_loss: 0.5862 - val_accuracy: 0.8625\n",
      "Epoch 229/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9325\n",
      "Epoch 229: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2543 - accuracy: 0.9325 - val_loss: 0.5778 - val_accuracy: 0.8645\n",
      "Epoch 230/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9317\n",
      "Epoch 230: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.2543 - accuracy: 0.9317 - val_loss: 0.7233 - val_accuracy: 0.8553\n",
      "Epoch 231/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9318\n",
      "Epoch 231: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2620 - accuracy: 0.9318 - val_loss: 0.6313 - val_accuracy: 0.8545\n",
      "Epoch 232/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9338\n",
      "Epoch 232: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2511 - accuracy: 0.9338 - val_loss: 0.6812 - val_accuracy: 0.8585\n",
      "Epoch 233/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9339\n",
      "Epoch 233: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2565 - accuracy: 0.9339 - val_loss: 0.6370 - val_accuracy: 0.8495\n",
      "Epoch 234/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9323\n",
      "Epoch 234: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.2558 - accuracy: 0.9323 - val_loss: 0.5738 - val_accuracy: 0.8545\n",
      "Epoch 235/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9334\n",
      "Epoch 235: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2521 - accuracy: 0.9334 - val_loss: 0.6500 - val_accuracy: 0.8705\n",
      "Epoch 236/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9360\n",
      "Epoch 236: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2488 - accuracy: 0.9360 - val_loss: 0.7044 - val_accuracy: 0.8515\n",
      "Epoch 237/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9358\n",
      "Epoch 237: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2504 - accuracy: 0.9358 - val_loss: 0.6207 - val_accuracy: 0.8658\n",
      "Epoch 238/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9332\n",
      "Epoch 238: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2545 - accuracy: 0.9332 - val_loss: 0.5878 - val_accuracy: 0.8708\n",
      "Epoch 239/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9365\n",
      "Epoch 239: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2490 - accuracy: 0.9365 - val_loss: 0.5358 - val_accuracy: 0.8683\n",
      "Epoch 240/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9359\n",
      "Epoch 240: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.2474 - accuracy: 0.9359 - val_loss: 0.5889 - val_accuracy: 0.8505\n",
      "Epoch 241/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9363\n",
      "Epoch 241: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2468 - accuracy: 0.9363 - val_loss: 0.7701 - val_accuracy: 0.8445\n",
      "Epoch 242/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9382\n",
      "Epoch 242: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 738s 492ms/step - loss: 0.2454 - accuracy: 0.9382 - val_loss: 0.7489 - val_accuracy: 0.8465\n",
      "Epoch 243/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9377\n",
      "Epoch 243: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.2444 - accuracy: 0.9377 - val_loss: 0.5696 - val_accuracy: 0.8610\n",
      "Epoch 244/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.9353\n",
      "Epoch 244: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.2453 - accuracy: 0.9353 - val_loss: 0.7373 - val_accuracy: 0.8332\n",
      "Epoch 245/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9337\n",
      "Epoch 245: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.2552 - accuracy: 0.9337 - val_loss: 0.6992 - val_accuracy: 0.8443\n",
      "Epoch 246/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9355\n",
      "Epoch 246: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.2542 - accuracy: 0.9355 - val_loss: 0.5948 - val_accuracy: 0.8583\n",
      "Epoch 247/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9355\n",
      "Epoch 247: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.2463 - accuracy: 0.9355 - val_loss: 0.6106 - val_accuracy: 0.8640\n",
      "Epoch 248/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9352\n",
      "Epoch 248: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 496ms/step - loss: 0.2511 - accuracy: 0.9352 - val_loss: 0.7702 - val_accuracy: 0.8267\n",
      "Epoch 249/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9359\n",
      "Epoch 249: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 736s 491ms/step - loss: 0.2514 - accuracy: 0.9359 - val_loss: 0.7582 - val_accuracy: 0.8400\n",
      "Epoch 250/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9356\n",
      "Epoch 250: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.2494 - accuracy: 0.9356 - val_loss: 0.5887 - val_accuracy: 0.8580\n",
      "Epoch 251/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9373\n",
      "Epoch 251: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 734s 489ms/step - loss: 0.2432 - accuracy: 0.9373 - val_loss: 0.6428 - val_accuracy: 0.8635\n",
      "Epoch 252/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9351\n",
      "Epoch 252: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 737s 491ms/step - loss: 0.2497 - accuracy: 0.9351 - val_loss: 0.5972 - val_accuracy: 0.8690\n",
      "Epoch 253/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.9398\n",
      "Epoch 253: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2420 - accuracy: 0.9398 - val_loss: 0.6237 - val_accuracy: 0.8535\n",
      "Epoch 254/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9378\n",
      "Epoch 254: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2451 - accuracy: 0.9378 - val_loss: 0.6356 - val_accuracy: 0.8485\n",
      "Epoch 255/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9337\n",
      "Epoch 255: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2440 - accuracy: 0.9337 - val_loss: 0.6486 - val_accuracy: 0.8555\n",
      "Epoch 256/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9337\n",
      "Epoch 256: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 768s 512ms/step - loss: 0.2575 - accuracy: 0.9337 - val_loss: 0.6541 - val_accuracy: 0.8580\n",
      "Epoch 257/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9352\n",
      "Epoch 257: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2449 - accuracy: 0.9352 - val_loss: 0.6802 - val_accuracy: 0.8555\n",
      "Epoch 258/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9400\n",
      "Epoch 258: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2448 - accuracy: 0.9400 - val_loss: 0.6648 - val_accuracy: 0.8562\n",
      "Epoch 259/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9362\n",
      "Epoch 259: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2535 - accuracy: 0.9362 - val_loss: 0.6151 - val_accuracy: 0.8583\n",
      "Epoch 260/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9352\n",
      "Epoch 260: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2510 - accuracy: 0.9352 - val_loss: 0.7149 - val_accuracy: 0.8580\n",
      "Epoch 261/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9403\n",
      "Epoch 261: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2433 - accuracy: 0.9403 - val_loss: 0.6782 - val_accuracy: 0.8487\n",
      "Epoch 262/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9398\n",
      "Epoch 262: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2472 - accuracy: 0.9398 - val_loss: 0.7256 - val_accuracy: 0.8443\n",
      "Epoch 263/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9417\n",
      "Epoch 263: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2383 - accuracy: 0.9417 - val_loss: 0.6007 - val_accuracy: 0.8605\n",
      "Epoch 264/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9442\n",
      "Epoch 264: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2343 - accuracy: 0.9442 - val_loss: 0.6610 - val_accuracy: 0.8622\n",
      "Epoch 265/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9385\n",
      "Epoch 265: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2446 - accuracy: 0.9385 - val_loss: 0.7656 - val_accuracy: 0.8393\n",
      "Epoch 266/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9388\n",
      "Epoch 266: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2451 - accuracy: 0.9388 - val_loss: 0.6306 - val_accuracy: 0.8585\n",
      "Epoch 267/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9384\n",
      "Epoch 267: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 506ms/step - loss: 0.2475 - accuracy: 0.9384 - val_loss: 0.6651 - val_accuracy: 0.8558\n",
      "Epoch 268/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9439\n",
      "Epoch 268: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2382 - accuracy: 0.9439 - val_loss: 0.6001 - val_accuracy: 0.8618\n",
      "Epoch 269/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9441\n",
      "Epoch 269: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2297 - accuracy: 0.9441 - val_loss: 0.6424 - val_accuracy: 0.8630\n",
      "Epoch 270/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9409\n",
      "Epoch 270: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2403 - accuracy: 0.9409 - val_loss: 0.5851 - val_accuracy: 0.8577\n",
      "Epoch 271/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9416\n",
      "Epoch 271: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2358 - accuracy: 0.9416 - val_loss: 0.6622 - val_accuracy: 0.8568\n",
      "Epoch 272/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9418\n",
      "Epoch 272: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.2347 - accuracy: 0.9418 - val_loss: 0.6168 - val_accuracy: 0.8580\n",
      "Epoch 273/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9417\n",
      "Epoch 273: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2390 - accuracy: 0.9417 - val_loss: 0.6668 - val_accuracy: 0.8625\n",
      "Epoch 274/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9428\n",
      "Epoch 274: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2316 - accuracy: 0.9428 - val_loss: 0.6522 - val_accuracy: 0.8577\n",
      "Epoch 275/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9436\n",
      "Epoch 275: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2355 - accuracy: 0.9436 - val_loss: 0.6290 - val_accuracy: 0.8685\n",
      "Epoch 276/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9435\n",
      "Epoch 276: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 760s 506ms/step - loss: 0.2308 - accuracy: 0.9435 - val_loss: 0.7292 - val_accuracy: 0.8605\n",
      "Epoch 277/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9444\n",
      "Epoch 277: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.2352 - accuracy: 0.9444 - val_loss: 0.6814 - val_accuracy: 0.8622\n",
      "Epoch 278/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9442\n",
      "Epoch 278: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2315 - accuracy: 0.9442 - val_loss: 0.6063 - val_accuracy: 0.8670\n",
      "Epoch 279/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9460\n",
      "Epoch 279: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 475s 316ms/step - loss: 0.2325 - accuracy: 0.9460 - val_loss: 0.7050 - val_accuracy: 0.8555\n",
      "Epoch 280/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9497\n",
      "Epoch 280: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2248 - accuracy: 0.9497 - val_loss: 0.6594 - val_accuracy: 0.8568\n",
      "Epoch 281/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9445\n",
      "Epoch 281: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2353 - accuracy: 0.9445 - val_loss: 0.6153 - val_accuracy: 0.8577\n",
      "Epoch 282/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9488\n",
      "Epoch 282: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2301 - accuracy: 0.9488 - val_loss: 0.5963 - val_accuracy: 0.8630\n",
      "Epoch 283/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9476\n",
      "Epoch 283: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2242 - accuracy: 0.9476 - val_loss: 0.6223 - val_accuracy: 0.8662\n",
      "Epoch 284/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9479\n",
      "Epoch 284: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2287 - accuracy: 0.9479 - val_loss: 0.6908 - val_accuracy: 0.8530\n",
      "Epoch 285/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9498\n",
      "Epoch 285: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2207 - accuracy: 0.9498 - val_loss: 0.7239 - val_accuracy: 0.8313\n",
      "Epoch 286/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9478\n",
      "Epoch 286: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2315 - accuracy: 0.9478 - val_loss: 0.5781 - val_accuracy: 0.8640\n",
      "Epoch 287/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9499\n",
      "Epoch 287: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2252 - accuracy: 0.9499 - val_loss: 0.6050 - val_accuracy: 0.8583\n",
      "Epoch 288/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9463\n",
      "Epoch 288: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2291 - accuracy: 0.9463 - val_loss: 0.6327 - val_accuracy: 0.8612\n",
      "Epoch 289/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9474\n",
      "Epoch 289: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2318 - accuracy: 0.9474 - val_loss: 0.6607 - val_accuracy: 0.8615\n",
      "Epoch 290/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9465\n",
      "Epoch 290: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2305 - accuracy: 0.9465 - val_loss: 0.7476 - val_accuracy: 0.8382\n",
      "Epoch 291/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9483\n",
      "Epoch 291: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2278 - accuracy: 0.9483 - val_loss: 0.5564 - val_accuracy: 0.8508\n",
      "Epoch 292/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9440\n",
      "Epoch 292: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2308 - accuracy: 0.9440 - val_loss: 0.5942 - val_accuracy: 0.8652\n",
      "Epoch 293/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9472\n",
      "Epoch 293: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2289 - accuracy: 0.9472 - val_loss: 0.6966 - val_accuracy: 0.8462\n",
      "Epoch 294/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9463\n",
      "Epoch 294: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2313 - accuracy: 0.9463 - val_loss: 0.6454 - val_accuracy: 0.8640\n",
      "Epoch 295/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9487\n",
      "Epoch 295: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2268 - accuracy: 0.9487 - val_loss: 0.6411 - val_accuracy: 0.8680\n",
      "Epoch 296/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9461\n",
      "Epoch 296: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2313 - accuracy: 0.9461 - val_loss: 0.6701 - val_accuracy: 0.8645\n",
      "Epoch 297/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9489\n",
      "Epoch 297: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2244 - accuracy: 0.9489 - val_loss: 0.5881 - val_accuracy: 0.8395\n",
      "Epoch 298/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9478\n",
      "Epoch 298: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2260 - accuracy: 0.9478 - val_loss: 0.5785 - val_accuracy: 0.8635\n",
      "Epoch 299/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9498\n",
      "Epoch 299: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2251 - accuracy: 0.9498 - val_loss: 0.7466 - val_accuracy: 0.8275\n",
      "Epoch 300/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9492\n",
      "Epoch 300: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2236 - accuracy: 0.9492 - val_loss: 0.5978 - val_accuracy: 0.8618\n",
      "Epoch 301/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9466\n",
      "Epoch 301: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2300 - accuracy: 0.9466 - val_loss: 0.7446 - val_accuracy: 0.8475\n",
      "Epoch 302/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9503\n",
      "Epoch 302: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2260 - accuracy: 0.9503 - val_loss: 0.7470 - val_accuracy: 0.8325\n",
      "Epoch 303/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9495\n",
      "Epoch 303: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2207 - accuracy: 0.9495 - val_loss: 0.5932 - val_accuracy: 0.8643\n",
      "Epoch 304/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9499\n",
      "Epoch 304: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2254 - accuracy: 0.9499 - val_loss: 0.5709 - val_accuracy: 0.8590\n",
      "Epoch 305/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9487\n",
      "Epoch 305: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2236 - accuracy: 0.9487 - val_loss: 0.6491 - val_accuracy: 0.8597\n",
      "Epoch 306/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9525\n",
      "Epoch 306: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2205 - accuracy: 0.9525 - val_loss: 0.6897 - val_accuracy: 0.8590\n",
      "Epoch 307/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9475\n",
      "Epoch 307: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2334 - accuracy: 0.9475 - val_loss: 0.6425 - val_accuracy: 0.8543\n",
      "Epoch 308/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9519\n",
      "Epoch 308: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2240 - accuracy: 0.9519 - val_loss: 0.7446 - val_accuracy: 0.8445\n",
      "Epoch 309/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9517\n",
      "Epoch 309: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2235 - accuracy: 0.9517 - val_loss: 0.5834 - val_accuracy: 0.8595\n",
      "Epoch 310/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9483\n",
      "Epoch 310: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2302 - accuracy: 0.9483 - val_loss: 0.5817 - val_accuracy: 0.8637\n",
      "Epoch 311/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9542\n",
      "Epoch 311: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2190 - accuracy: 0.9542 - val_loss: 0.7788 - val_accuracy: 0.8518\n",
      "Epoch 312/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9506\n",
      "Epoch 312: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2235 - accuracy: 0.9506 - val_loss: 0.5687 - val_accuracy: 0.8560\n",
      "Epoch 313/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9523\n",
      "Epoch 313: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2226 - accuracy: 0.9523 - val_loss: 0.7361 - val_accuracy: 0.8350\n",
      "Epoch 314/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9517\n",
      "Epoch 314: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2228 - accuracy: 0.9517 - val_loss: 0.6135 - val_accuracy: 0.8572\n",
      "Epoch 315/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9460\n",
      "Epoch 315: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2288 - accuracy: 0.9460 - val_loss: 0.6539 - val_accuracy: 0.8553\n",
      "Epoch 316/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9524\n",
      "Epoch 316: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2205 - accuracy: 0.9524 - val_loss: 0.7876 - val_accuracy: 0.8213\n",
      "Epoch 317/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9519\n",
      "Epoch 317: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2230 - accuracy: 0.9519 - val_loss: 0.6560 - val_accuracy: 0.8692\n",
      "Epoch 318/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9521\n",
      "Epoch 318: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2226 - accuracy: 0.9521 - val_loss: 0.6279 - val_accuracy: 0.8612\n",
      "Epoch 319/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9528\n",
      "Epoch 319: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2189 - accuracy: 0.9528 - val_loss: 0.6340 - val_accuracy: 0.8608\n",
      "Epoch 320/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9552\n",
      "Epoch 320: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.2155 - accuracy: 0.9552 - val_loss: 0.6556 - val_accuracy: 0.8635\n",
      "Epoch 321/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9569\n",
      "Epoch 321: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2156 - accuracy: 0.9569 - val_loss: 0.6511 - val_accuracy: 0.8565\n",
      "Epoch 322/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9554\n",
      "Epoch 322: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 278ms/step - loss: 0.2169 - accuracy: 0.9554 - val_loss: 0.6862 - val_accuracy: 0.8485\n",
      "Epoch 323/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9562\n",
      "Epoch 323: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2147 - accuracy: 0.9562 - val_loss: 0.6550 - val_accuracy: 0.8570\n",
      "Epoch 324/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9538\n",
      "Epoch 324: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2215 - accuracy: 0.9538 - val_loss: 0.6195 - val_accuracy: 0.8585\n",
      "Epoch 325/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9499\n",
      "Epoch 325: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2244 - accuracy: 0.9499 - val_loss: 0.6150 - val_accuracy: 0.8675\n",
      "Epoch 326/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9582\n",
      "Epoch 326: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2070 - accuracy: 0.9582 - val_loss: 0.6141 - val_accuracy: 0.8565\n",
      "Epoch 327/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9551\n",
      "Epoch 327: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2184 - accuracy: 0.9551 - val_loss: 0.7015 - val_accuracy: 0.8540\n",
      "Epoch 328/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9523\n",
      "Epoch 328: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2241 - accuracy: 0.9523 - val_loss: 0.6424 - val_accuracy: 0.8522\n",
      "Epoch 329/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9553\n",
      "Epoch 329: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2176 - accuracy: 0.9553 - val_loss: 0.6905 - val_accuracy: 0.8508\n",
      "Epoch 330/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9500\n",
      "Epoch 330: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2304 - accuracy: 0.9500 - val_loss: 0.6293 - val_accuracy: 0.8528\n",
      "Epoch 331/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9556\n",
      "Epoch 331: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2165 - accuracy: 0.9556 - val_loss: 0.6780 - val_accuracy: 0.8472\n",
      "Epoch 332/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9542\n",
      "Epoch 332: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2197 - accuracy: 0.9542 - val_loss: 0.6535 - val_accuracy: 0.8510\n",
      "Epoch 333/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9532\n",
      "Epoch 333: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2249 - accuracy: 0.9532 - val_loss: 0.6284 - val_accuracy: 0.8633\n",
      "Epoch 334/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9554\n",
      "Epoch 334: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2156 - accuracy: 0.9554 - val_loss: 0.6597 - val_accuracy: 0.8570\n",
      "Epoch 335/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9594\n",
      "Epoch 335: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2046 - accuracy: 0.9594 - val_loss: 0.7393 - val_accuracy: 0.8608\n",
      "Epoch 336/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9543\n",
      "Epoch 336: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2181 - accuracy: 0.9543 - val_loss: 0.6529 - val_accuracy: 0.8605\n",
      "Epoch 337/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9540\n",
      "Epoch 337: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2135 - accuracy: 0.9540 - val_loss: 0.6651 - val_accuracy: 0.8543\n",
      "Epoch 338/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9593\n",
      "Epoch 338: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2092 - accuracy: 0.9593 - val_loss: 0.9330 - val_accuracy: 0.8198\n",
      "Epoch 339/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9571\n",
      "Epoch 339: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2106 - accuracy: 0.9571 - val_loss: 0.6834 - val_accuracy: 0.8670\n",
      "Epoch 340/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9581\n",
      "Epoch 340: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2098 - accuracy: 0.9581 - val_loss: 0.5584 - val_accuracy: 0.8545\n",
      "Epoch 341/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9586\n",
      "Epoch 341: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2099 - accuracy: 0.9586 - val_loss: 0.6978 - val_accuracy: 0.8612\n",
      "Epoch 342/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9607\n",
      "Epoch 342: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2016 - accuracy: 0.9607 - val_loss: 0.8146 - val_accuracy: 0.8420\n",
      "Epoch 343/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9564\n",
      "Epoch 343: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2133 - accuracy: 0.9564 - val_loss: 0.5890 - val_accuracy: 0.8705\n",
      "Epoch 344/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9598\n",
      "Epoch 344: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2117 - accuracy: 0.9598 - val_loss: 0.7101 - val_accuracy: 0.8518\n",
      "Epoch 345/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9609\n",
      "Epoch 345: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2074 - accuracy: 0.9609 - val_loss: 0.7003 - val_accuracy: 0.8430\n",
      "Epoch 346/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9606\n",
      "Epoch 346: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2037 - accuracy: 0.9606 - val_loss: 0.6913 - val_accuracy: 0.8435\n",
      "Epoch 347/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9613\n",
      "Epoch 347: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2059 - accuracy: 0.9613 - val_loss: 0.6385 - val_accuracy: 0.8608\n",
      "Epoch 348/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9635\n",
      "Epoch 348: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1988 - accuracy: 0.9635 - val_loss: 0.7071 - val_accuracy: 0.8577\n",
      "Epoch 349/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9611\n",
      "Epoch 349: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2079 - accuracy: 0.9611 - val_loss: 0.7630 - val_accuracy: 0.8545\n",
      "Epoch 350/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9607\n",
      "Epoch 350: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2086 - accuracy: 0.9607 - val_loss: 0.6665 - val_accuracy: 0.8627\n",
      "Epoch 351/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9600\n",
      "Epoch 351: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2085 - accuracy: 0.9600 - val_loss: 0.6834 - val_accuracy: 0.8465\n",
      "Epoch 352/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9605\n",
      "Epoch 352: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2044 - accuracy: 0.9605 - val_loss: 0.6209 - val_accuracy: 0.8560\n",
      "Epoch 353/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9548\n",
      "Epoch 353: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2218 - accuracy: 0.9548 - val_loss: 0.6682 - val_accuracy: 0.8595\n",
      "Epoch 354/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9624\n",
      "Epoch 354: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2001 - accuracy: 0.9624 - val_loss: 0.6548 - val_accuracy: 0.8615\n",
      "Epoch 355/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9559\n",
      "Epoch 355: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2198 - accuracy: 0.9559 - val_loss: 0.6273 - val_accuracy: 0.8618\n",
      "Epoch 356/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9627\n",
      "Epoch 356: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2033 - accuracy: 0.9627 - val_loss: 0.7658 - val_accuracy: 0.8533\n",
      "Epoch 357/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9583\n",
      "Epoch 357: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2123 - accuracy: 0.9583 - val_loss: 0.6463 - val_accuracy: 0.8597\n",
      "Epoch 358/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9624\n",
      "Epoch 358: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2065 - accuracy: 0.9624 - val_loss: 0.6027 - val_accuracy: 0.8615\n",
      "Epoch 359/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9623\n",
      "Epoch 359: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2000 - accuracy: 0.9623 - val_loss: 0.7061 - val_accuracy: 0.8630\n",
      "Epoch 360/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9617\n",
      "Epoch 360: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2050 - accuracy: 0.9617 - val_loss: 0.7112 - val_accuracy: 0.8660\n",
      "Epoch 361/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9661\n",
      "Epoch 361: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1995 - accuracy: 0.9661 - val_loss: 0.6883 - val_accuracy: 0.8470\n",
      "Epoch 362/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9632\n",
      "Epoch 362: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2053 - accuracy: 0.9632 - val_loss: 0.6321 - val_accuracy: 0.8608\n",
      "Epoch 363/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9581\n",
      "Epoch 363: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2086 - accuracy: 0.9581 - val_loss: 0.6516 - val_accuracy: 0.8530\n",
      "Epoch 364/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9641\n",
      "Epoch 364: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1985 - accuracy: 0.9641 - val_loss: 0.8099 - val_accuracy: 0.8485\n",
      "Epoch 365/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9583\n",
      "Epoch 365: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2134 - accuracy: 0.9583 - val_loss: 0.7601 - val_accuracy: 0.8405\n",
      "Epoch 366/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9647\n",
      "Epoch 366: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2031 - accuracy: 0.9647 - val_loss: 0.6578 - val_accuracy: 0.8622\n",
      "Epoch 367/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9605\n",
      "Epoch 367: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2051 - accuracy: 0.9605 - val_loss: 0.7548 - val_accuracy: 0.8568\n",
      "Epoch 368/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9625\n",
      "Epoch 368: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2071 - accuracy: 0.9625 - val_loss: 0.7283 - val_accuracy: 0.8545\n",
      "Epoch 369/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9638\n",
      "Epoch 369: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1993 - accuracy: 0.9638 - val_loss: 0.6977 - val_accuracy: 0.8395\n",
      "Epoch 370/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9637\n",
      "Epoch 370: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2039 - accuracy: 0.9637 - val_loss: 0.6594 - val_accuracy: 0.8618\n",
      "Epoch 371/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9620\n",
      "Epoch 371: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2065 - accuracy: 0.9620 - val_loss: 0.6435 - val_accuracy: 0.8580\n",
      "Epoch 372/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9671\n",
      "Epoch 372: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1961 - accuracy: 0.9671 - val_loss: 0.7080 - val_accuracy: 0.8508\n",
      "Epoch 373/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9644\n",
      "Epoch 373: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2022 - accuracy: 0.9644 - val_loss: 0.6164 - val_accuracy: 0.8650\n",
      "Epoch 374/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9677\n",
      "Epoch 374: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1962 - accuracy: 0.9677 - val_loss: 0.8550 - val_accuracy: 0.8495\n",
      "Epoch 375/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9637\n",
      "Epoch 375: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2032 - accuracy: 0.9637 - val_loss: 0.7570 - val_accuracy: 0.8395\n",
      "Epoch 376/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9679\n",
      "Epoch 376: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1973 - accuracy: 0.9679 - val_loss: 0.7066 - val_accuracy: 0.8612\n",
      "Epoch 377/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9675\n",
      "Epoch 377: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1987 - accuracy: 0.9675 - val_loss: 0.7355 - val_accuracy: 0.8432\n",
      "Epoch 378/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9638\n",
      "Epoch 378: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2048 - accuracy: 0.9638 - val_loss: 0.6593 - val_accuracy: 0.8710\n",
      "Epoch 379/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9663\n",
      "Epoch 379: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1943 - accuracy: 0.9663 - val_loss: 0.7567 - val_accuracy: 0.8435\n",
      "Epoch 380/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9643\n",
      "Epoch 380: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2007 - accuracy: 0.9643 - val_loss: 0.6572 - val_accuracy: 0.8577\n",
      "Epoch 381/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9679\n",
      "Epoch 381: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1943 - accuracy: 0.9679 - val_loss: 0.7304 - val_accuracy: 0.8510\n",
      "Epoch 382/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9607\n",
      "Epoch 382: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2113 - accuracy: 0.9607 - val_loss: 0.6726 - val_accuracy: 0.8593\n",
      "Epoch 383/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9642\n",
      "Epoch 383: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2043 - accuracy: 0.9642 - val_loss: 0.6979 - val_accuracy: 0.8415\n",
      "Epoch 384/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9639\n",
      "Epoch 384: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2035 - accuracy: 0.9639 - val_loss: 0.7544 - val_accuracy: 0.8443\n",
      "Epoch 385/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9649\n",
      "Epoch 385: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2035 - accuracy: 0.9649 - val_loss: 0.6590 - val_accuracy: 0.8640\n",
      "Epoch 386/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9629\n",
      "Epoch 386: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2064 - accuracy: 0.9629 - val_loss: 0.7110 - val_accuracy: 0.8558\n",
      "Epoch 387/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9677\n",
      "Epoch 387: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1970 - accuracy: 0.9677 - val_loss: 0.6913 - val_accuracy: 0.8612\n",
      "Epoch 388/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9648\n",
      "Epoch 388: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2046 - accuracy: 0.9648 - val_loss: 0.6834 - val_accuracy: 0.8583\n",
      "Epoch 389/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9665\n",
      "Epoch 389: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1983 - accuracy: 0.9665 - val_loss: 0.6353 - val_accuracy: 0.8460\n",
      "Epoch 390/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9635\n",
      "Epoch 390: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2041 - accuracy: 0.9635 - val_loss: 0.7509 - val_accuracy: 0.8415\n",
      "Epoch 391/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9613\n",
      "Epoch 391: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2102 - accuracy: 0.9613 - val_loss: 0.6635 - val_accuracy: 0.8505\n",
      "Epoch 392/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9682\n",
      "Epoch 392: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1970 - accuracy: 0.9682 - val_loss: 0.7228 - val_accuracy: 0.8568\n",
      "Epoch 393/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9658\n",
      "Epoch 393: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2036 - accuracy: 0.9658 - val_loss: 0.8013 - val_accuracy: 0.8407\n",
      "Epoch 394/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9664\n",
      "Epoch 394: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1996 - accuracy: 0.9664 - val_loss: 0.6770 - val_accuracy: 0.8577\n",
      "Epoch 395/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9684\n",
      "Epoch 395: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1926 - accuracy: 0.9684 - val_loss: 0.8773 - val_accuracy: 0.8288\n",
      "Epoch 396/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9651\n",
      "Epoch 396: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2055 - accuracy: 0.9651 - val_loss: 0.6900 - val_accuracy: 0.8533\n",
      "Epoch 397/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9682\n",
      "Epoch 397: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1986 - accuracy: 0.9682 - val_loss: 0.7551 - val_accuracy: 0.8515\n",
      "Epoch 398/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9673\n",
      "Epoch 398: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1990 - accuracy: 0.9673 - val_loss: 0.6737 - val_accuracy: 0.8585\n",
      "Epoch 399/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9688\n",
      "Epoch 399: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1965 - accuracy: 0.9688 - val_loss: 0.6944 - val_accuracy: 0.8545\n",
      "Epoch 400/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9722\n",
      "Epoch 400: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1894 - accuracy: 0.9722 - val_loss: 0.6979 - val_accuracy: 0.8562\n",
      "Epoch 401/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9694\n",
      "Epoch 401: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1972 - accuracy: 0.9694 - val_loss: 0.7740 - val_accuracy: 0.8525\n",
      "Epoch 402/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9674\n",
      "Epoch 402: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2005 - accuracy: 0.9674 - val_loss: 0.6211 - val_accuracy: 0.8555\n",
      "Epoch 403/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9710\n",
      "Epoch 403: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1877 - accuracy: 0.9710 - val_loss: 0.7948 - val_accuracy: 0.8610\n",
      "Epoch 404/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9733\n",
      "Epoch 404: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1862 - accuracy: 0.9733 - val_loss: 0.8176 - val_accuracy: 0.8443\n",
      "Epoch 405/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9675\n",
      "Epoch 405: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2000 - accuracy: 0.9675 - val_loss: 0.7016 - val_accuracy: 0.8568\n",
      "Epoch 406/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9708\n",
      "Epoch 406: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1926 - accuracy: 0.9708 - val_loss: 0.6267 - val_accuracy: 0.8608\n",
      "Epoch 407/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9681\n",
      "Epoch 407: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1983 - accuracy: 0.9681 - val_loss: 0.7468 - val_accuracy: 0.8457\n",
      "Epoch 408/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9704\n",
      "Epoch 408: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1929 - accuracy: 0.9704 - val_loss: 0.6864 - val_accuracy: 0.8575\n",
      "Epoch 409/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9735\n",
      "Epoch 409: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1835 - accuracy: 0.9735 - val_loss: 0.7609 - val_accuracy: 0.8503\n",
      "Epoch 410/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9713\n",
      "Epoch 410: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1926 - accuracy: 0.9713 - val_loss: 0.7085 - val_accuracy: 0.8593\n",
      "Epoch 411/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9709\n",
      "Epoch 411: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1891 - accuracy: 0.9709 - val_loss: 0.9292 - val_accuracy: 0.8142\n",
      "Epoch 412/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9699\n",
      "Epoch 412: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 429s 286ms/step - loss: 0.1930 - accuracy: 0.9699 - val_loss: 0.7233 - val_accuracy: 0.8550\n",
      "Epoch 413/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9707\n",
      "Epoch 413: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1912 - accuracy: 0.9707 - val_loss: 0.6810 - val_accuracy: 0.8677\n",
      "Epoch 414/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9730\n",
      "Epoch 414: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1898 - accuracy: 0.9730 - val_loss: 0.8072 - val_accuracy: 0.8472\n",
      "Epoch 415/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9703\n",
      "Epoch 415: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1927 - accuracy: 0.9703 - val_loss: 0.7218 - val_accuracy: 0.8593\n",
      "Epoch 416/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9707\n",
      "Epoch 416: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1933 - accuracy: 0.9707 - val_loss: 0.8059 - val_accuracy: 0.8447\n",
      "Epoch 417/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9747\n",
      "Epoch 417: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1807 - accuracy: 0.9747 - val_loss: 0.6920 - val_accuracy: 0.8528\n",
      "Epoch 418/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9738\n",
      "Epoch 418: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1887 - accuracy: 0.9738 - val_loss: 0.6773 - val_accuracy: 0.8625\n",
      "Epoch 419/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9704\n",
      "Epoch 419: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1940 - accuracy: 0.9704 - val_loss: 0.6958 - val_accuracy: 0.8593\n",
      "Epoch 420/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9758\n",
      "Epoch 420: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1836 - accuracy: 0.9758 - val_loss: 0.8324 - val_accuracy: 0.8322\n",
      "Epoch 421/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9704\n",
      "Epoch 421: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1901 - accuracy: 0.9704 - val_loss: 0.7821 - val_accuracy: 0.8543\n",
      "Epoch 422/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9718\n",
      "Epoch 422: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1922 - accuracy: 0.9718 - val_loss: 0.7436 - val_accuracy: 0.8508\n",
      "Epoch 423/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9712\n",
      "Epoch 423: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1918 - accuracy: 0.9712 - val_loss: 0.7627 - val_accuracy: 0.8495\n",
      "Epoch 424/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.9702\n",
      "Epoch 424: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1907 - accuracy: 0.9702 - val_loss: 0.8213 - val_accuracy: 0.8485\n",
      "Epoch 425/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9713\n",
      "Epoch 425: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1924 - accuracy: 0.9713 - val_loss: 0.7240 - val_accuracy: 0.8400\n",
      "Epoch 426/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9754\n",
      "Epoch 426: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1820 - accuracy: 0.9754 - val_loss: 0.7149 - val_accuracy: 0.8547\n",
      "Epoch 427/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9762\n",
      "Epoch 427: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1823 - accuracy: 0.9762 - val_loss: 0.8195 - val_accuracy: 0.8535\n",
      "Epoch 428/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9738\n",
      "Epoch 428: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1874 - accuracy: 0.9738 - val_loss: 0.7562 - val_accuracy: 0.8558\n",
      "Epoch 429/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9743\n",
      "Epoch 429: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1874 - accuracy: 0.9743 - val_loss: 0.7695 - val_accuracy: 0.8570\n",
      "Epoch 430/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9727\n",
      "Epoch 430: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1877 - accuracy: 0.9727 - val_loss: 0.6922 - val_accuracy: 0.8662\n",
      "Epoch 431/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9723\n",
      "Epoch 431: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1889 - accuracy: 0.9723 - val_loss: 0.6243 - val_accuracy: 0.8695\n",
      "Epoch 432/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9735\n",
      "Epoch 432: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1894 - accuracy: 0.9735 - val_loss: 0.6620 - val_accuracy: 0.8612\n",
      "Epoch 433/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9770\n",
      "Epoch 433: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1807 - accuracy: 0.9770 - val_loss: 0.7555 - val_accuracy: 0.8545\n",
      "Epoch 434/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9735\n",
      "Epoch 434: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1870 - accuracy: 0.9735 - val_loss: 0.7842 - val_accuracy: 0.8480\n",
      "Epoch 435/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9778\n",
      "Epoch 435: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1834 - accuracy: 0.9778 - val_loss: 0.7673 - val_accuracy: 0.8385\n",
      "Epoch 436/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9734\n",
      "Epoch 436: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1877 - accuracy: 0.9734 - val_loss: 0.6987 - val_accuracy: 0.8380\n",
      "Epoch 437/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9742\n",
      "Epoch 437: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1856 - accuracy: 0.9742 - val_loss: 0.7242 - val_accuracy: 0.8533\n",
      "Epoch 438/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9735\n",
      "Epoch 438: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1866 - accuracy: 0.9735 - val_loss: 0.6961 - val_accuracy: 0.8610\n",
      "Epoch 439/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9778\n",
      "Epoch 439: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1761 - accuracy: 0.9778 - val_loss: 0.8130 - val_accuracy: 0.8583\n",
      "Epoch 440/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9729\n",
      "Epoch 440: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1906 - accuracy: 0.9729 - val_loss: 0.6247 - val_accuracy: 0.8590\n",
      "Epoch 441/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9715\n",
      "Epoch 441: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1923 - accuracy: 0.9715 - val_loss: 0.6649 - val_accuracy: 0.8540\n",
      "Epoch 442/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9762\n",
      "Epoch 442: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1777 - accuracy: 0.9762 - val_loss: 0.7370 - val_accuracy: 0.8490\n",
      "Epoch 443/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9707\n",
      "Epoch 443: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1959 - accuracy: 0.9707 - val_loss: 0.6570 - val_accuracy: 0.8568\n",
      "Epoch 444/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9718\n",
      "Epoch 444: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1903 - accuracy: 0.9718 - val_loss: 0.7209 - val_accuracy: 0.8622\n",
      "Epoch 445/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9739\n",
      "Epoch 445: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1864 - accuracy: 0.9739 - val_loss: 0.6791 - val_accuracy: 0.8535\n",
      "Epoch 446/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9712\n",
      "Epoch 446: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1885 - accuracy: 0.9712 - val_loss: 0.6040 - val_accuracy: 0.8565\n",
      "Epoch 447/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9745\n",
      "Epoch 447: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1844 - accuracy: 0.9745 - val_loss: 0.6710 - val_accuracy: 0.8612\n",
      "Epoch 448/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9753\n",
      "Epoch 448: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1808 - accuracy: 0.9753 - val_loss: 0.7035 - val_accuracy: 0.8625\n",
      "Epoch 449/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9761\n",
      "Epoch 449: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1856 - accuracy: 0.9761 - val_loss: 0.6927 - val_accuracy: 0.8620\n",
      "Epoch 450/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9759\n",
      "Epoch 450: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1849 - accuracy: 0.9759 - val_loss: 0.6611 - val_accuracy: 0.8637\n",
      "Epoch 451/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9784\n",
      "Epoch 451: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1760 - accuracy: 0.9784 - val_loss: 0.7806 - val_accuracy: 0.8558\n",
      "Epoch 452/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9760\n",
      "Epoch 452: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1819 - accuracy: 0.9760 - val_loss: 0.6905 - val_accuracy: 0.8472\n",
      "Epoch 453/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9755\n",
      "Epoch 453: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1862 - accuracy: 0.9755 - val_loss: 0.7122 - val_accuracy: 0.8495\n",
      "Epoch 454/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9758\n",
      "Epoch 454: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1843 - accuracy: 0.9758 - val_loss: 0.7548 - val_accuracy: 0.8537\n",
      "Epoch 455/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9753\n",
      "Epoch 455: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1864 - accuracy: 0.9753 - val_loss: 0.7276 - val_accuracy: 0.8443\n",
      "Epoch 456/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9772\n",
      "Epoch 456: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1782 - accuracy: 0.9772 - val_loss: 0.7605 - val_accuracy: 0.8528\n",
      "Epoch 457/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9787\n",
      "Epoch 457: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1740 - accuracy: 0.9787 - val_loss: 0.7633 - val_accuracy: 0.8438\n",
      "Epoch 458/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9744\n",
      "Epoch 458: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1905 - accuracy: 0.9744 - val_loss: 0.6670 - val_accuracy: 0.8555\n",
      "Epoch 459/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9762\n",
      "Epoch 459: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1766 - accuracy: 0.9762 - val_loss: 0.6757 - val_accuracy: 0.8560\n",
      "Epoch 460/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9740\n",
      "Epoch 460: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1897 - accuracy: 0.9740 - val_loss: 0.6718 - val_accuracy: 0.8575\n",
      "Epoch 461/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9774\n",
      "Epoch 461: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 280ms/step - loss: 0.1822 - accuracy: 0.9774 - val_loss: 0.6352 - val_accuracy: 0.8570\n",
      "Epoch 462/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9739\n",
      "Epoch 462: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1870 - accuracy: 0.9739 - val_loss: 0.6473 - val_accuracy: 0.8590\n",
      "Epoch 463/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9739\n",
      "Epoch 463: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 272ms/step - loss: 0.1865 - accuracy: 0.9739 - val_loss: 0.6107 - val_accuracy: 0.8587\n",
      "Epoch 464/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9789\n",
      "Epoch 464: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1771 - accuracy: 0.9789 - val_loss: 0.6892 - val_accuracy: 0.8453\n",
      "Epoch 465/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9747\n",
      "Epoch 465: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1837 - accuracy: 0.9747 - val_loss: 0.7545 - val_accuracy: 0.8530\n",
      "Epoch 466/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9758\n",
      "Epoch 466: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1827 - accuracy: 0.9758 - val_loss: 0.6576 - val_accuracy: 0.8633\n",
      "Epoch 467/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9772\n",
      "Epoch 467: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1820 - accuracy: 0.9772 - val_loss: 0.7039 - val_accuracy: 0.8662\n",
      "Epoch 468/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9787\n",
      "Epoch 468: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1753 - accuracy: 0.9787 - val_loss: 0.7173 - val_accuracy: 0.8530\n",
      "Epoch 469/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9799\n",
      "Epoch 469: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1786 - accuracy: 0.9799 - val_loss: 0.7225 - val_accuracy: 0.8590\n",
      "Epoch 470/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9803\n",
      "Epoch 470: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1712 - accuracy: 0.9803 - val_loss: 0.7782 - val_accuracy: 0.8490\n",
      "Epoch 471/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9811\n",
      "Epoch 471: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1716 - accuracy: 0.9811 - val_loss: 0.7389 - val_accuracy: 0.8528\n",
      "Epoch 472/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9770\n",
      "Epoch 472: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1789 - accuracy: 0.9770 - val_loss: 0.7211 - val_accuracy: 0.8618\n",
      "Epoch 473/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9774\n",
      "Epoch 473: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1785 - accuracy: 0.9774 - val_loss: 0.7503 - val_accuracy: 0.8610\n",
      "Epoch 474/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9802\n",
      "Epoch 474: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1740 - accuracy: 0.9802 - val_loss: 0.9829 - val_accuracy: 0.8100\n",
      "Epoch 475/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9768\n",
      "Epoch 475: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1814 - accuracy: 0.9768 - val_loss: 0.7151 - val_accuracy: 0.8490\n",
      "Epoch 476/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9797\n",
      "Epoch 476: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1734 - accuracy: 0.9797 - val_loss: 0.7753 - val_accuracy: 0.8460\n",
      "Epoch 477/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9768\n",
      "Epoch 477: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1808 - accuracy: 0.9768 - val_loss: 0.7006 - val_accuracy: 0.8490\n",
      "Epoch 478/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9810\n",
      "Epoch 478: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1729 - accuracy: 0.9810 - val_loss: 0.7504 - val_accuracy: 0.8560\n",
      "Epoch 479/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9812\n",
      "Epoch 479: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1717 - accuracy: 0.9812 - val_loss: 0.8326 - val_accuracy: 0.8450\n",
      "Epoch 480/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9788\n",
      "Epoch 480: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1770 - accuracy: 0.9788 - val_loss: 0.7274 - val_accuracy: 0.8300\n",
      "Epoch 481/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9780\n",
      "Epoch 481: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1754 - accuracy: 0.9780 - val_loss: 1.1286 - val_accuracy: 0.7793\n",
      "Epoch 482/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9824\n",
      "Epoch 482: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1682 - accuracy: 0.9824 - val_loss: 0.7250 - val_accuracy: 0.8568\n",
      "Epoch 483/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9835\n",
      "Epoch 483: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1664 - accuracy: 0.9835 - val_loss: 0.6903 - val_accuracy: 0.8512\n",
      "Epoch 484/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9827\n",
      "Epoch 484: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1686 - accuracy: 0.9827 - val_loss: 0.7484 - val_accuracy: 0.8465\n",
      "Epoch 485/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9778\n",
      "Epoch 485: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1768 - accuracy: 0.9778 - val_loss: 0.7484 - val_accuracy: 0.8577\n",
      "Epoch 486/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9784\n",
      "Epoch 486: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1754 - accuracy: 0.9784 - val_loss: 0.7431 - val_accuracy: 0.8425\n",
      "Epoch 487/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9791\n",
      "Epoch 487: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1808 - accuracy: 0.9791 - val_loss: 0.6413 - val_accuracy: 0.8457\n",
      "Epoch 488/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9809\n",
      "Epoch 488: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1718 - accuracy: 0.9809 - val_loss: 0.7293 - val_accuracy: 0.8640\n",
      "Epoch 489/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9812\n",
      "Epoch 489: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1731 - accuracy: 0.9812 - val_loss: 0.7044 - val_accuracy: 0.8610\n",
      "Epoch 490/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9793\n",
      "Epoch 490: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1734 - accuracy: 0.9793 - val_loss: 0.6976 - val_accuracy: 0.8618\n",
      "Epoch 491/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9811\n",
      "Epoch 491: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1727 - accuracy: 0.9811 - val_loss: 0.7102 - val_accuracy: 0.8580\n",
      "Epoch 492/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9827\n",
      "Epoch 492: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1673 - accuracy: 0.9827 - val_loss: 0.8686 - val_accuracy: 0.8465\n",
      "Epoch 493/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9768\n",
      "Epoch 493: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 413s 275ms/step - loss: 0.1793 - accuracy: 0.9768 - val_loss: 0.6923 - val_accuracy: 0.8547\n",
      "Epoch 494/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9783\n",
      "Epoch 494: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1798 - accuracy: 0.9783 - val_loss: 0.6460 - val_accuracy: 0.8575\n",
      "Epoch 495/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9828\n",
      "Epoch 495: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1679 - accuracy: 0.9828 - val_loss: 0.7282 - val_accuracy: 0.8508\n",
      "Epoch 496/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9838\n",
      "Epoch 496: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1644 - accuracy: 0.9838 - val_loss: 0.7651 - val_accuracy: 0.8520\n",
      "Epoch 497/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9811\n",
      "Epoch 497: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1717 - accuracy: 0.9811 - val_loss: 0.7070 - val_accuracy: 0.8515\n",
      "Epoch 498/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9747\n",
      "Epoch 498: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1868 - accuracy: 0.9747 - val_loss: 0.7503 - val_accuracy: 0.8357\n",
      "Epoch 499/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9818\n",
      "Epoch 499: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1700 - accuracy: 0.9818 - val_loss: 0.7142 - val_accuracy: 0.8478\n",
      "Epoch 500/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9858\n",
      "Epoch 500: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1597 - accuracy: 0.9858 - val_loss: 0.7094 - val_accuracy: 0.8485\n",
      "Epoch 501/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9826\n",
      "Epoch 501: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1675 - accuracy: 0.9826 - val_loss: 0.7016 - val_accuracy: 0.8465\n",
      "Epoch 502/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9808\n",
      "Epoch 502: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1704 - accuracy: 0.9808 - val_loss: 0.6868 - val_accuracy: 0.8475\n",
      "Epoch 503/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9826\n",
      "Epoch 503: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1683 - accuracy: 0.9826 - val_loss: 0.7138 - val_accuracy: 0.8637\n",
      "Epoch 504/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9790\n",
      "Epoch 504: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1746 - accuracy: 0.9790 - val_loss: 0.7191 - val_accuracy: 0.8460\n",
      "Epoch 505/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9818\n",
      "Epoch 505: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1656 - accuracy: 0.9818 - val_loss: 0.8013 - val_accuracy: 0.8562\n",
      "Epoch 506/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9769\n",
      "Epoch 506: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1816 - accuracy: 0.9769 - val_loss: 0.6559 - val_accuracy: 0.8575\n",
      "Epoch 507/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9768\n",
      "Epoch 507: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1813 - accuracy: 0.9768 - val_loss: 0.7657 - val_accuracy: 0.8580\n",
      "Epoch 508/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9814\n",
      "Epoch 508: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1681 - accuracy: 0.9814 - val_loss: 0.6756 - val_accuracy: 0.8500\n",
      "Epoch 509/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9792\n",
      "Epoch 509: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1719 - accuracy: 0.9792 - val_loss: 0.7482 - val_accuracy: 0.8465\n",
      "Epoch 510/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1829 - accuracy: 0.9769\n",
      "Epoch 510: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1829 - accuracy: 0.9769 - val_loss: 0.7012 - val_accuracy: 0.8595\n",
      "Epoch 511/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9823\n",
      "Epoch 511: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1651 - accuracy: 0.9823 - val_loss: 0.7020 - val_accuracy: 0.8535\n",
      "Epoch 512/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9824\n",
      "Epoch 512: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1695 - accuracy: 0.9824 - val_loss: 0.6583 - val_accuracy: 0.8568\n",
      "Epoch 513/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9839\n",
      "Epoch 513: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1635 - accuracy: 0.9839 - val_loss: 0.7722 - val_accuracy: 0.8390\n",
      "Epoch 514/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9791\n",
      "Epoch 514: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1765 - accuracy: 0.9791 - val_loss: 0.7179 - val_accuracy: 0.8540\n",
      "Epoch 515/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9797\n",
      "Epoch 515: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1737 - accuracy: 0.9797 - val_loss: 0.7262 - val_accuracy: 0.8295\n",
      "Epoch 516/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9858\n",
      "Epoch 516: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1594 - accuracy: 0.9858 - val_loss: 0.8533 - val_accuracy: 0.8335\n",
      "Epoch 517/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9792\n",
      "Epoch 517: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1771 - accuracy: 0.9792 - val_loss: 0.6983 - val_accuracy: 0.8618\n",
      "Epoch 518/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9814\n",
      "Epoch 518: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1702 - accuracy: 0.9814 - val_loss: 0.7153 - val_accuracy: 0.8537\n",
      "Epoch 519/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9826\n",
      "Epoch 519: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1679 - accuracy: 0.9826 - val_loss: 0.6712 - val_accuracy: 0.8545\n",
      "Epoch 520/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9818\n",
      "Epoch 520: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1679 - accuracy: 0.9818 - val_loss: 0.8289 - val_accuracy: 0.8205\n",
      "Epoch 521/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9769\n",
      "Epoch 521: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1819 - accuracy: 0.9769 - val_loss: 0.6313 - val_accuracy: 0.8535\n",
      "Epoch 522/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9829\n",
      "Epoch 522: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1658 - accuracy: 0.9829 - val_loss: 0.9441 - val_accuracy: 0.8070\n",
      "Epoch 523/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9831\n",
      "Epoch 523: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1665 - accuracy: 0.9831 - val_loss: 0.7636 - val_accuracy: 0.8468\n",
      "Epoch 524/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9803\n",
      "Epoch 524: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1724 - accuracy: 0.9803 - val_loss: 0.7515 - val_accuracy: 0.8547\n",
      "Epoch 525/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9793\n",
      "Epoch 525: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1733 - accuracy: 0.9793 - val_loss: 0.7146 - val_accuracy: 0.8420\n",
      "Epoch 526/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9803\n",
      "Epoch 526: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1730 - accuracy: 0.9803 - val_loss: 0.6999 - val_accuracy: 0.8537\n",
      "Epoch 527/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9859\n",
      "Epoch 527: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1577 - accuracy: 0.9859 - val_loss: 0.8528 - val_accuracy: 0.8390\n",
      "Epoch 528/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9848\n",
      "Epoch 528: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1589 - accuracy: 0.9848 - val_loss: 0.7390 - val_accuracy: 0.8468\n",
      "Epoch 529/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9825\n",
      "Epoch 529: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1680 - accuracy: 0.9825 - val_loss: 0.7316 - val_accuracy: 0.8505\n",
      "Epoch 530/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9804\n",
      "Epoch 530: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1713 - accuracy: 0.9804 - val_loss: 0.7554 - val_accuracy: 0.8508\n",
      "Epoch 531/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9824\n",
      "Epoch 531: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1673 - accuracy: 0.9824 - val_loss: 0.7100 - val_accuracy: 0.8460\n",
      "Epoch 532/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9818\n",
      "Epoch 532: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1674 - accuracy: 0.9818 - val_loss: 0.6600 - val_accuracy: 0.8580\n",
      "Epoch 533/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9821\n",
      "Epoch 533: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1692 - accuracy: 0.9821 - val_loss: 0.6807 - val_accuracy: 0.8462\n",
      "Epoch 534/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9842\n",
      "Epoch 534: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1637 - accuracy: 0.9842 - val_loss: 0.6892 - val_accuracy: 0.8577\n",
      "Epoch 535/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9880\n",
      "Epoch 535: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1553 - accuracy: 0.9880 - val_loss: 0.6787 - val_accuracy: 0.8720\n",
      "Epoch 536/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9840\n",
      "Epoch 536: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 272ms/step - loss: 0.1622 - accuracy: 0.9840 - val_loss: 0.7008 - val_accuracy: 0.8655\n",
      "Epoch 537/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9858\n",
      "Epoch 537: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1559 - accuracy: 0.9858 - val_loss: 0.8515 - val_accuracy: 0.8465\n",
      "Epoch 538/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9849\n",
      "Epoch 538: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1611 - accuracy: 0.9849 - val_loss: 0.7133 - val_accuracy: 0.8465\n",
      "Epoch 539/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9867\n",
      "Epoch 539: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1557 - accuracy: 0.9867 - val_loss: 0.7769 - val_accuracy: 0.8535\n",
      "Epoch 540/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9855\n",
      "Epoch 540: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1586 - accuracy: 0.9855 - val_loss: 0.7233 - val_accuracy: 0.8520\n",
      "Epoch 541/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9850\n",
      "Epoch 541: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1601 - accuracy: 0.9850 - val_loss: 0.8944 - val_accuracy: 0.8300\n",
      "Epoch 542/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9825\n",
      "Epoch 542: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1646 - accuracy: 0.9825 - val_loss: 0.9421 - val_accuracy: 0.8303\n",
      "Epoch 543/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9832\n",
      "Epoch 543: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1642 - accuracy: 0.9832 - val_loss: 0.7192 - val_accuracy: 0.8440\n",
      "Epoch 544/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9842\n",
      "Epoch 544: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1619 - accuracy: 0.9842 - val_loss: 0.7877 - val_accuracy: 0.8482\n",
      "Epoch 545/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9829\n",
      "Epoch 545: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1658 - accuracy: 0.9829 - val_loss: 0.6823 - val_accuracy: 0.8465\n",
      "Epoch 546/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9859\n",
      "Epoch 546: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 270ms/step - loss: 0.1582 - accuracy: 0.9859 - val_loss: 0.7150 - val_accuracy: 0.8345\n",
      "Epoch 547/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9846\n",
      "Epoch 547: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1593 - accuracy: 0.9846 - val_loss: 0.7113 - val_accuracy: 0.8558\n",
      "Epoch 548/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9847\n",
      "Epoch 548: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1602 - accuracy: 0.9847 - val_loss: 0.7366 - val_accuracy: 0.8587\n",
      "Epoch 549/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9846\n",
      "Epoch 549: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1611 - accuracy: 0.9846 - val_loss: 0.8195 - val_accuracy: 0.8372\n",
      "Epoch 550/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9837\n",
      "Epoch 550: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1610 - accuracy: 0.9837 - val_loss: 0.6897 - val_accuracy: 0.8550\n",
      "Epoch 551/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9867\n",
      "Epoch 551: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1575 - accuracy: 0.9867 - val_loss: 0.7319 - val_accuracy: 0.8497\n",
      "Epoch 552/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9871\n",
      "Epoch 552: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1543 - accuracy: 0.9871 - val_loss: 0.9206 - val_accuracy: 0.8338\n",
      "Epoch 553/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9849\n",
      "Epoch 553: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1598 - accuracy: 0.9849 - val_loss: 0.6658 - val_accuracy: 0.8543\n",
      "Epoch 554/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9833\n",
      "Epoch 554: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1629 - accuracy: 0.9833 - val_loss: 0.8609 - val_accuracy: 0.8435\n",
      "Epoch 555/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9848\n",
      "Epoch 555: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1573 - accuracy: 0.9848 - val_loss: 0.7729 - val_accuracy: 0.8472\n",
      "Epoch 556/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9844\n",
      "Epoch 556: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1608 - accuracy: 0.9844 - val_loss: 0.7692 - val_accuracy: 0.8372\n",
      "Epoch 557/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9848\n",
      "Epoch 557: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1571 - accuracy: 0.9848 - val_loss: 0.7468 - val_accuracy: 0.8320\n",
      "Epoch 558/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9857\n",
      "Epoch 558: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1579 - accuracy: 0.9857 - val_loss: 0.7133 - val_accuracy: 0.8340\n",
      "Epoch 559/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9837\n",
      "Epoch 559: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1632 - accuracy: 0.9837 - val_loss: 0.6778 - val_accuracy: 0.8583\n",
      "Epoch 560/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9830\n",
      "Epoch 560: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1617 - accuracy: 0.9830 - val_loss: 0.8141 - val_accuracy: 0.8472\n",
      "Epoch 561/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9843\n",
      "Epoch 561: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1624 - accuracy: 0.9843 - val_loss: 0.7871 - val_accuracy: 0.8480\n",
      "Epoch 562/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9840\n",
      "Epoch 562: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1599 - accuracy: 0.9840 - val_loss: 0.7321 - val_accuracy: 0.8510\n",
      "Epoch 563/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9843\n",
      "Epoch 563: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1602 - accuracy: 0.9843 - val_loss: 0.7129 - val_accuracy: 0.8497\n",
      "Epoch 564/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9839\n",
      "Epoch 564: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1598 - accuracy: 0.9839 - val_loss: 0.6895 - val_accuracy: 0.8590\n",
      "Epoch 565/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9864\n",
      "Epoch 565: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1584 - accuracy: 0.9864 - val_loss: 0.7096 - val_accuracy: 0.8583\n",
      "Epoch 566/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9855\n",
      "Epoch 566: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1553 - accuracy: 0.9855 - val_loss: 0.6941 - val_accuracy: 0.8593\n",
      "Epoch 567/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9862\n",
      "Epoch 567: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1580 - accuracy: 0.9862 - val_loss: 0.7767 - val_accuracy: 0.8522\n",
      "Epoch 568/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9859\n",
      "Epoch 568: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1569 - accuracy: 0.9859 - val_loss: 0.7797 - val_accuracy: 0.8453\n",
      "Epoch 569/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9834\n",
      "Epoch 569: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1617 - accuracy: 0.9834 - val_loss: 0.7255 - val_accuracy: 0.8612\n",
      "Epoch 570/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9859\n",
      "Epoch 570: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1575 - accuracy: 0.9859 - val_loss: 0.7562 - val_accuracy: 0.8425\n",
      "Epoch 571/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9843\n",
      "Epoch 571: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1606 - accuracy: 0.9843 - val_loss: 0.9028 - val_accuracy: 0.8165\n",
      "Epoch 572/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9808\n",
      "Epoch 572: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1669 - accuracy: 0.9808 - val_loss: 0.6924 - val_accuracy: 0.8482\n",
      "Epoch 573/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9855\n",
      "Epoch 573: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1577 - accuracy: 0.9855 - val_loss: 0.9698 - val_accuracy: 0.7975\n",
      "Epoch 574/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9827\n",
      "Epoch 574: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1631 - accuracy: 0.9827 - val_loss: 0.8960 - val_accuracy: 0.8280\n",
      "Epoch 575/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9852\n",
      "Epoch 575: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1578 - accuracy: 0.9852 - val_loss: 0.7301 - val_accuracy: 0.8608\n",
      "Epoch 576/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9893\n",
      "Epoch 576: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1475 - accuracy: 0.9893 - val_loss: 0.7837 - val_accuracy: 0.8470\n",
      "Epoch 577/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9845\n",
      "Epoch 577: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1608 - accuracy: 0.9845 - val_loss: 0.7311 - val_accuracy: 0.8528\n",
      "Epoch 578/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9835\n",
      "Epoch 578: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1625 - accuracy: 0.9835 - val_loss: 0.7931 - val_accuracy: 0.8438\n",
      "Epoch 579/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9880\n",
      "Epoch 579: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1525 - accuracy: 0.9880 - val_loss: 0.7752 - val_accuracy: 0.8510\n",
      "Epoch 580/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9835\n",
      "Epoch 580: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1608 - accuracy: 0.9835 - val_loss: 0.6925 - val_accuracy: 0.8520\n",
      "Epoch 581/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9845\n",
      "Epoch 581: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1598 - accuracy: 0.9845 - val_loss: 0.7284 - val_accuracy: 0.8393\n",
      "Epoch 582/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9855\n",
      "Epoch 582: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1568 - accuracy: 0.9855 - val_loss: 0.6924 - val_accuracy: 0.8450\n",
      "Epoch 583/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9822\n",
      "Epoch 583: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1642 - accuracy: 0.9822 - val_loss: 0.7024 - val_accuracy: 0.8425\n",
      "Epoch 584/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9854\n",
      "Epoch 584: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1596 - accuracy: 0.9854 - val_loss: 0.6926 - val_accuracy: 0.8562\n",
      "Epoch 585/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9835\n",
      "Epoch 585: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1598 - accuracy: 0.9835 - val_loss: 0.6875 - val_accuracy: 0.8540\n",
      "Epoch 586/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9833\n",
      "Epoch 586: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1627 - accuracy: 0.9833 - val_loss: 0.6795 - val_accuracy: 0.8583\n",
      "Epoch 587/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9852\n",
      "Epoch 587: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1590 - accuracy: 0.9852 - val_loss: 0.7192 - val_accuracy: 0.8608\n",
      "Epoch 588/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9838\n",
      "Epoch 588: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1610 - accuracy: 0.9838 - val_loss: 0.6851 - val_accuracy: 0.8550\n",
      "Epoch 589/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9828\n",
      "Epoch 589: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1614 - accuracy: 0.9828 - val_loss: 0.7471 - val_accuracy: 0.8478\n",
      "Epoch 590/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9890\n",
      "Epoch 590: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1502 - accuracy: 0.9890 - val_loss: 0.8106 - val_accuracy: 0.8432\n",
      "Epoch 591/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9852\n",
      "Epoch 591: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1595 - accuracy: 0.9852 - val_loss: 0.6648 - val_accuracy: 0.8550\n",
      "Epoch 592/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9847\n",
      "Epoch 592: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1572 - accuracy: 0.9847 - val_loss: 0.6923 - val_accuracy: 0.8537\n",
      "Epoch 593/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9868\n",
      "Epoch 593: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1559 - accuracy: 0.9868 - val_loss: 0.8386 - val_accuracy: 0.8397\n",
      "Epoch 594/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9837\n",
      "Epoch 594: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1613 - accuracy: 0.9837 - val_loss: 0.7222 - val_accuracy: 0.8347\n",
      "Epoch 595/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9845\n",
      "Epoch 595: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1579 - accuracy: 0.9845 - val_loss: 0.6911 - val_accuracy: 0.8558\n",
      "Epoch 596/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9843\n",
      "Epoch 596: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1597 - accuracy: 0.9843 - val_loss: 0.6975 - val_accuracy: 0.8512\n",
      "Epoch 597/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9847\n",
      "Epoch 597: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1585 - accuracy: 0.9847 - val_loss: 0.6719 - val_accuracy: 0.8637\n",
      "Epoch 598/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9877\n",
      "Epoch 598: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1529 - accuracy: 0.9877 - val_loss: 0.7349 - val_accuracy: 0.8572\n",
      "Epoch 599/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9865\n",
      "Epoch 599: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1540 - accuracy: 0.9865 - val_loss: 0.7222 - val_accuracy: 0.8440\n",
      "Epoch 600/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9833\n",
      "Epoch 600: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1620 - accuracy: 0.9833 - val_loss: 0.6514 - val_accuracy: 0.8478\n",
      "Epoch 601/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9877\n",
      "Epoch 601: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1535 - accuracy: 0.9877 - val_loss: 0.6941 - val_accuracy: 0.8602\n",
      "Epoch 602/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9899\n",
      "Epoch 602: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1456 - accuracy: 0.9899 - val_loss: 0.6839 - val_accuracy: 0.8612\n",
      "Epoch 603/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9852\n",
      "Epoch 603: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1554 - accuracy: 0.9852 - val_loss: 0.7413 - val_accuracy: 0.8500\n",
      "Epoch 604/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9870\n",
      "Epoch 604: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1484 - accuracy: 0.9870 - val_loss: 0.6922 - val_accuracy: 0.8612\n",
      "Epoch 605/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9884\n",
      "Epoch 605: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1489 - accuracy: 0.9884 - val_loss: 0.8846 - val_accuracy: 0.8253\n",
      "Epoch 606/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9833\n",
      "Epoch 606: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1590 - accuracy: 0.9833 - val_loss: 0.7115 - val_accuracy: 0.8447\n",
      "Epoch 607/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9888\n",
      "Epoch 607: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1488 - accuracy: 0.9888 - val_loss: 0.7269 - val_accuracy: 0.8540\n",
      "Epoch 608/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9853\n",
      "Epoch 608: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1540 - accuracy: 0.9853 - val_loss: 0.7577 - val_accuracy: 0.8450\n",
      "Epoch 609/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9869\n",
      "Epoch 609: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1526 - accuracy: 0.9869 - val_loss: 0.7338 - val_accuracy: 0.8530\n",
      "Epoch 610/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9905\n",
      "Epoch 610: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1428 - accuracy: 0.9905 - val_loss: 0.8435 - val_accuracy: 0.8465\n",
      "Epoch 611/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1506 - accuracy: 0.9869\n",
      "Epoch 611: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1506 - accuracy: 0.9869 - val_loss: 0.7998 - val_accuracy: 0.8503\n",
      "Epoch 612/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9845\n",
      "Epoch 612: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1551 - accuracy: 0.9845 - val_loss: 0.7721 - val_accuracy: 0.8485\n",
      "Epoch 613/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9893\n",
      "Epoch 613: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1439 - accuracy: 0.9893 - val_loss: 0.7711 - val_accuracy: 0.8543\n",
      "Epoch 614/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9863\n",
      "Epoch 614: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1553 - accuracy: 0.9863 - val_loss: 0.7028 - val_accuracy: 0.8655\n",
      "Epoch 615/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9893\n",
      "Epoch 615: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1469 - accuracy: 0.9893 - val_loss: 0.7246 - val_accuracy: 0.8645\n",
      "Epoch 616/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9854\n",
      "Epoch 616: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1536 - accuracy: 0.9854 - val_loss: 0.7340 - val_accuracy: 0.8518\n",
      "Epoch 617/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9858\n",
      "Epoch 617: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1511 - accuracy: 0.9858 - val_loss: 0.7032 - val_accuracy: 0.8490\n",
      "Epoch 618/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9883\n",
      "Epoch 618: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1466 - accuracy: 0.9883 - val_loss: 0.7479 - val_accuracy: 0.8568\n",
      "Epoch 619/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9875\n",
      "Epoch 619: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1500 - accuracy: 0.9875 - val_loss: 0.6690 - val_accuracy: 0.8555\n",
      "Epoch 620/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9903\n",
      "Epoch 620: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1378 - accuracy: 0.9903 - val_loss: 0.7734 - val_accuracy: 0.8562\n",
      "Epoch 621/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9812\n",
      "Epoch 621: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1631 - accuracy: 0.9812 - val_loss: 0.6571 - val_accuracy: 0.8600\n",
      "Epoch 622/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9899\n",
      "Epoch 622: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1433 - accuracy: 0.9899 - val_loss: 0.8152 - val_accuracy: 0.8453\n",
      "Epoch 623/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9837\n",
      "Epoch 623: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1600 - accuracy: 0.9837 - val_loss: 0.7668 - val_accuracy: 0.8355\n",
      "Epoch 624/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9845\n",
      "Epoch 624: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1573 - accuracy: 0.9845 - val_loss: 0.7134 - val_accuracy: 0.8522\n",
      "Epoch 625/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9905\n",
      "Epoch 625: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1397 - accuracy: 0.9905 - val_loss: 1.0442 - val_accuracy: 0.8155\n",
      "Epoch 626/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9853\n",
      "Epoch 626: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1547 - accuracy: 0.9853 - val_loss: 0.6858 - val_accuracy: 0.8482\n",
      "Epoch 627/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9885\n",
      "Epoch 627: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1474 - accuracy: 0.9885 - val_loss: 0.7373 - val_accuracy: 0.8547\n",
      "Epoch 628/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9841\n",
      "Epoch 628: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1567 - accuracy: 0.9841 - val_loss: 0.7215 - val_accuracy: 0.8572\n",
      "Epoch 629/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9848\n",
      "Epoch 629: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1556 - accuracy: 0.9848 - val_loss: 0.7755 - val_accuracy: 0.8430\n",
      "Epoch 630/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9866\n",
      "Epoch 630: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1508 - accuracy: 0.9866 - val_loss: 0.7496 - val_accuracy: 0.8610\n",
      "Epoch 631/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.9861\n",
      "Epoch 631: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1494 - accuracy: 0.9861 - val_loss: 0.7078 - val_accuracy: 0.8593\n",
      "Epoch 632/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9915\n",
      "Epoch 632: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1399 - accuracy: 0.9915 - val_loss: 0.7401 - val_accuracy: 0.8602\n",
      "Epoch 633/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9918\n",
      "Epoch 633: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1369 - accuracy: 0.9918 - val_loss: 0.7235 - val_accuracy: 0.8537\n",
      "Epoch 634/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9892\n",
      "Epoch 634: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1440 - accuracy: 0.9892 - val_loss: 0.7778 - val_accuracy: 0.8520\n",
      "Epoch 635/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9812\n",
      "Epoch 635: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1586 - accuracy: 0.9812 - val_loss: 0.6903 - val_accuracy: 0.8422\n",
      "Epoch 636/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9895\n",
      "Epoch 636: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1414 - accuracy: 0.9895 - val_loss: 0.7441 - val_accuracy: 0.8540\n",
      "Epoch 637/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9872\n",
      "Epoch 637: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1480 - accuracy: 0.9872 - val_loss: 0.7098 - val_accuracy: 0.8648\n",
      "Epoch 638/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9860\n",
      "Epoch 638: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1485 - accuracy: 0.9860 - val_loss: 0.6676 - val_accuracy: 0.8537\n",
      "Epoch 639/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9843\n",
      "Epoch 639: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1538 - accuracy: 0.9843 - val_loss: 0.7433 - val_accuracy: 0.8587\n",
      "Epoch 640/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9883\n",
      "Epoch 640: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1448 - accuracy: 0.9883 - val_loss: 0.7485 - val_accuracy: 0.8558\n",
      "Epoch 641/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9845\n",
      "Epoch 641: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1561 - accuracy: 0.9845 - val_loss: 0.7164 - val_accuracy: 0.8537\n",
      "Epoch 642/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9861\n",
      "Epoch 642: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1485 - accuracy: 0.9861 - val_loss: 0.7190 - val_accuracy: 0.8535\n",
      "Epoch 643/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9881\n",
      "Epoch 643: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1468 - accuracy: 0.9881 - val_loss: 0.7102 - val_accuracy: 0.8553\n",
      "Epoch 644/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9897\n",
      "Epoch 644: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1421 - accuracy: 0.9897 - val_loss: 0.7779 - val_accuracy: 0.8518\n",
      "Epoch 645/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9898\n",
      "Epoch 645: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1425 - accuracy: 0.9898 - val_loss: 0.6863 - val_accuracy: 0.8670\n",
      "Epoch 646/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9923\n",
      "Epoch 646: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1341 - accuracy: 0.9923 - val_loss: 1.0717 - val_accuracy: 0.8330\n",
      "Epoch 647/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9843\n",
      "Epoch 647: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1550 - accuracy: 0.9843 - val_loss: 0.7190 - val_accuracy: 0.8385\n",
      "Epoch 648/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9871\n",
      "Epoch 648: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1452 - accuracy: 0.9871 - val_loss: 0.7015 - val_accuracy: 0.8487\n",
      "Epoch 649/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9841\n",
      "Epoch 649: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1562 - accuracy: 0.9841 - val_loss: 0.7399 - val_accuracy: 0.8482\n",
      "Epoch 650/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9852\n",
      "Epoch 650: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1534 - accuracy: 0.9852 - val_loss: 0.6713 - val_accuracy: 0.8378\n",
      "Epoch 651/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9871\n",
      "Epoch 651: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1497 - accuracy: 0.9871 - val_loss: 0.7252 - val_accuracy: 0.8562\n",
      "Epoch 652/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9877\n",
      "Epoch 652: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1447 - accuracy: 0.9877 - val_loss: 0.8116 - val_accuracy: 0.8355\n",
      "Epoch 653/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9852\n",
      "Epoch 653: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1520 - accuracy: 0.9852 - val_loss: 0.7606 - val_accuracy: 0.8520\n",
      "Epoch 654/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9898\n",
      "Epoch 654: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1413 - accuracy: 0.9898 - val_loss: 0.6309 - val_accuracy: 0.8630\n",
      "Epoch 655/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9899\n",
      "Epoch 655: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1398 - accuracy: 0.9899 - val_loss: 0.7595 - val_accuracy: 0.8495\n",
      "Epoch 656/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9889\n",
      "Epoch 656: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1455 - accuracy: 0.9889 - val_loss: 0.7162 - val_accuracy: 0.8525\n",
      "Epoch 657/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9912\n",
      "Epoch 657: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1368 - accuracy: 0.9912 - val_loss: 0.8476 - val_accuracy: 0.8367\n",
      "Epoch 658/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9898\n",
      "Epoch 658: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1405 - accuracy: 0.9898 - val_loss: 0.7292 - val_accuracy: 0.8553\n",
      "Epoch 659/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9926\n",
      "Epoch 659: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1322 - accuracy: 0.9926 - val_loss: 0.7746 - val_accuracy: 0.8568\n",
      "Epoch 660/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9882\n",
      "Epoch 660: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1415 - accuracy: 0.9882 - val_loss: 0.6681 - val_accuracy: 0.8522\n",
      "Epoch 661/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9877\n",
      "Epoch 661: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1482 - accuracy: 0.9877 - val_loss: 0.7979 - val_accuracy: 0.8465\n",
      "Epoch 662/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9880\n",
      "Epoch 662: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1411 - accuracy: 0.9880 - val_loss: 0.6942 - val_accuracy: 0.8665\n",
      "Epoch 663/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9860\n",
      "Epoch 663: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 432s 288ms/step - loss: 0.1460 - accuracy: 0.9860 - val_loss: 0.7424 - val_accuracy: 0.8583\n",
      "Epoch 664/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9839\n",
      "Epoch 664: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 438s 292ms/step - loss: 0.1557 - accuracy: 0.9839 - val_loss: 0.7195 - val_accuracy: 0.8482\n",
      "Epoch 665/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9823\n",
      "Epoch 665: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 438s 292ms/step - loss: 0.1576 - accuracy: 0.9823 - val_loss: 0.7259 - val_accuracy: 0.8430\n",
      "Epoch 666/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9877\n",
      "Epoch 666: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 435s 290ms/step - loss: 0.1444 - accuracy: 0.9877 - val_loss: 0.6841 - val_accuracy: 0.8570\n",
      "Epoch 667/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9867\n",
      "Epoch 667: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 439s 293ms/step - loss: 0.1447 - accuracy: 0.9867 - val_loss: 0.7963 - val_accuracy: 0.8245\n",
      "Epoch 668/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9908\n",
      "Epoch 668: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1392 - accuracy: 0.9908 - val_loss: 0.7581 - val_accuracy: 0.8575\n",
      "Epoch 669/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9923\n",
      "Epoch 669: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1329 - accuracy: 0.9923 - val_loss: 0.7518 - val_accuracy: 0.8443\n",
      "Epoch 670/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9884\n",
      "Epoch 670: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 429s 286ms/step - loss: 0.1406 - accuracy: 0.9884 - val_loss: 0.9131 - val_accuracy: 0.8403\n",
      "Epoch 671/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9871\n",
      "Epoch 671: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1414 - accuracy: 0.9871 - val_loss: 0.7404 - val_accuracy: 0.8475\n",
      "Epoch 672/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9872\n",
      "Epoch 672: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1453 - accuracy: 0.9872 - val_loss: 0.6848 - val_accuracy: 0.8512\n",
      "Epoch 673/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9888\n",
      "Epoch 673: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1430 - accuracy: 0.9888 - val_loss: 0.6857 - val_accuracy: 0.8630\n",
      "Epoch 674/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9918\n",
      "Epoch 674: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1360 - accuracy: 0.9918 - val_loss: 0.8137 - val_accuracy: 0.8422\n",
      "Epoch 675/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9949\n",
      "Epoch 675: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 413s 275ms/step - loss: 0.1251 - accuracy: 0.9949 - val_loss: 0.8830 - val_accuracy: 0.8320\n",
      "Epoch 676/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9895\n",
      "Epoch 676: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1410 - accuracy: 0.9895 - val_loss: 0.7739 - val_accuracy: 0.8522\n",
      "Epoch 677/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9882\n",
      "Epoch 677: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 411s 274ms/step - loss: 0.1414 - accuracy: 0.9882 - val_loss: 0.6986 - val_accuracy: 0.8518\n",
      "Epoch 678/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9887\n",
      "Epoch 678: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 411s 274ms/step - loss: 0.1439 - accuracy: 0.9887 - val_loss: 0.7116 - val_accuracy: 0.8550\n",
      "Epoch 679/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9862\n",
      "Epoch 679: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 410s 273ms/step - loss: 0.1454 - accuracy: 0.9862 - val_loss: 0.7409 - val_accuracy: 0.8418\n",
      "Epoch 680/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9859\n",
      "Epoch 680: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1461 - accuracy: 0.9859 - val_loss: 0.6941 - val_accuracy: 0.8595\n",
      "Epoch 681/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9865\n",
      "Epoch 681: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1440 - accuracy: 0.9865 - val_loss: 0.7958 - val_accuracy: 0.8278\n",
      "Epoch 682/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9883\n",
      "Epoch 682: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 415s 277ms/step - loss: 0.1434 - accuracy: 0.9883 - val_loss: 0.6962 - val_accuracy: 0.8518\n",
      "Epoch 683/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9908\n",
      "Epoch 683: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1343 - accuracy: 0.9908 - val_loss: 0.7177 - val_accuracy: 0.8570\n",
      "Epoch 684/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9882\n",
      "Epoch 684: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1419 - accuracy: 0.9882 - val_loss: 0.7370 - val_accuracy: 0.8590\n",
      "Epoch 685/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9844\n",
      "Epoch 685: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1480 - accuracy: 0.9844 - val_loss: 0.7435 - val_accuracy: 0.8400\n",
      "Epoch 686/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9883\n",
      "Epoch 686: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1406 - accuracy: 0.9883 - val_loss: 0.7646 - val_accuracy: 0.8425\n",
      "Epoch 687/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9874\n",
      "Epoch 687: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 426s 284ms/step - loss: 0.1434 - accuracy: 0.9874 - val_loss: 0.7216 - val_accuracy: 0.8602\n",
      "Epoch 688/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9877\n",
      "Epoch 688: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1426 - accuracy: 0.9877 - val_loss: 0.7560 - val_accuracy: 0.8490\n",
      "Epoch 689/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9862\n",
      "Epoch 689: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1483 - accuracy: 0.9862 - val_loss: 0.6659 - val_accuracy: 0.8558\n",
      "Epoch 690/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9906\n",
      "Epoch 690: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1371 - accuracy: 0.9906 - val_loss: 0.8165 - val_accuracy: 0.8530\n",
      "Epoch 691/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9886\n",
      "Epoch 691: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1421 - accuracy: 0.9886 - val_loss: 0.7864 - val_accuracy: 0.8522\n",
      "Epoch 692/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9849\n",
      "Epoch 692: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1468 - accuracy: 0.9849 - val_loss: 0.7908 - val_accuracy: 0.8390\n",
      "Epoch 693/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9862\n",
      "Epoch 693: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1470 - accuracy: 0.9862 - val_loss: 0.8256 - val_accuracy: 0.8375\n",
      "Epoch 694/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9894\n",
      "Epoch 694: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1362 - accuracy: 0.9894 - val_loss: 0.7693 - val_accuracy: 0.8558\n",
      "Epoch 695/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9877\n",
      "Epoch 695: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1430 - accuracy: 0.9877 - val_loss: 0.7986 - val_accuracy: 0.8375\n",
      "Epoch 696/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9854\n",
      "Epoch 696: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1472 - accuracy: 0.9854 - val_loss: 0.7055 - val_accuracy: 0.8525\n",
      "Epoch 697/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9855\n",
      "Epoch 697: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1472 - accuracy: 0.9855 - val_loss: 0.7261 - val_accuracy: 0.8495\n",
      "Epoch 698/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9884\n",
      "Epoch 698: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1402 - accuracy: 0.9884 - val_loss: 0.7881 - val_accuracy: 0.8453\n",
      "Epoch 699/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9897\n",
      "Epoch 699: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1387 - accuracy: 0.9897 - val_loss: 0.7231 - val_accuracy: 0.8558\n",
      "Epoch 700/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9890\n",
      "Epoch 700: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1392 - accuracy: 0.9890 - val_loss: 0.6950 - val_accuracy: 0.8505\n",
      "Epoch 701/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9891\n",
      "Epoch 701: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1382 - accuracy: 0.9891 - val_loss: 0.7477 - val_accuracy: 0.8510\n",
      "Epoch 702/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9902\n",
      "Epoch 702: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1336 - accuracy: 0.9902 - val_loss: 0.8149 - val_accuracy: 0.8405\n",
      "Epoch 703/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9887\n",
      "Epoch 703: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9887 - val_loss: 0.9238 - val_accuracy: 0.8163\n",
      "Epoch 704/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9870\n",
      "Epoch 704: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1433 - accuracy: 0.9870 - val_loss: 0.7347 - val_accuracy: 0.8512\n",
      "Epoch 705/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9860\n",
      "Epoch 705: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1467 - accuracy: 0.9860 - val_loss: 0.6777 - val_accuracy: 0.8533\n",
      "Epoch 706/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9878\n",
      "Epoch 706: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1406 - accuracy: 0.9878 - val_loss: 0.8143 - val_accuracy: 0.8440\n",
      "Epoch 707/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9877\n",
      "Epoch 707: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1407 - accuracy: 0.9877 - val_loss: 0.6718 - val_accuracy: 0.8668\n",
      "Epoch 708/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9904\n",
      "Epoch 708: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1349 - accuracy: 0.9904 - val_loss: 0.7232 - val_accuracy: 0.8635\n",
      "Epoch 709/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9891\n",
      "Epoch 709: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1362 - accuracy: 0.9891 - val_loss: 0.7729 - val_accuracy: 0.8363\n",
      "Epoch 710/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9868\n",
      "Epoch 710: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1440 - accuracy: 0.9868 - val_loss: 0.7576 - val_accuracy: 0.8570\n",
      "Epoch 711/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9916\n",
      "Epoch 711: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1318 - accuracy: 0.9916 - val_loss: 0.9236 - val_accuracy: 0.8400\n",
      "Epoch 712/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9877\n",
      "Epoch 712: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1432 - accuracy: 0.9877 - val_loss: 0.7622 - val_accuracy: 0.8400\n",
      "Epoch 713/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9870\n",
      "Epoch 713: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1442 - accuracy: 0.9870 - val_loss: 0.7225 - val_accuracy: 0.8485\n",
      "Epoch 714/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9923\n",
      "Epoch 714: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1316 - accuracy: 0.9923 - val_loss: 0.7398 - val_accuracy: 0.8495\n",
      "Epoch 715/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9893\n",
      "Epoch 715: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1367 - accuracy: 0.9893 - val_loss: 0.7546 - val_accuracy: 0.8605\n",
      "Epoch 716/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9823\n",
      "Epoch 716: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1559 - accuracy: 0.9823 - val_loss: 0.6297 - val_accuracy: 0.8522\n",
      "Epoch 717/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9876\n",
      "Epoch 717: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1399 - accuracy: 0.9876 - val_loss: 0.7073 - val_accuracy: 0.8528\n",
      "Epoch 718/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9863\n",
      "Epoch 718: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1444 - accuracy: 0.9863 - val_loss: 0.6902 - val_accuracy: 0.8587\n",
      "Epoch 719/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9876\n",
      "Epoch 719: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1431 - accuracy: 0.9876 - val_loss: 0.7891 - val_accuracy: 0.8480\n",
      "Epoch 720/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9866\n",
      "Epoch 720: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1439 - accuracy: 0.9866 - val_loss: 0.7395 - val_accuracy: 0.8322\n",
      "Epoch 721/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9879\n",
      "Epoch 721: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1383 - accuracy: 0.9879 - val_loss: 0.7335 - val_accuracy: 0.8525\n",
      "Epoch 722/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9883\n",
      "Epoch 722: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1420 - accuracy: 0.9883 - val_loss: 0.7317 - val_accuracy: 0.8420\n",
      "Epoch 723/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9877\n",
      "Epoch 723: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1452 - accuracy: 0.9877 - val_loss: 0.6799 - val_accuracy: 0.8595\n",
      "Epoch 724/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9872\n",
      "Epoch 724: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1411 - accuracy: 0.9872 - val_loss: 0.7854 - val_accuracy: 0.8347\n",
      "Epoch 725/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9872\n",
      "Epoch 725: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1422 - accuracy: 0.9872 - val_loss: 0.7360 - val_accuracy: 0.8540\n",
      "Epoch 726/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9833\n",
      "Epoch 726: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1527 - accuracy: 0.9833 - val_loss: 0.7249 - val_accuracy: 0.8363\n",
      "Epoch 727/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9881\n",
      "Epoch 727: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1400 - accuracy: 0.9881 - val_loss: 0.7344 - val_accuracy: 0.8533\n",
      "Epoch 728/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9898\n",
      "Epoch 728: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9898 - val_loss: 0.7354 - val_accuracy: 0.8490\n",
      "Epoch 729/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9905\n",
      "Epoch 729: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1331 - accuracy: 0.9905 - val_loss: 0.7654 - val_accuracy: 0.8315\n",
      "Epoch 730/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9884\n",
      "Epoch 730: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1402 - accuracy: 0.9884 - val_loss: 0.8508 - val_accuracy: 0.8407\n",
      "Epoch 731/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9882\n",
      "Epoch 731: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9882 - val_loss: 0.7166 - val_accuracy: 0.8605\n",
      "Epoch 732/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9859\n",
      "Epoch 732: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1484 - accuracy: 0.9859 - val_loss: 0.7211 - val_accuracy: 0.8453\n",
      "Epoch 733/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9890\n",
      "Epoch 733: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9890 - val_loss: 0.6876 - val_accuracy: 0.8562\n",
      "Epoch 734/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9861\n",
      "Epoch 734: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1452 - accuracy: 0.9861 - val_loss: 0.7420 - val_accuracy: 0.8443\n",
      "Epoch 735/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9908\n",
      "Epoch 735: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1304 - accuracy: 0.9908 - val_loss: 0.7381 - val_accuracy: 0.8558\n",
      "Epoch 736/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9898\n",
      "Epoch 736: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1365 - accuracy: 0.9898 - val_loss: 0.7308 - val_accuracy: 0.8558\n",
      "Epoch 737/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9888\n",
      "Epoch 737: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9888 - val_loss: 0.7776 - val_accuracy: 0.8447\n",
      "Epoch 738/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9907\n",
      "Epoch 738: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1313 - accuracy: 0.9907 - val_loss: 0.8214 - val_accuracy: 0.8378\n",
      "Epoch 739/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9899\n",
      "Epoch 739: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1375 - accuracy: 0.9899 - val_loss: 0.7200 - val_accuracy: 0.8522\n",
      "Epoch 740/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9912\n",
      "Epoch 740: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1342 - accuracy: 0.9912 - val_loss: 0.7446 - val_accuracy: 0.8570\n",
      "Epoch 741/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9911\n",
      "Epoch 741: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1323 - accuracy: 0.9911 - val_loss: 0.8283 - val_accuracy: 0.8265\n",
      "Epoch 742/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9884\n",
      "Epoch 742: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1397 - accuracy: 0.9884 - val_loss: 0.8797 - val_accuracy: 0.8300\n",
      "Epoch 743/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9884\n",
      "Epoch 743: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1385 - accuracy: 0.9884 - val_loss: 0.8520 - val_accuracy: 0.8317\n",
      "Epoch 744/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9903\n",
      "Epoch 744: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1320 - accuracy: 0.9903 - val_loss: 0.7697 - val_accuracy: 0.8457\n",
      "Epoch 745/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9928\n",
      "Epoch 745: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1262 - accuracy: 0.9928 - val_loss: 0.8139 - val_accuracy: 0.8455\n",
      "Epoch 746/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9879\n",
      "Epoch 746: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1388 - accuracy: 0.9879 - val_loss: 0.7490 - val_accuracy: 0.8590\n",
      "Epoch 747/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9881\n",
      "Epoch 747: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1390 - accuracy: 0.9881 - val_loss: 0.7455 - val_accuracy: 0.8540\n",
      "Epoch 748/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9896\n",
      "Epoch 748: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1335 - accuracy: 0.9896 - val_loss: 0.6918 - val_accuracy: 0.8577\n",
      "Epoch 749/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9919\n",
      "Epoch 749: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1282 - accuracy: 0.9919 - val_loss: 0.8083 - val_accuracy: 0.8420\n",
      "Epoch 750/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9896\n",
      "Epoch 750: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1354 - accuracy: 0.9896 - val_loss: 0.7425 - val_accuracy: 0.8543\n",
      "Epoch 751/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9891\n",
      "Epoch 751: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1385 - accuracy: 0.9891 - val_loss: 0.6564 - val_accuracy: 0.8640\n",
      "Epoch 752/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9909\n",
      "Epoch 752: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1331 - accuracy: 0.9909 - val_loss: 0.7609 - val_accuracy: 0.8355\n",
      "Epoch 753/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9903\n",
      "Epoch 753: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1346 - accuracy: 0.9903 - val_loss: 0.6912 - val_accuracy: 0.8580\n",
      "Epoch 754/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9914\n",
      "Epoch 754: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1306 - accuracy: 0.9914 - val_loss: 0.8074 - val_accuracy: 0.8313\n",
      "Epoch 755/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9907\n",
      "Epoch 755: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1335 - accuracy: 0.9907 - val_loss: 0.6893 - val_accuracy: 0.8530\n",
      "Epoch 756/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9875\n",
      "Epoch 756: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1388 - accuracy: 0.9875 - val_loss: 0.6987 - val_accuracy: 0.8648\n",
      "Epoch 757/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9906\n",
      "Epoch 757: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1333 - accuracy: 0.9906 - val_loss: 0.8827 - val_accuracy: 0.8347\n",
      "Epoch 758/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9847\n",
      "Epoch 758: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1490 - accuracy: 0.9847 - val_loss: 0.6560 - val_accuracy: 0.8533\n",
      "Epoch 759/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9900\n",
      "Epoch 759: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1337 - accuracy: 0.9900 - val_loss: 0.7372 - val_accuracy: 0.8462\n",
      "Epoch 760/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9889\n",
      "Epoch 760: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1353 - accuracy: 0.9889 - val_loss: 0.7817 - val_accuracy: 0.8475\n",
      "Epoch 761/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9887\n",
      "Epoch 761: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1351 - accuracy: 0.9887 - val_loss: 0.8869 - val_accuracy: 0.8338\n",
      "Epoch 762/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9869\n",
      "Epoch 762: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9869 - val_loss: 0.6899 - val_accuracy: 0.8487\n",
      "Epoch 763/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9880\n",
      "Epoch 763: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1367 - accuracy: 0.9880 - val_loss: 0.7510 - val_accuracy: 0.8583\n",
      "Epoch 764/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9899\n",
      "Epoch 764: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1356 - accuracy: 0.9899 - val_loss: 0.7138 - val_accuracy: 0.8528\n",
      "Epoch 765/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9937\n",
      "Epoch 765: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1228 - accuracy: 0.9937 - val_loss: 0.7454 - val_accuracy: 0.8568\n",
      "Epoch 766/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9892\n",
      "Epoch 766: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1365 - accuracy: 0.9892 - val_loss: 0.7035 - val_accuracy: 0.8447\n",
      "Epoch 767/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9901\n",
      "Epoch 767: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1337 - accuracy: 0.9901 - val_loss: 0.7366 - val_accuracy: 0.8543\n",
      "Epoch 768/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9908\n",
      "Epoch 768: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1289 - accuracy: 0.9908 - val_loss: 0.9892 - val_accuracy: 0.8170\n",
      "Epoch 769/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9882\n",
      "Epoch 769: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1381 - accuracy: 0.9882 - val_loss: 0.7683 - val_accuracy: 0.8487\n",
      "Epoch 770/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9877\n",
      "Epoch 770: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1392 - accuracy: 0.9877 - val_loss: 0.6755 - val_accuracy: 0.8600\n",
      "Epoch 771/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9926\n",
      "Epoch 771: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1249 - accuracy: 0.9926 - val_loss: 0.7187 - val_accuracy: 0.8610\n",
      "Epoch 772/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9934\n",
      "Epoch 772: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1240 - accuracy: 0.9934 - val_loss: 0.7731 - val_accuracy: 0.8393\n",
      "Epoch 773/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9916\n",
      "Epoch 773: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1293 - accuracy: 0.9916 - val_loss: 0.8094 - val_accuracy: 0.8482\n",
      "Epoch 774/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9889\n",
      "Epoch 774: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1370 - accuracy: 0.9889 - val_loss: 0.7241 - val_accuracy: 0.8547\n",
      "Epoch 775/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9931\n",
      "Epoch 775: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1233 - accuracy: 0.9931 - val_loss: 0.7918 - val_accuracy: 0.8360\n",
      "Epoch 776/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9887\n",
      "Epoch 776: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1327 - accuracy: 0.9887 - val_loss: 0.7241 - val_accuracy: 0.8515\n",
      "Epoch 777/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9897\n",
      "Epoch 777: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1315 - accuracy: 0.9897 - val_loss: 0.6792 - val_accuracy: 0.8518\n",
      "Epoch 778/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9925\n",
      "Epoch 778: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1274 - accuracy: 0.9925 - val_loss: 0.7743 - val_accuracy: 0.8465\n",
      "Epoch 779/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9901\n",
      "Epoch 779: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1319 - accuracy: 0.9901 - val_loss: 0.7550 - val_accuracy: 0.8605\n",
      "Epoch 780/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9868\n",
      "Epoch 780: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1427 - accuracy: 0.9868 - val_loss: 0.6658 - val_accuracy: 0.8520\n",
      "Epoch 781/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9902\n",
      "Epoch 781: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1328 - accuracy: 0.9902 - val_loss: 0.7571 - val_accuracy: 0.8320\n",
      "Epoch 782/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9887\n",
      "Epoch 782: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1351 - accuracy: 0.9887 - val_loss: 0.7844 - val_accuracy: 0.8363\n",
      "Epoch 783/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9922\n",
      "Epoch 783: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1278 - accuracy: 0.9922 - val_loss: 0.7473 - val_accuracy: 0.8620\n",
      "Epoch 784/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9912\n",
      "Epoch 784: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1300 - accuracy: 0.9912 - val_loss: 0.7155 - val_accuracy: 0.8627\n",
      "Epoch 785/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9887\n",
      "Epoch 785: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1326 - accuracy: 0.9887 - val_loss: 0.7613 - val_accuracy: 0.8572\n",
      "Epoch 786/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9868\n",
      "Epoch 786: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1393 - accuracy: 0.9868 - val_loss: 0.7015 - val_accuracy: 0.8572\n",
      "Epoch 787/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9913\n",
      "Epoch 787: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1276 - accuracy: 0.9913 - val_loss: 0.7988 - val_accuracy: 0.8372\n",
      "Epoch 788/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9864\n",
      "Epoch 788: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1407 - accuracy: 0.9864 - val_loss: 0.7371 - val_accuracy: 0.8447\n",
      "Epoch 789/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9896\n",
      "Epoch 789: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1333 - accuracy: 0.9896 - val_loss: 0.8675 - val_accuracy: 0.8227\n",
      "Epoch 790/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9880\n",
      "Epoch 790: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1330 - accuracy: 0.9880 - val_loss: 0.7816 - val_accuracy: 0.8545\n",
      "Epoch 791/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9857\n",
      "Epoch 791: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1400 - accuracy: 0.9857 - val_loss: 0.7870 - val_accuracy: 0.8420\n",
      "Epoch 792/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9897\n",
      "Epoch 792: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1345 - accuracy: 0.9897 - val_loss: 0.7232 - val_accuracy: 0.8595\n",
      "Epoch 793/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9913\n",
      "Epoch 793: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1260 - accuracy: 0.9913 - val_loss: 0.8365 - val_accuracy: 0.8407\n",
      "Epoch 794/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9864\n",
      "Epoch 794: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1391 - accuracy: 0.9864 - val_loss: 0.8136 - val_accuracy: 0.8350\n",
      "Epoch 795/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9866\n",
      "Epoch 795: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1411 - accuracy: 0.9866 - val_loss: 0.7095 - val_accuracy: 0.8605\n",
      "Epoch 796/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9885\n",
      "Epoch 796: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1323 - accuracy: 0.9885 - val_loss: 0.7467 - val_accuracy: 0.8487\n",
      "Epoch 797/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9845\n",
      "Epoch 797: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1468 - accuracy: 0.9845 - val_loss: 0.6984 - val_accuracy: 0.8622\n",
      "Epoch 798/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9877\n",
      "Epoch 798: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1346 - accuracy: 0.9877 - val_loss: 0.7324 - val_accuracy: 0.8440\n",
      "Epoch 799/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9910\n",
      "Epoch 799: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1317 - accuracy: 0.9910 - val_loss: 0.7011 - val_accuracy: 0.8455\n",
      "Epoch 800/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9923\n",
      "Epoch 800: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1285 - accuracy: 0.9923 - val_loss: 0.7863 - val_accuracy: 0.8512\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 6000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m base_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m04S-UNIWARD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVT_prueba1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mbase_name\n\u001b[1;32m----> 3\u001b[0m _, history  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_cvt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch, model_name, num_test)\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m     14\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[0;32m     15\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[tensorboard,  checkpoint], \n\u001b[0;32m     16\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     17\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid),\n\u001b[0;32m     18\u001b[0m                     initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch)\n\u001b[1;32m---> 20\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m results_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/testing_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mnum_test\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_dir):\n",
      "File \u001b[1;32mc:\\Users\\sergioa.holguin\\.conda\\envs\\machine\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\sergioa.holguin\\.conda\\envs\\machine\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 6000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "base_name=\"04S-UNIWARD\"\n",
    "name=\"Model_\"+'CVT_prueba1'+\"_\"+base_name\n",
    "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=8, epochs=800, model_name=name, num_test='1_cvt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan_create\n"
     ]
    }
   ],
   "source": [
    "def new_arch_kan():\n",
    "  tf.keras.backend.clear_session()\n",
    "  inputs = tf.keras.Input(shape=(256,256,1), name=\"input_1\")\n",
    "  #Layer 1\n",
    "  layers_ty = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "  layers_tn = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=True, activation=Tanh3, use_bias=True)(inputs)\n",
    "\n",
    "  layers = tf.keras.layers.add([layers_ty, layers_tn])\n",
    "  layers1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "  #Layer 2\n",
    "  \n",
    "  # L1\n",
    "  layers = Block_1(layers1,64)\n",
    "\n",
    "  # L2\n",
    "  layers = Block_1(layers,64)\n",
    "\n",
    "  # L3 - L7\n",
    "  for i in range(5):\n",
    "    layers = Block_2(layers,64)\n",
    "\n",
    "  # L8 - L11\n",
    "  for i in [64, 64, 128, 256]:\n",
    "    layers = Block_3(layers,i)\n",
    "\n",
    "  # L12\n",
    "  layers = Block_4(layers,512)\n",
    "  #CVT=Transform_sh_1(layers)\n",
    "\n",
    "  representation = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(layers)\n",
    "  representation = tf.keras.layers.GlobalAvgPool2D()(representation)\n",
    "  #---------------------------------------------------Fin de Transformer 2------------------------------------------------------------------------#\n",
    "  # Classify outputs.\n",
    "      #FC\n",
    "  layers = DenseKAN(64)(representation)\n",
    "  layers = DenseKAN(32)(layers)\n",
    "  layers = DenseKAN(16)(layers)\n",
    "\n",
    "  #Softmax\n",
    "  layers = DenseKAN(2)(layers)\n",
    "  predictions = tf.keras.layers.Softmax(axis=1)(layers)\n",
    "  model =tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "\n",
    "  # Compile the model if the compile flag is set\n",
    "  if compile:\n",
    "      model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "      print(\"kan_create\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "model2 = new_arch_kan()  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7809 - accuracy: 0.4998\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to D:/testing_1_kan/Model_CVkan_prueba1_04S-UNIWARD_1729969123.5718586\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 412s 275ms/step - loss: 0.7809 - accuracy: 0.4998 - val_loss: 0.7001 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.4942\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 415s 277ms/step - loss: 0.6959 - accuracy: 0.4942 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.4938\n",
      "Epoch 3: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.7638 - accuracy: 0.4938 - val_loss: 0.7340 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7356 - accuracy: 0.5034\n",
      "Epoch 4: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 425s 284ms/step - loss: 0.7356 - accuracy: 0.5034 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4998\n",
      "Epoch 5: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.6935 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5030\n",
      "Epoch 6: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.6935 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5030\n",
      "Epoch 7: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 415s 277ms/step - loss: 0.6936 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5012\n",
      "Epoch 8: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 415s 276ms/step - loss: 0.6938 - accuracy: 0.5012 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4893\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.6937 - accuracy: 0.4893 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4955\n",
      "Epoch 10: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.6939 - accuracy: 0.4955 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4968\n",
      "Epoch 11: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.6937 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4963\n",
      "Epoch 12: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.6937 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4912\n",
      "Epoch 13: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.6937 - accuracy: 0.4912 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5038\n",
      "Epoch 14: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.6935 - accuracy: 0.5038 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5008\n",
      "Epoch 15: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.6936 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5022\n",
      "Epoch 16: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.6937 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4937\n",
      "Epoch 17: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 398s 265ms/step - loss: 0.6938 - accuracy: 0.4937 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4975\n",
      "Epoch 18: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 397s 265ms/step - loss: 0.6936 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5002\n",
      "Epoch 19: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 401s 267ms/step - loss: 0.6936 - accuracy: 0.5002 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4995\n",
      "Epoch 20: val_accuracy did not improve from 0.50000\n",
      "1500/1500 [==============================] - 404s 270ms/step - loss: 0.6938 - accuracy: 0.4995 - val_loss: 0.6934 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Model_CVkan_prueba1_04S-UNIWARD = 8277.6695394516 [seconds]\n"
     ]
    }
   ],
   "source": [
    "base_name=\"04S-UNIWARD\"\n",
    "name=\"Model_\"+'CVkan_prueba1'+\"_\"+base_name\n",
    "_, history  = train(model2, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=8, epochs=20, model_name=name, num_test='1_kan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
