{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate, Lambda, Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Conv2D, MaxPooling2D,\n",
    "    AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization, ReLU, Input\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import optimizers, regularizers, backend as K\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report,\n",
    "    cohen_kappa_score, hamming_loss, log_loss, zero_one_loss, matthews_corrcoef, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from time import time\n",
    "import time as tm\n",
    "import datetime\n",
    "from skimage.util.shape import view_as_blocks\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import ntpath\n",
    "import copy\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tfkan.layers import DenseKAN\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tensorflow import MlflowCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters and Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('../SRM_Kernels.npy') \n",
    "biasSRM=np.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################\n",
    "def thtanh(x,t):\n",
    "    th=K.tanh(x)*t\n",
    "    return th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-UNIWARD BOSSbase 1.01 PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 256, 256, 1)\n",
      "(12000, 2)\n",
      "(4000, 256, 256, 1)\n",
      "(4000, 2)\n",
      "(4000, 256, 256, 1)\n",
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "X_train = np.load('../data_gbras/X_train.npy')\n",
    "y_train = np.load('../data_gbras/y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load('../data_gbras/X_valid.npy')\n",
    "y_valid = np.load('../data_gbras/y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load('../data_gbras/X_test.npy')\n",
    "y_test = np.load('../data_gbras/y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions arquitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excitation_layer(input_layer, out_dim, ratio, conv):\n",
    "  squeeze = tf.keras.layers.GlobalAveragePooling2D()(input_layer)\n",
    "  excitation = tf.keras.layers.Dense(units=out_dim / ratio, activation='relu')(squeeze)\n",
    "  excitation = tf.keras.layers.Dense(out_dim,activation='sigmoid')(excitation)\n",
    "  excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "  scale = tf.keras.layers.multiply([input_layer, excitation])\n",
    "  if conv:\n",
    "    shortcut = tf.keras.layers.Conv2D(out_dim,kernel_size=1,strides=1,\n",
    "                                      padding='same',kernel_initializer='he_normal')(input_layer)\n",
    "    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "  else:\n",
    "    shortcut = input_layer\n",
    "  out = tf.keras.layers.add([shortcut, scale])\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "def sreLu (input):\n",
    "  return ReLU(negative_slope=0.1, threshold=0)(input)\n",
    "\n",
    "def sConv(input,parameters,size,nstrides):\n",
    "  return Conv2D(parameters, (size,size), strides=(nstrides,nstrides),padding=\"same\", kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(input)\n",
    "\n",
    "def sBN (input):\n",
    "  return tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=True, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(input)\n",
    "\n",
    "def sGlobal_Avg_Pooling (input):\n",
    "  return tf.keras.layers.GlobalAveragePooling2D()(input)\n",
    "\n",
    "def sDense (input, n_units, activate_c):\n",
    "  return tf.keras.layers.Dense(n_units,activation=activate_c)(input)\n",
    "\n",
    "def smultiply (input_1, input_2):\n",
    "  return tf.keras.layers.multiply([input_1, input_2])\n",
    "\n",
    "def sadd (input_1, input_2):\n",
    "  return tf.keras.layers.add([input_1, input_2])\n",
    "\n",
    "# Initial learning rate for the optimizer\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "# Learning rate schedule with exponential decay\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Block_1 (input, parameter):\n",
    "  output = sConv(input, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  output = sreLu(output)\n",
    "  return output\n",
    "  \n",
    "\n",
    "\n",
    "def SE_Block(input, out_dim, ratio):\n",
    "  output = sGlobal_Avg_Pooling(input)\n",
    "  output = sDense(output, out_dim/ratio, 'relu')\n",
    "  output = sDense(output, out_dim, 'sigmoid')\n",
    "  return output\n",
    "  \n",
    "  \n",
    "  \n",
    "def Block_2 (input, parameter):\n",
    "  output = Block_1(input, parameter)\n",
    "  output = sConv(output, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  multiplier = SE_Block(output,  parameter, parameter)\n",
    "  # output = smultiply(output, output)\n",
    "  output = smultiply(multiplier, output)\n",
    "  output = sadd(output, input)\n",
    "  return output\n",
    "  \n",
    "  \n",
    "\n",
    "def Block_3 (input, parameter):\n",
    "  addition = sConv(input, parameter, 1, 2)\n",
    "  addition = sBN(addition)\n",
    "  output = sConv(input, parameter, 3, 2)\n",
    "  output = sBN(output)\n",
    "  output = sreLu(output)\n",
    "  output = sConv(output, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  multiplier = SE_Block(output,  parameter, parameter)\n",
    "  output = smultiply(multiplier, output)\n",
    "  output = sadd(output, addition)\n",
    "  return output  \n",
    "  \n",
    "def Block_4 (input, parameter):\n",
    "  output = Block_1(input, parameter)\n",
    "  output = sConv(input, parameter, 3, 1)\n",
    "  output = sBN(output)\n",
    "  \n",
    "  return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS_1 = 1e-6\n",
    "PROJECTION_DIM_1 = 16\n",
    "NUM_HEADS_1 = 4\n",
    "NUM_LAYERS_1 = 4\n",
    "MLP_UNITS_1 = [\n",
    "    PROJECTION_DIM_1 * 2,\n",
    "    PROJECTION_DIM_1,\n",
    "]\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE_2 = 1e-3\n",
    "WEIGHT_DECAY_2 = 1e-4\n",
    "\n",
    "IMAGE_SIZE_2 =  16# We will resize input images to this size.\n",
    "PATCH_SIZE_2 = 4  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES_2 = (IMAGE_SIZE_2 // PATCH_SIZE_2) ** 2\n",
    "print(NUM_PATCHES_2)\n",
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS_2 = 1e-6\n",
    "PROJECTION_DIM_2 = 128\n",
    "NUM_HEADS_2 = 4\n",
    "NUM_LAYERS_2 = 4\n",
    "MLP_UNITS_2 = [\n",
    "    PROJECTION_DIM_2 * 2,\n",
    "    PROJECTION_DIM_2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(projected_patches, num_patches=NUM_PATCHES_2, projection_dim=PROJECTION_DIM_2):\n",
    "    # Build the positions: create a range of position indices from 0 to num_patches - 1\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "\n",
    "    # Encode the positions with an Embedding layer\n",
    "    encoded_positions = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim\n",
    "    )(positions)\n",
    "\n",
    "    # Add encoded positions to the projected patches\n",
    "    return projected_patches + encoded_positions\n",
    "\n",
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and add Dense => Dropout layers\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)  # Dense layer with GELU activation\n",
    "        x = layers.Dropout(dropout_rate)(x)  # Dropout layer\n",
    "    return x\n",
    "\n",
    "def transformer_2(encoded_patches):\n",
    "    # Apply layer normalization\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(encoded_patches)\n",
    "\n",
    "    # Multi-Head Self Attention layer\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS_2, key_dim=PROJECTION_DIM_2, dropout=0.1\n",
    "    )(x1, x1)\n",
    "\n",
    "    # Skip connection\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Apply layer normalization again\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(x2)\n",
    "\n",
    "    # Apply MLP layer\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS_2, dropout_rate=0.1)\n",
    "\n",
    "    # Second skip connection\n",
    "    encoded_patches = layers.Add()([x4, x2])\n",
    "    return encoded_patches\n",
    "\n",
    "def Transform_sh_2(inputs):\n",
    "    # Apply squeeze and excitation layer\n",
    "    inputs1 = squeeze_excitation_layer(inputs, out_dim=512, ratio=32.0, conv=False)\n",
    "    print(inputs1.shape)\n",
    "\n",
    "    # Project input patches using a Conv2D layer\n",
    "    projected_patches = layers.Conv2D(\n",
    "        filters=PROJECTION_DIM_2,\n",
    "        kernel_size=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
    "        strides=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
    "        padding=\"VALID\",\n",
    "    )(inputs1)\n",
    "\n",
    "    # Get the shape of the projected patches\n",
    "    _, h, w, c = projected_patches.shape\n",
    "\n",
    "    # Reshape the projected patches\n",
    "    projected_patches = layers.Reshape((h * w, c))(projected_patches)  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Add positional embeddings to the projected patches\n",
    "    encoded_patches = position_embedding(projected_patches)  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Apply dropout\n",
    "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
    "\n",
    "    # Iterate over the number of layers and stack transformer blocks\n",
    "    for i in range(NUM_LAYERS_2):\n",
    "        # Add a transformer block\n",
    "        encoded_patches = transformer_2(encoded_patches)\n",
    "\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch = 0, model_name=\"\", num_test=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=\"D:/testing_\"+num_test+\"/\"+model_name+\"_\"+\"{}\".format(time())\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "    filepath = log_dir+\"/saved-model.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath, \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    model.reset_states()\n",
    "    with mlflow.start_run() as run:\n",
    "        history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                            callbacks=[tensorboard,  checkpoint,\n",
    "                                       MlflowCallback(run)], \n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid, y_valid),\n",
    "                            initial_epoch=initial_epoch)\n",
    "\n",
    "        metrics = model.evaluate(X_test, y_test, verbose=0, callbacks=[MlflowCallback(run)])\n",
    "\n",
    "    results_dir=\"D:/testing_\"+num_test+\"/\"+model_name+\"/\"\n",
    "   \n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "      \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Xu_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Xu_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVT-h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output last layer before transformer (None, 16, 16, 512)\n",
      "(None, 16, 16, 512)\n",
      "Transformer_create\n"
     ]
    }
   ],
   "source": [
    "def new_arch():\n",
    "  tf.keras.backend.clear_session()\n",
    "  inputs = tf.keras.Input(shape=(256,256,1), name=\"input_1\")\n",
    "  #Layer 1\n",
    "  layers_ty = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "  layers_tn = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=True, activation=Tanh3, use_bias=True)(inputs)\n",
    "\n",
    "  layers = tf.keras.layers.add([layers_ty, layers_tn])\n",
    "  layers1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "  #Layer 2\n",
    "  \n",
    "  # L1\n",
    "  layers = Block_1(layers1,64)\n",
    "\n",
    "  # L2\n",
    "  layers = Block_1(layers,64)\n",
    "\n",
    "  # L3 - L7\n",
    "  for i in range(5):\n",
    "    layers = Block_2(layers,64)\n",
    "\n",
    "  # L8 - L11\n",
    "  for i in [64, 64, 128, 256]:\n",
    "    layers = Block_3(layers,i)\n",
    "\n",
    "  # L12\n",
    "  layers = Block_4(layers,512)\n",
    "  print('output last layer before transformer', layers.shape)\n",
    "  #CVT=Transform_sh_1(layers)\n",
    "  #CVT_2=Transform_sh_1(CVT)\n",
    "  CVT1=Transform_sh_2(layers)\n",
    "\n",
    "  representation = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(CVT1)\n",
    "  representation = tf.keras.layers.GlobalAvgPool1D()(representation)\n",
    "  #---------------------------------------------------Fin de Transformer 2------------------------------------------------------------------------#\n",
    "  # Classify outputs.\n",
    "      #FC\n",
    "  layers = Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(representation)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "  layers = Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "  layers = Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "\n",
    "  #Softmax\n",
    "  predictions = Dense(2, activation=\"softmax\", name=\"output_1\",kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "  model =tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    " \n",
    "  # Compile the model if the compile flag is set\n",
    "\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.95)\n",
    "  if compile:\n",
    "      model.compile(optimizer= optimizer,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "      \n",
    "      print (\"Transformer_create\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "model = new_arch()  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.4864\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50225, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 719s 471ms/step - loss: 0.9567 - accuracy: 0.4864 - val_loss: 0.9519 - val_accuracy: 0.5023\n",
      "Epoch 2/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9508 - accuracy: 0.4990\n",
      "Epoch 2: val_accuracy improved from 0.50225 to 0.53375, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 725s 483ms/step - loss: 0.9508 - accuracy: 0.4990 - val_loss: 0.9438 - val_accuracy: 0.5337\n",
      "Epoch 3/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.5309\n",
      "Epoch 3: val_accuracy improved from 0.53375 to 0.55350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 728s 485ms/step - loss: 0.9437 - accuracy: 0.5309 - val_loss: 0.9344 - val_accuracy: 0.5535\n",
      "Epoch 4/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9332 - accuracy: 0.5458\n",
      "Epoch 4: val_accuracy improved from 0.55350 to 0.63100, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 728s 486ms/step - loss: 0.9332 - accuracy: 0.5458 - val_loss: 0.8824 - val_accuracy: 0.6310\n",
      "Epoch 5/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9192 - accuracy: 0.5634\n",
      "Epoch 5: val_accuracy improved from 0.63100 to 0.66850, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 727s 485ms/step - loss: 0.9192 - accuracy: 0.5634 - val_loss: 0.8367 - val_accuracy: 0.6685\n",
      "Epoch 6/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.5885\n",
      "Epoch 6: val_accuracy improved from 0.66850 to 0.67175, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.9045 - accuracy: 0.5885 - val_loss: 0.8318 - val_accuracy: 0.6718\n",
      "Epoch 7/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.5980\n",
      "Epoch 7: val_accuracy improved from 0.67175 to 0.70350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.8882 - accuracy: 0.5980 - val_loss: 0.7703 - val_accuracy: 0.7035\n",
      "Epoch 8/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8762 - accuracy: 0.6071\n",
      "Epoch 8: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.8762 - accuracy: 0.6071 - val_loss: 0.7814 - val_accuracy: 0.6930\n",
      "Epoch 9/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.6203\n",
      "Epoch 9: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 733s 488ms/step - loss: 0.8621 - accuracy: 0.6203 - val_loss: 0.7741 - val_accuracy: 0.6995\n",
      "Epoch 10/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8524 - accuracy: 0.6285\n",
      "Epoch 10: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.8524 - accuracy: 0.6285 - val_loss: 0.7664 - val_accuracy: 0.6980\n",
      "Epoch 11/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.6296\n",
      "Epoch 11: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.8390 - accuracy: 0.6296 - val_loss: 0.7767 - val_accuracy: 0.6902\n",
      "Epoch 12/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.6398\n",
      "Epoch 12: val_accuracy did not improve from 0.70350\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.8359 - accuracy: 0.6398 - val_loss: 0.7615 - val_accuracy: 0.7020\n",
      "Epoch 13/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.6423\n",
      "Epoch 13: val_accuracy improved from 0.70350 to 0.71125, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.8291 - accuracy: 0.6423 - val_loss: 0.7526 - val_accuracy: 0.7113\n",
      "Epoch 14/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.6506\n",
      "Epoch 14: val_accuracy improved from 0.71125 to 0.75000, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 731s 487ms/step - loss: 0.8117 - accuracy: 0.6506 - val_loss: 0.6696 - val_accuracy: 0.7500\n",
      "Epoch 15/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8033 - accuracy: 0.6561\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 745s 496ms/step - loss: 0.8033 - accuracy: 0.6561 - val_loss: 0.7872 - val_accuracy: 0.7290\n",
      "Epoch 16/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.6603\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.8005 - accuracy: 0.6603 - val_loss: 0.7240 - val_accuracy: 0.7212\n",
      "Epoch 17/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7895 - accuracy: 0.6629\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.7895 - accuracy: 0.6629 - val_loss: 0.7291 - val_accuracy: 0.7225\n",
      "Epoch 18/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7788 - accuracy: 0.6664\n",
      "Epoch 18: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 734s 490ms/step - loss: 0.7788 - accuracy: 0.6664 - val_loss: 0.7096 - val_accuracy: 0.7230\n",
      "Epoch 19/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7748 - accuracy: 0.6744\n",
      "Epoch 19: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.7748 - accuracy: 0.6744 - val_loss: 0.6625 - val_accuracy: 0.7498\n",
      "Epoch 20/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.6818\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.7604 - accuracy: 0.6818 - val_loss: 0.7019 - val_accuracy: 0.7335\n",
      "Epoch 21/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7570 - accuracy: 0.6772\n",
      "Epoch 21: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.7570 - accuracy: 0.6772 - val_loss: 0.6745 - val_accuracy: 0.7330\n",
      "Epoch 22/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7475 - accuracy: 0.6809\n",
      "Epoch 22: val_accuracy did not improve from 0.75000\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.7475 - accuracy: 0.6809 - val_loss: 0.7112 - val_accuracy: 0.7168\n",
      "Epoch 23/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.6957\n",
      "Epoch 23: val_accuracy improved from 0.75000 to 0.76700, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.7391 - accuracy: 0.6957 - val_loss: 0.6568 - val_accuracy: 0.7670\n",
      "Epoch 24/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7357 - accuracy: 0.6938\n",
      "Epoch 24: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 730s 487ms/step - loss: 0.7357 - accuracy: 0.6938 - val_loss: 0.7120 - val_accuracy: 0.7437\n",
      "Epoch 25/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.6973\n",
      "Epoch 25: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.7272 - accuracy: 0.6973 - val_loss: 0.7198 - val_accuracy: 0.7477\n",
      "Epoch 26/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.6992\n",
      "Epoch 26: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 734s 490ms/step - loss: 0.7191 - accuracy: 0.6992 - val_loss: 0.6966 - val_accuracy: 0.7570\n",
      "Epoch 27/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.7078\n",
      "Epoch 27: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 737s 491ms/step - loss: 0.7086 - accuracy: 0.7078 - val_loss: 0.6937 - val_accuracy: 0.7433\n",
      "Epoch 28/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.7080\n",
      "Epoch 28: val_accuracy did not improve from 0.76700\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.7073 - accuracy: 0.7080 - val_loss: 0.7187 - val_accuracy: 0.7495\n",
      "Epoch 29/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.7164\n",
      "Epoch 29: val_accuracy improved from 0.76700 to 0.77075, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6917 - accuracy: 0.7164 - val_loss: 0.6557 - val_accuracy: 0.7707\n",
      "Epoch 30/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.7167\n",
      "Epoch 30: val_accuracy did not improve from 0.77075\n",
      "1500/1500 [==============================] - 736s 491ms/step - loss: 0.6901 - accuracy: 0.7167 - val_loss: 0.6293 - val_accuracy: 0.7685\n",
      "Epoch 31/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.7225\n",
      "Epoch 31: val_accuracy improved from 0.77075 to 0.78625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.6797 - accuracy: 0.7225 - val_loss: 0.6509 - val_accuracy: 0.7862\n",
      "Epoch 32/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7243\n",
      "Epoch 32: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 727s 484ms/step - loss: 0.6764 - accuracy: 0.7243 - val_loss: 0.7232 - val_accuracy: 0.7475\n",
      "Epoch 33/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6703 - accuracy: 0.7330\n",
      "Epoch 33: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 726s 484ms/step - loss: 0.6703 - accuracy: 0.7330 - val_loss: 0.6426 - val_accuracy: 0.7540\n",
      "Epoch 34/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6615 - accuracy: 0.7322\n",
      "Epoch 34: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 727s 484ms/step - loss: 0.6615 - accuracy: 0.7322 - val_loss: 0.6363 - val_accuracy: 0.7697\n",
      "Epoch 35/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7371\n",
      "Epoch 35: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.6571 - accuracy: 0.7371 - val_loss: 0.6141 - val_accuracy: 0.7807\n",
      "Epoch 36/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7397\n",
      "Epoch 36: val_accuracy did not improve from 0.78625\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6531 - accuracy: 0.7397 - val_loss: 0.7280 - val_accuracy: 0.7513\n",
      "Epoch 37/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.7441\n",
      "Epoch 37: val_accuracy improved from 0.78625 to 0.79650, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.6436 - accuracy: 0.7441 - val_loss: 0.5980 - val_accuracy: 0.7965\n",
      "Epoch 38/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.7498\n",
      "Epoch 38: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6336 - accuracy: 0.7498 - val_loss: 0.5873 - val_accuracy: 0.7878\n",
      "Epoch 39/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.7552\n",
      "Epoch 39: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.6254 - accuracy: 0.7552 - val_loss: 0.7391 - val_accuracy: 0.7640\n",
      "Epoch 40/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.7553\n",
      "Epoch 40: val_accuracy did not improve from 0.79650\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6238 - accuracy: 0.7553 - val_loss: 0.6414 - val_accuracy: 0.7617\n",
      "Epoch 41/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7617\n",
      "Epoch 41: val_accuracy improved from 0.79650 to 0.80475, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.6093 - accuracy: 0.7617 - val_loss: 0.5609 - val_accuracy: 0.8048\n",
      "Epoch 42/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7642\n",
      "Epoch 42: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.6065 - accuracy: 0.7642 - val_loss: 0.5898 - val_accuracy: 0.7995\n",
      "Epoch 43/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.7655\n",
      "Epoch 43: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 729s 486ms/step - loss: 0.6023 - accuracy: 0.7655 - val_loss: 0.6141 - val_accuracy: 0.7958\n",
      "Epoch 44/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.7686\n",
      "Epoch 44: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 732s 488ms/step - loss: 0.5971 - accuracy: 0.7686 - val_loss: 0.7490 - val_accuracy: 0.7778\n",
      "Epoch 45/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.7746\n",
      "Epoch 45: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 738s 492ms/step - loss: 0.5881 - accuracy: 0.7746 - val_loss: 0.6979 - val_accuracy: 0.7845\n",
      "Epoch 46/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7736\n",
      "Epoch 46: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.5833 - accuracy: 0.7736 - val_loss: 0.6287 - val_accuracy: 0.7815\n",
      "Epoch 47/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7747\n",
      "Epoch 47: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.5843 - accuracy: 0.7747 - val_loss: 0.5917 - val_accuracy: 0.7843\n",
      "Epoch 48/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.7841\n",
      "Epoch 48: val_accuracy did not improve from 0.80475\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.5687 - accuracy: 0.7841 - val_loss: 0.6671 - val_accuracy: 0.7952\n",
      "Epoch 49/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.7798\n",
      "Epoch 49: val_accuracy improved from 0.80475 to 0.81350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.5712 - accuracy: 0.7798 - val_loss: 0.5863 - val_accuracy: 0.8135\n",
      "Epoch 50/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7857\n",
      "Epoch 50: val_accuracy did not improve from 0.81350\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.5653 - accuracy: 0.7857 - val_loss: 0.5616 - val_accuracy: 0.8117\n",
      "Epoch 51/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.7877\n",
      "Epoch 51: val_accuracy improved from 0.81350 to 0.81575, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.5599 - accuracy: 0.7877 - val_loss: 0.6071 - val_accuracy: 0.8158\n",
      "Epoch 52/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.7947\n",
      "Epoch 52: val_accuracy did not improve from 0.81575\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.5548 - accuracy: 0.7947 - val_loss: 0.6255 - val_accuracy: 0.8045\n",
      "Epoch 53/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.7912\n",
      "Epoch 53: val_accuracy did not improve from 0.81575\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.5467 - accuracy: 0.7912 - val_loss: 0.6471 - val_accuracy: 0.7993\n",
      "Epoch 54/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.7937\n",
      "Epoch 54: val_accuracy improved from 0.81575 to 0.82775, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.5438 - accuracy: 0.7937 - val_loss: 0.5188 - val_accuracy: 0.8278\n",
      "Epoch 55/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.8020\n",
      "Epoch 55: val_accuracy did not improve from 0.82775\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.5362 - accuracy: 0.8020 - val_loss: 0.5787 - val_accuracy: 0.7990\n",
      "Epoch 56/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.8008\n",
      "Epoch 56: val_accuracy improved from 0.82775 to 0.83275, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 764s 510ms/step - loss: 0.5328 - accuracy: 0.8008 - val_loss: 0.5255 - val_accuracy: 0.8328\n",
      "Epoch 57/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.8030\n",
      "Epoch 57: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.5289 - accuracy: 0.8030 - val_loss: 0.8143 - val_accuracy: 0.7295\n",
      "Epoch 58/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8020\n",
      "Epoch 58: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.5258 - accuracy: 0.8020 - val_loss: 0.5573 - val_accuracy: 0.8130\n",
      "Epoch 59/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.8117\n",
      "Epoch 59: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.5094 - accuracy: 0.8117 - val_loss: 0.5561 - val_accuracy: 0.8285\n",
      "Epoch 60/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.8148\n",
      "Epoch 60: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 757s 504ms/step - loss: 0.5074 - accuracy: 0.8148 - val_loss: 0.5645 - val_accuracy: 0.8207\n",
      "Epoch 61/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8123\n",
      "Epoch 61: val_accuracy did not improve from 0.83275\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.5084 - accuracy: 0.8123 - val_loss: 0.5686 - val_accuracy: 0.8310\n",
      "Epoch 62/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.8158\n",
      "Epoch 62: val_accuracy improved from 0.83275 to 0.83525, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.5032 - accuracy: 0.8158 - val_loss: 0.5154 - val_accuracy: 0.8353\n",
      "Epoch 63/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.8191\n",
      "Epoch 63: val_accuracy improved from 0.83525 to 0.83625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4987 - accuracy: 0.8191 - val_loss: 0.5229 - val_accuracy: 0.8363\n",
      "Epoch 64/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.8187\n",
      "Epoch 64: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.4944 - accuracy: 0.8187 - val_loss: 0.5820 - val_accuracy: 0.8225\n",
      "Epoch 65/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8172\n",
      "Epoch 65: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.5020 - accuracy: 0.8172 - val_loss: 0.5226 - val_accuracy: 0.8292\n",
      "Epoch 66/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.8224\n",
      "Epoch 66: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4869 - accuracy: 0.8224 - val_loss: 0.5649 - val_accuracy: 0.8288\n",
      "Epoch 67/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.8263\n",
      "Epoch 67: val_accuracy did not improve from 0.83625\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4812 - accuracy: 0.8263 - val_loss: 0.6090 - val_accuracy: 0.8347\n",
      "Epoch 68/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.8332\n",
      "Epoch 68: val_accuracy improved from 0.83625 to 0.84500, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4669 - accuracy: 0.8332 - val_loss: 0.5678 - val_accuracy: 0.8450\n",
      "Epoch 69/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.8332\n",
      "Epoch 69: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4710 - accuracy: 0.8332 - val_loss: 0.6529 - val_accuracy: 0.8320\n",
      "Epoch 70/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.8287\n",
      "Epoch 70: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.4743 - accuracy: 0.8287 - val_loss: 0.5901 - val_accuracy: 0.8278\n",
      "Epoch 71/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.8367\n",
      "Epoch 71: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.4680 - accuracy: 0.8367 - val_loss: 0.6101 - val_accuracy: 0.8305\n",
      "Epoch 72/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.8321\n",
      "Epoch 72: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.4642 - accuracy: 0.8321 - val_loss: 0.6540 - val_accuracy: 0.8303\n",
      "Epoch 73/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.8368\n",
      "Epoch 73: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.4612 - accuracy: 0.8368 - val_loss: 0.5910 - val_accuracy: 0.8205\n",
      "Epoch 74/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4573 - accuracy: 0.8391\n",
      "Epoch 74: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.4573 - accuracy: 0.8391 - val_loss: 0.5450 - val_accuracy: 0.8440\n",
      "Epoch 75/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.8424\n",
      "Epoch 75: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.4531 - accuracy: 0.8424 - val_loss: 0.6260 - val_accuracy: 0.8257\n",
      "Epoch 76/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8398\n",
      "Epoch 76: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4508 - accuracy: 0.8398 - val_loss: 0.6273 - val_accuracy: 0.8303\n",
      "Epoch 77/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8427\n",
      "Epoch 77: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4451 - accuracy: 0.8427 - val_loss: 0.5127 - val_accuracy: 0.8422\n",
      "Epoch 78/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8450\n",
      "Epoch 78: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4410 - accuracy: 0.8450 - val_loss: 0.5476 - val_accuracy: 0.8397\n",
      "Epoch 79/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8404\n",
      "Epoch 79: val_accuracy did not improve from 0.84500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4475 - accuracy: 0.8404 - val_loss: 0.5336 - val_accuracy: 0.8235\n",
      "Epoch 80/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8497\n",
      "Epoch 80: val_accuracy improved from 0.84500 to 0.84675, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4302 - accuracy: 0.8497 - val_loss: 0.5888 - val_accuracy: 0.8468\n",
      "Epoch 81/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4434 - accuracy: 0.8421\n",
      "Epoch 81: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.4434 - accuracy: 0.8421 - val_loss: 0.5270 - val_accuracy: 0.8393\n",
      "Epoch 82/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8561\n",
      "Epoch 82: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.4263 - accuracy: 0.8561 - val_loss: 0.9185 - val_accuracy: 0.7782\n",
      "Epoch 83/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8515\n",
      "Epoch 83: val_accuracy did not improve from 0.84675\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.4281 - accuracy: 0.8515 - val_loss: 0.5874 - val_accuracy: 0.8462\n",
      "Epoch 84/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8523\n",
      "Epoch 84: val_accuracy improved from 0.84675 to 0.85350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4223 - accuracy: 0.8523 - val_loss: 0.4755 - val_accuracy: 0.8535\n",
      "Epoch 85/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8564\n",
      "Epoch 85: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4235 - accuracy: 0.8564 - val_loss: 0.4876 - val_accuracy: 0.8505\n",
      "Epoch 86/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8580\n",
      "Epoch 86: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.4162 - accuracy: 0.8580 - val_loss: 0.5609 - val_accuracy: 0.8347\n",
      "Epoch 87/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8520\n",
      "Epoch 87: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4246 - accuracy: 0.8520 - val_loss: 0.5541 - val_accuracy: 0.8510\n",
      "Epoch 88/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8623\n",
      "Epoch 88: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4128 - accuracy: 0.8623 - val_loss: 0.5537 - val_accuracy: 0.8432\n",
      "Epoch 89/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.8581\n",
      "Epoch 89: val_accuracy did not improve from 0.85350\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.4173 - accuracy: 0.8581 - val_loss: 0.4995 - val_accuracy: 0.8447\n",
      "Epoch 90/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8601\n",
      "Epoch 90: val_accuracy improved from 0.85350 to 0.85500, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.4080 - accuracy: 0.8601 - val_loss: 0.5557 - val_accuracy: 0.8550\n",
      "Epoch 91/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8617\n",
      "Epoch 91: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4084 - accuracy: 0.8617 - val_loss: 0.5751 - val_accuracy: 0.8375\n",
      "Epoch 92/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8612\n",
      "Epoch 92: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.4101 - accuracy: 0.8612 - val_loss: 0.4743 - val_accuracy: 0.8543\n",
      "Epoch 93/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.8608\n",
      "Epoch 93: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.4023 - accuracy: 0.8608 - val_loss: 0.6251 - val_accuracy: 0.8422\n",
      "Epoch 94/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8645\n",
      "Epoch 94: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4021 - accuracy: 0.8645 - val_loss: 0.6131 - val_accuracy: 0.8435\n",
      "Epoch 95/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8664\n",
      "Epoch 95: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.4003 - accuracy: 0.8664 - val_loss: 0.5612 - val_accuracy: 0.8545\n",
      "Epoch 96/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8668\n",
      "Epoch 96: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.3941 - accuracy: 0.8668 - val_loss: 0.8340 - val_accuracy: 0.8008\n",
      "Epoch 97/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8677\n",
      "Epoch 97: val_accuracy did not improve from 0.85500\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.3985 - accuracy: 0.8677 - val_loss: 0.4928 - val_accuracy: 0.8533\n",
      "Epoch 98/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8688\n",
      "Epoch 98: val_accuracy improved from 0.85500 to 0.85650, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3914 - accuracy: 0.8688 - val_loss: 0.5252 - val_accuracy: 0.8565\n",
      "Epoch 99/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8697\n",
      "Epoch 99: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.3973 - accuracy: 0.8697 - val_loss: 0.5896 - val_accuracy: 0.8520\n",
      "Epoch 100/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8723\n",
      "Epoch 100: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3855 - accuracy: 0.8723 - val_loss: 0.6307 - val_accuracy: 0.8345\n",
      "Epoch 101/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8710\n",
      "Epoch 101: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3894 - accuracy: 0.8710 - val_loss: 0.6345 - val_accuracy: 0.8320\n",
      "Epoch 102/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8684\n",
      "Epoch 102: val_accuracy did not improve from 0.85650\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.3891 - accuracy: 0.8684 - val_loss: 0.7340 - val_accuracy: 0.8288\n",
      "Epoch 103/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8738\n",
      "Epoch 103: val_accuracy improved from 0.85650 to 0.86275, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3830 - accuracy: 0.8738 - val_loss: 0.5133 - val_accuracy: 0.8627\n",
      "Epoch 104/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.8748\n",
      "Epoch 104: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3823 - accuracy: 0.8748 - val_loss: 0.5505 - val_accuracy: 0.8535\n",
      "Epoch 105/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8796\n",
      "Epoch 105: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.3718 - accuracy: 0.8796 - val_loss: 0.5561 - val_accuracy: 0.8515\n",
      "Epoch 106/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.8758\n",
      "Epoch 106: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.3762 - accuracy: 0.8758 - val_loss: 0.6139 - val_accuracy: 0.8485\n",
      "Epoch 107/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.8766\n",
      "Epoch 107: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3737 - accuracy: 0.8766 - val_loss: 0.6620 - val_accuracy: 0.8393\n",
      "Epoch 108/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8739\n",
      "Epoch 108: val_accuracy did not improve from 0.86275\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3779 - accuracy: 0.8739 - val_loss: 0.4906 - val_accuracy: 0.8610\n",
      "Epoch 109/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8783\n",
      "Epoch 109: val_accuracy improved from 0.86275 to 0.86575, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3731 - accuracy: 0.8783 - val_loss: 0.5566 - val_accuracy: 0.8658\n",
      "Epoch 110/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8743\n",
      "Epoch 110: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3759 - accuracy: 0.8743 - val_loss: 0.5314 - val_accuracy: 0.8643\n",
      "Epoch 111/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8813\n",
      "Epoch 111: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3678 - accuracy: 0.8813 - val_loss: 0.8752 - val_accuracy: 0.7822\n",
      "Epoch 112/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8753\n",
      "Epoch 112: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3703 - accuracy: 0.8753 - val_loss: 0.5261 - val_accuracy: 0.8533\n",
      "Epoch 113/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.8864\n",
      "Epoch 113: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3528 - accuracy: 0.8864 - val_loss: 0.6082 - val_accuracy: 0.8540\n",
      "Epoch 114/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8746\n",
      "Epoch 114: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 760s 507ms/step - loss: 0.3689 - accuracy: 0.8746 - val_loss: 0.7334 - val_accuracy: 0.8328\n",
      "Epoch 115/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8814\n",
      "Epoch 115: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.3636 - accuracy: 0.8814 - val_loss: 0.5768 - val_accuracy: 0.8565\n",
      "Epoch 116/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8788\n",
      "Epoch 116: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.3662 - accuracy: 0.8788 - val_loss: 0.5324 - val_accuracy: 0.8472\n",
      "Epoch 117/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.8817\n",
      "Epoch 117: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3598 - accuracy: 0.8817 - val_loss: 0.5813 - val_accuracy: 0.8397\n",
      "Epoch 118/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8800\n",
      "Epoch 118: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3602 - accuracy: 0.8800 - val_loss: 0.5183 - val_accuracy: 0.8500\n",
      "Epoch 119/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8844\n",
      "Epoch 119: val_accuracy did not improve from 0.86575\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.3566 - accuracy: 0.8844 - val_loss: 0.7469 - val_accuracy: 0.8332\n",
      "Epoch 120/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8857\n",
      "Epoch 120: val_accuracy improved from 0.86575 to 0.86625, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.3539 - accuracy: 0.8857 - val_loss: 0.5423 - val_accuracy: 0.8662\n",
      "Epoch 121/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.8834\n",
      "Epoch 121: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3535 - accuracy: 0.8834 - val_loss: 0.5976 - val_accuracy: 0.8528\n",
      "Epoch 122/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8915\n",
      "Epoch 122: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3464 - accuracy: 0.8915 - val_loss: 0.5434 - val_accuracy: 0.8572\n",
      "Epoch 123/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8877\n",
      "Epoch 123: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 768s 512ms/step - loss: 0.3461 - accuracy: 0.8877 - val_loss: 0.5654 - val_accuracy: 0.8597\n",
      "Epoch 124/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8827\n",
      "Epoch 124: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.3558 - accuracy: 0.8827 - val_loss: 0.6053 - val_accuracy: 0.8450\n",
      "Epoch 125/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8877\n",
      "Epoch 125: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.3508 - accuracy: 0.8877 - val_loss: 0.5532 - val_accuracy: 0.8500\n",
      "Epoch 126/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8898\n",
      "Epoch 126: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3446 - accuracy: 0.8898 - val_loss: 0.4910 - val_accuracy: 0.8637\n",
      "Epoch 127/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8921\n",
      "Epoch 127: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3400 - accuracy: 0.8921 - val_loss: 0.5339 - val_accuracy: 0.8633\n",
      "Epoch 128/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8902\n",
      "Epoch 128: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3409 - accuracy: 0.8902 - val_loss: 0.5272 - val_accuracy: 0.8635\n",
      "Epoch 129/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8913\n",
      "Epoch 129: val_accuracy did not improve from 0.86625\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.3441 - accuracy: 0.8913 - val_loss: 0.5396 - val_accuracy: 0.8595\n",
      "Epoch 130/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8903\n",
      "Epoch 130: val_accuracy improved from 0.86625 to 0.86675, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.3441 - accuracy: 0.8903 - val_loss: 0.5354 - val_accuracy: 0.8668\n",
      "Epoch 131/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8900\n",
      "Epoch 131: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3425 - accuracy: 0.8900 - val_loss: 0.5735 - val_accuracy: 0.8487\n",
      "Epoch 132/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.8901\n",
      "Epoch 132: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 761s 508ms/step - loss: 0.3435 - accuracy: 0.8901 - val_loss: 0.5628 - val_accuracy: 0.8515\n",
      "Epoch 133/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.8945\n",
      "Epoch 133: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.3362 - accuracy: 0.8945 - val_loss: 0.6370 - val_accuracy: 0.8510\n",
      "Epoch 134/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.8928\n",
      "Epoch 134: val_accuracy did not improve from 0.86675\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.3298 - accuracy: 0.8928 - val_loss: 0.6382 - val_accuracy: 0.8512\n",
      "Epoch 135/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8957\n",
      "Epoch 135: val_accuracy improved from 0.86675 to 0.87025, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.3329 - accuracy: 0.8957 - val_loss: 0.5651 - val_accuracy: 0.8702\n",
      "Epoch 136/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8947\n",
      "Epoch 136: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.3280 - accuracy: 0.8947 - val_loss: 0.6832 - val_accuracy: 0.8320\n",
      "Epoch 137/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8964\n",
      "Epoch 137: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.3296 - accuracy: 0.8964 - val_loss: 0.5452 - val_accuracy: 0.8618\n",
      "Epoch 138/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8962\n",
      "Epoch 138: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3270 - accuracy: 0.8962 - val_loss: 0.6319 - val_accuracy: 0.8570\n",
      "Epoch 139/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8953\n",
      "Epoch 139: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.3288 - accuracy: 0.8953 - val_loss: 0.5578 - val_accuracy: 0.8503\n",
      "Epoch 140/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8952\n",
      "Epoch 140: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 762s 508ms/step - loss: 0.3329 - accuracy: 0.8952 - val_loss: 0.4714 - val_accuracy: 0.8655\n",
      "Epoch 141/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9030\n",
      "Epoch 141: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 761s 508ms/step - loss: 0.3201 - accuracy: 0.9030 - val_loss: 0.6045 - val_accuracy: 0.8615\n",
      "Epoch 142/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9040\n",
      "Epoch 142: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.3142 - accuracy: 0.9040 - val_loss: 0.6268 - val_accuracy: 0.8665\n",
      "Epoch 143/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8986\n",
      "Epoch 143: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.3220 - accuracy: 0.8986 - val_loss: 0.5623 - val_accuracy: 0.8537\n",
      "Epoch 144/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9013\n",
      "Epoch 144: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.3185 - accuracy: 0.9013 - val_loss: 0.5865 - val_accuracy: 0.8590\n",
      "Epoch 145/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.9001\n",
      "Epoch 145: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3180 - accuracy: 0.9001 - val_loss: 0.4967 - val_accuracy: 0.8625\n",
      "Epoch 146/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.9033\n",
      "Epoch 146: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.3178 - accuracy: 0.9033 - val_loss: 0.6036 - val_accuracy: 0.8630\n",
      "Epoch 147/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9017\n",
      "Epoch 147: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 748s 498ms/step - loss: 0.3207 - accuracy: 0.9017 - val_loss: 0.5571 - val_accuracy: 0.8627\n",
      "Epoch 148/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9062\n",
      "Epoch 148: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.3125 - accuracy: 0.9062 - val_loss: 0.5739 - val_accuracy: 0.8625\n",
      "Epoch 149/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.9039\n",
      "Epoch 149: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.3160 - accuracy: 0.9039 - val_loss: 0.6418 - val_accuracy: 0.8553\n",
      "Epoch 150/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.9041\n",
      "Epoch 150: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3174 - accuracy: 0.9041 - val_loss: 0.5952 - val_accuracy: 0.8470\n",
      "Epoch 151/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9047\n",
      "Epoch 151: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3110 - accuracy: 0.9047 - val_loss: 0.6212 - val_accuracy: 0.8615\n",
      "Epoch 152/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.9022\n",
      "Epoch 152: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3144 - accuracy: 0.9022 - val_loss: 0.8678 - val_accuracy: 0.8100\n",
      "Epoch 153/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.9030\n",
      "Epoch 153: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3155 - accuracy: 0.9030 - val_loss: 0.5589 - val_accuracy: 0.8505\n",
      "Epoch 154/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9057\n",
      "Epoch 154: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3111 - accuracy: 0.9057 - val_loss: 0.6499 - val_accuracy: 0.8435\n",
      "Epoch 155/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9067\n",
      "Epoch 155: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.3108 - accuracy: 0.9067 - val_loss: 0.5307 - val_accuracy: 0.8618\n",
      "Epoch 156/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.9073\n",
      "Epoch 156: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 735s 490ms/step - loss: 0.3052 - accuracy: 0.9073 - val_loss: 0.7724 - val_accuracy: 0.8167\n",
      "Epoch 157/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.9103\n",
      "Epoch 157: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.3043 - accuracy: 0.9103 - val_loss: 0.6471 - val_accuracy: 0.8493\n",
      "Epoch 158/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9043\n",
      "Epoch 158: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3127 - accuracy: 0.9043 - val_loss: 0.5510 - val_accuracy: 0.8468\n",
      "Epoch 159/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9097\n",
      "Epoch 159: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3030 - accuracy: 0.9097 - val_loss: 0.6106 - val_accuracy: 0.8575\n",
      "Epoch 160/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.9057\n",
      "Epoch 160: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3060 - accuracy: 0.9057 - val_loss: 0.5879 - val_accuracy: 0.8608\n",
      "Epoch 161/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9083\n",
      "Epoch 161: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 740s 493ms/step - loss: 0.3047 - accuracy: 0.9083 - val_loss: 0.5434 - val_accuracy: 0.8618\n",
      "Epoch 162/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9093\n",
      "Epoch 162: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.3017 - accuracy: 0.9093 - val_loss: 0.6106 - val_accuracy: 0.8553\n",
      "Epoch 163/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9105\n",
      "Epoch 163: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2990 - accuracy: 0.9105 - val_loss: 0.6778 - val_accuracy: 0.8587\n",
      "Epoch 164/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9078\n",
      "Epoch 164: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3019 - accuracy: 0.9078 - val_loss: 0.6376 - val_accuracy: 0.8405\n",
      "Epoch 165/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.9067\n",
      "Epoch 165: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.3090 - accuracy: 0.9067 - val_loss: 0.5399 - val_accuracy: 0.8602\n",
      "Epoch 166/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9088\n",
      "Epoch 166: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2985 - accuracy: 0.9088 - val_loss: 0.6354 - val_accuracy: 0.8537\n",
      "Epoch 167/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.9116\n",
      "Epoch 167: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 742s 495ms/step - loss: 0.3014 - accuracy: 0.9116 - val_loss: 0.6144 - val_accuracy: 0.8615\n",
      "Epoch 168/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9149\n",
      "Epoch 168: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.2941 - accuracy: 0.9149 - val_loss: 0.5417 - val_accuracy: 0.8675\n",
      "Epoch 169/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9121\n",
      "Epoch 169: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 746s 498ms/step - loss: 0.2905 - accuracy: 0.9121 - val_loss: 0.5251 - val_accuracy: 0.8553\n",
      "Epoch 170/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9073\n",
      "Epoch 170: val_accuracy did not improve from 0.87025\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2962 - accuracy: 0.9073 - val_loss: 0.5392 - val_accuracy: 0.8620\n",
      "Epoch 171/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9146\n",
      "Epoch 171: val_accuracy improved from 0.87025 to 0.87150, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2932 - accuracy: 0.9146 - val_loss: 0.5480 - val_accuracy: 0.8715\n",
      "Epoch 172/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9087\n",
      "Epoch 172: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 747s 498ms/step - loss: 0.2996 - accuracy: 0.9087 - val_loss: 0.6608 - val_accuracy: 0.8497\n",
      "Epoch 173/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9109\n",
      "Epoch 173: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 745s 496ms/step - loss: 0.2990 - accuracy: 0.9109 - val_loss: 0.5804 - val_accuracy: 0.8675\n",
      "Epoch 174/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9130\n",
      "Epoch 174: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 759s 506ms/step - loss: 0.2914 - accuracy: 0.9130 - val_loss: 0.6666 - val_accuracy: 0.8522\n",
      "Epoch 175/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9149\n",
      "Epoch 175: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2894 - accuracy: 0.9149 - val_loss: 0.6381 - val_accuracy: 0.8455\n",
      "Epoch 176/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9112\n",
      "Epoch 176: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 743s 496ms/step - loss: 0.2942 - accuracy: 0.9112 - val_loss: 0.6315 - val_accuracy: 0.8270\n",
      "Epoch 177/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9146\n",
      "Epoch 177: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2889 - accuracy: 0.9146 - val_loss: 0.7251 - val_accuracy: 0.8280\n",
      "Epoch 178/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9187\n",
      "Epoch 178: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 746s 497ms/step - loss: 0.2884 - accuracy: 0.9187 - val_loss: 0.6678 - val_accuracy: 0.8677\n",
      "Epoch 179/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9131\n",
      "Epoch 179: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.2971 - accuracy: 0.9131 - val_loss: 0.5331 - val_accuracy: 0.8620\n",
      "Epoch 180/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9161\n",
      "Epoch 180: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2878 - accuracy: 0.9161 - val_loss: 0.6271 - val_accuracy: 0.8370\n",
      "Epoch 181/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9137\n",
      "Epoch 181: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2909 - accuracy: 0.9137 - val_loss: 0.9455 - val_accuracy: 0.7930\n",
      "Epoch 182/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9178\n",
      "Epoch 182: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2858 - accuracy: 0.9178 - val_loss: 0.5681 - val_accuracy: 0.8640\n",
      "Epoch 183/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9141\n",
      "Epoch 183: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 770s 513ms/step - loss: 0.2874 - accuracy: 0.9141 - val_loss: 0.6032 - val_accuracy: 0.8572\n",
      "Epoch 184/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.9124\n",
      "Epoch 184: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2951 - accuracy: 0.9124 - val_loss: 0.5316 - val_accuracy: 0.8673\n",
      "Epoch 185/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9182\n",
      "Epoch 185: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2881 - accuracy: 0.9182 - val_loss: 0.5979 - val_accuracy: 0.8683\n",
      "Epoch 186/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9205\n",
      "Epoch 186: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2823 - accuracy: 0.9205 - val_loss: 0.6136 - val_accuracy: 0.8610\n",
      "Epoch 187/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.9125\n",
      "Epoch 187: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2921 - accuracy: 0.9125 - val_loss: 0.6200 - val_accuracy: 0.8540\n",
      "Epoch 188/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.9187\n",
      "Epoch 188: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.2826 - accuracy: 0.9187 - val_loss: 0.5010 - val_accuracy: 0.8597\n",
      "Epoch 189/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9184\n",
      "Epoch 189: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2858 - accuracy: 0.9184 - val_loss: 0.6183 - val_accuracy: 0.8677\n",
      "Epoch 190/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9170\n",
      "Epoch 190: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.2819 - accuracy: 0.9170 - val_loss: 0.5665 - val_accuracy: 0.8712\n",
      "Epoch 191/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9158\n",
      "Epoch 191: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.2834 - accuracy: 0.9158 - val_loss: 0.6570 - val_accuracy: 0.8353\n",
      "Epoch 192/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9224\n",
      "Epoch 192: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2750 - accuracy: 0.9224 - val_loss: 0.6368 - val_accuracy: 0.8530\n",
      "Epoch 193/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9216\n",
      "Epoch 193: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.2771 - accuracy: 0.9216 - val_loss: 0.5601 - val_accuracy: 0.8555\n",
      "Epoch 194/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9218\n",
      "Epoch 194: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 755s 503ms/step - loss: 0.2771 - accuracy: 0.9218 - val_loss: 0.6327 - val_accuracy: 0.8587\n",
      "Epoch 195/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9204\n",
      "Epoch 195: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 502ms/step - loss: 0.2797 - accuracy: 0.9204 - val_loss: 0.6723 - val_accuracy: 0.8353\n",
      "Epoch 196/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9232\n",
      "Epoch 196: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2786 - accuracy: 0.9232 - val_loss: 0.5733 - val_accuracy: 0.8658\n",
      "Epoch 197/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9217\n",
      "Epoch 197: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2798 - accuracy: 0.9217 - val_loss: 0.6187 - val_accuracy: 0.8472\n",
      "Epoch 198/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9242\n",
      "Epoch 198: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.2739 - accuracy: 0.9242 - val_loss: 0.5745 - val_accuracy: 0.8633\n",
      "Epoch 199/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9182\n",
      "Epoch 199: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2813 - accuracy: 0.9182 - val_loss: 0.5253 - val_accuracy: 0.8515\n",
      "Epoch 200/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.9211\n",
      "Epoch 200: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2756 - accuracy: 0.9211 - val_loss: 0.6415 - val_accuracy: 0.8612\n",
      "Epoch 201/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9220\n",
      "Epoch 201: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2736 - accuracy: 0.9220 - val_loss: 0.5672 - val_accuracy: 0.8605\n",
      "Epoch 202/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9277\n",
      "Epoch 202: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2697 - accuracy: 0.9277 - val_loss: 0.6030 - val_accuracy: 0.8627\n",
      "Epoch 203/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9258\n",
      "Epoch 203: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2661 - accuracy: 0.9258 - val_loss: 0.6926 - val_accuracy: 0.8522\n",
      "Epoch 204/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9295\n",
      "Epoch 204: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2618 - accuracy: 0.9295 - val_loss: 0.6071 - val_accuracy: 0.8627\n",
      "Epoch 205/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9227\n",
      "Epoch 205: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 769s 513ms/step - loss: 0.2763 - accuracy: 0.9227 - val_loss: 0.5368 - val_accuracy: 0.8618\n",
      "Epoch 206/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9241\n",
      "Epoch 206: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2676 - accuracy: 0.9241 - val_loss: 0.6840 - val_accuracy: 0.8570\n",
      "Epoch 207/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9248\n",
      "Epoch 207: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2697 - accuracy: 0.9248 - val_loss: 0.5905 - val_accuracy: 0.8597\n",
      "Epoch 208/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9258\n",
      "Epoch 208: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 502ms/step - loss: 0.2654 - accuracy: 0.9258 - val_loss: 0.5460 - val_accuracy: 0.8698\n",
      "Epoch 209/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9261\n",
      "Epoch 209: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2654 - accuracy: 0.9261 - val_loss: 0.6548 - val_accuracy: 0.8595\n",
      "Epoch 210/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9239\n",
      "Epoch 210: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2665 - accuracy: 0.9239 - val_loss: 0.6374 - val_accuracy: 0.8482\n",
      "Epoch 211/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9265\n",
      "Epoch 211: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2691 - accuracy: 0.9265 - val_loss: 0.5450 - val_accuracy: 0.8620\n",
      "Epoch 212/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9273\n",
      "Epoch 212: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2667 - accuracy: 0.9273 - val_loss: 0.5865 - val_accuracy: 0.8660\n",
      "Epoch 213/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9263\n",
      "Epoch 213: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2702 - accuracy: 0.9263 - val_loss: 0.6127 - val_accuracy: 0.8503\n",
      "Epoch 214/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9269\n",
      "Epoch 214: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2625 - accuracy: 0.9269 - val_loss: 0.6540 - val_accuracy: 0.8382\n",
      "Epoch 215/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9291\n",
      "Epoch 215: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2610 - accuracy: 0.9291 - val_loss: 0.5695 - val_accuracy: 0.8630\n",
      "Epoch 216/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9251\n",
      "Epoch 216: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2655 - accuracy: 0.9251 - val_loss: 0.6897 - val_accuracy: 0.8505\n",
      "Epoch 217/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9307\n",
      "Epoch 217: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2577 - accuracy: 0.9307 - val_loss: 0.5701 - val_accuracy: 0.8622\n",
      "Epoch 218/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9279\n",
      "Epoch 218: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 750s 500ms/step - loss: 0.2648 - accuracy: 0.9279 - val_loss: 0.6189 - val_accuracy: 0.8512\n",
      "Epoch 219/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9305\n",
      "Epoch 219: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 500ms/step - loss: 0.2607 - accuracy: 0.9305 - val_loss: 0.5832 - val_accuracy: 0.8577\n",
      "Epoch 220/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9300\n",
      "Epoch 220: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 763s 509ms/step - loss: 0.2614 - accuracy: 0.9300 - val_loss: 0.6382 - val_accuracy: 0.8555\n",
      "Epoch 221/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9280\n",
      "Epoch 221: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 749s 500ms/step - loss: 0.2619 - accuracy: 0.9280 - val_loss: 0.6007 - val_accuracy: 0.8568\n",
      "Epoch 222/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9314\n",
      "Epoch 222: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2599 - accuracy: 0.9314 - val_loss: 0.6310 - val_accuracy: 0.8447\n",
      "Epoch 223/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9307\n",
      "Epoch 223: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2598 - accuracy: 0.9307 - val_loss: 0.6310 - val_accuracy: 0.8700\n",
      "Epoch 224/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9290\n",
      "Epoch 224: val_accuracy did not improve from 0.87150\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2567 - accuracy: 0.9290 - val_loss: 0.8502 - val_accuracy: 0.8325\n",
      "Epoch 225/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9299\n",
      "Epoch 225: val_accuracy improved from 0.87150 to 0.87350, saving model to D:/testing_1_cvt/Model_CVT_prueba1_04S-UNIWARD_1729515488.1369982\\saved-model.hdf5\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2576 - accuracy: 0.9299 - val_loss: 0.5694 - val_accuracy: 0.8735\n",
      "Epoch 226/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9308\n",
      "Epoch 226: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2578 - accuracy: 0.9308 - val_loss: 0.7478 - val_accuracy: 0.8325\n",
      "Epoch 227/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9303\n",
      "Epoch 227: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 760s 507ms/step - loss: 0.2592 - accuracy: 0.9303 - val_loss: 0.6790 - val_accuracy: 0.8508\n",
      "Epoch 228/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9324\n",
      "Epoch 228: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2559 - accuracy: 0.9324 - val_loss: 0.5862 - val_accuracy: 0.8625\n",
      "Epoch 229/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9325\n",
      "Epoch 229: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2543 - accuracy: 0.9325 - val_loss: 0.5778 - val_accuracy: 0.8645\n",
      "Epoch 230/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9317\n",
      "Epoch 230: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 749s 499ms/step - loss: 0.2543 - accuracy: 0.9317 - val_loss: 0.7233 - val_accuracy: 0.8553\n",
      "Epoch 231/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9318\n",
      "Epoch 231: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2620 - accuracy: 0.9318 - val_loss: 0.6313 - val_accuracy: 0.8545\n",
      "Epoch 232/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9338\n",
      "Epoch 232: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2511 - accuracy: 0.9338 - val_loss: 0.6812 - val_accuracy: 0.8585\n",
      "Epoch 233/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9339\n",
      "Epoch 233: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2565 - accuracy: 0.9339 - val_loss: 0.6370 - val_accuracy: 0.8495\n",
      "Epoch 234/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9323\n",
      "Epoch 234: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 765s 510ms/step - loss: 0.2558 - accuracy: 0.9323 - val_loss: 0.5738 - val_accuracy: 0.8545\n",
      "Epoch 235/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9334\n",
      "Epoch 235: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2521 - accuracy: 0.9334 - val_loss: 0.6500 - val_accuracy: 0.8705\n",
      "Epoch 236/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9360\n",
      "Epoch 236: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2488 - accuracy: 0.9360 - val_loss: 0.7044 - val_accuracy: 0.8515\n",
      "Epoch 237/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9358\n",
      "Epoch 237: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2504 - accuracy: 0.9358 - val_loss: 0.6207 - val_accuracy: 0.8658\n",
      "Epoch 238/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9332\n",
      "Epoch 238: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2545 - accuracy: 0.9332 - val_loss: 0.5878 - val_accuracy: 0.8708\n",
      "Epoch 239/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9365\n",
      "Epoch 239: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 745s 497ms/step - loss: 0.2490 - accuracy: 0.9365 - val_loss: 0.5358 - val_accuracy: 0.8683\n",
      "Epoch 240/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9359\n",
      "Epoch 240: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 733s 489ms/step - loss: 0.2474 - accuracy: 0.9359 - val_loss: 0.5889 - val_accuracy: 0.8505\n",
      "Epoch 241/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9363\n",
      "Epoch 241: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2468 - accuracy: 0.9363 - val_loss: 0.7701 - val_accuracy: 0.8445\n",
      "Epoch 242/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9382\n",
      "Epoch 242: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 738s 492ms/step - loss: 0.2454 - accuracy: 0.9382 - val_loss: 0.7489 - val_accuracy: 0.8465\n",
      "Epoch 243/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9377\n",
      "Epoch 243: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 739s 493ms/step - loss: 0.2444 - accuracy: 0.9377 - val_loss: 0.5696 - val_accuracy: 0.8610\n",
      "Epoch 244/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.9353\n",
      "Epoch 244: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.2453 - accuracy: 0.9353 - val_loss: 0.7373 - val_accuracy: 0.8332\n",
      "Epoch 245/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9337\n",
      "Epoch 245: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.2552 - accuracy: 0.9337 - val_loss: 0.6992 - val_accuracy: 0.8443\n",
      "Epoch 246/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9355\n",
      "Epoch 246: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 495ms/step - loss: 0.2542 - accuracy: 0.9355 - val_loss: 0.5948 - val_accuracy: 0.8583\n",
      "Epoch 247/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9355\n",
      "Epoch 247: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 741s 494ms/step - loss: 0.2463 - accuracy: 0.9355 - val_loss: 0.6106 - val_accuracy: 0.8640\n",
      "Epoch 248/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9352\n",
      "Epoch 248: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 743s 496ms/step - loss: 0.2511 - accuracy: 0.9352 - val_loss: 0.7702 - val_accuracy: 0.8267\n",
      "Epoch 249/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9359\n",
      "Epoch 249: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 736s 491ms/step - loss: 0.2514 - accuracy: 0.9359 - val_loss: 0.7582 - val_accuracy: 0.8400\n",
      "Epoch 250/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9356\n",
      "Epoch 250: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 736s 490ms/step - loss: 0.2494 - accuracy: 0.9356 - val_loss: 0.5887 - val_accuracy: 0.8580\n",
      "Epoch 251/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9373\n",
      "Epoch 251: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 734s 489ms/step - loss: 0.2432 - accuracy: 0.9373 - val_loss: 0.6428 - val_accuracy: 0.8635\n",
      "Epoch 252/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9351\n",
      "Epoch 252: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 737s 491ms/step - loss: 0.2497 - accuracy: 0.9351 - val_loss: 0.5972 - val_accuracy: 0.8690\n",
      "Epoch 253/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.9398\n",
      "Epoch 253: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 748s 499ms/step - loss: 0.2420 - accuracy: 0.9398 - val_loss: 0.6237 - val_accuracy: 0.8535\n",
      "Epoch 254/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9378\n",
      "Epoch 254: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2451 - accuracy: 0.9378 - val_loss: 0.6356 - val_accuracy: 0.8485\n",
      "Epoch 255/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9337\n",
      "Epoch 255: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 753s 502ms/step - loss: 0.2440 - accuracy: 0.9337 - val_loss: 0.6486 - val_accuracy: 0.8555\n",
      "Epoch 256/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9337\n",
      "Epoch 256: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 768s 512ms/step - loss: 0.2575 - accuracy: 0.9337 - val_loss: 0.6541 - val_accuracy: 0.8580\n",
      "Epoch 257/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9352\n",
      "Epoch 257: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2449 - accuracy: 0.9352 - val_loss: 0.6802 - val_accuracy: 0.8555\n",
      "Epoch 258/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9400\n",
      "Epoch 258: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2448 - accuracy: 0.9400 - val_loss: 0.6648 - val_accuracy: 0.8562\n",
      "Epoch 259/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9362\n",
      "Epoch 259: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2535 - accuracy: 0.9362 - val_loss: 0.6151 - val_accuracy: 0.8583\n",
      "Epoch 260/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9352\n",
      "Epoch 260: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2510 - accuracy: 0.9352 - val_loss: 0.7149 - val_accuracy: 0.8580\n",
      "Epoch 261/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9403\n",
      "Epoch 261: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2433 - accuracy: 0.9403 - val_loss: 0.6782 - val_accuracy: 0.8487\n",
      "Epoch 262/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9398\n",
      "Epoch 262: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 751s 501ms/step - loss: 0.2472 - accuracy: 0.9398 - val_loss: 0.7256 - val_accuracy: 0.8443\n",
      "Epoch 263/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9417\n",
      "Epoch 263: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2383 - accuracy: 0.9417 - val_loss: 0.6007 - val_accuracy: 0.8605\n",
      "Epoch 264/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9442\n",
      "Epoch 264: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 744s 496ms/step - loss: 0.2343 - accuracy: 0.9442 - val_loss: 0.6610 - val_accuracy: 0.8622\n",
      "Epoch 265/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9385\n",
      "Epoch 265: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2446 - accuracy: 0.9385 - val_loss: 0.7656 - val_accuracy: 0.8393\n",
      "Epoch 266/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9388\n",
      "Epoch 266: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2451 - accuracy: 0.9388 - val_loss: 0.6306 - val_accuracy: 0.8585\n",
      "Epoch 267/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9384\n",
      "Epoch 267: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 506ms/step - loss: 0.2475 - accuracy: 0.9384 - val_loss: 0.6651 - val_accuracy: 0.8558\n",
      "Epoch 268/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9439\n",
      "Epoch 268: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 758s 505ms/step - loss: 0.2382 - accuracy: 0.9439 - val_loss: 0.6001 - val_accuracy: 0.8618\n",
      "Epoch 269/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9441\n",
      "Epoch 269: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 752s 501ms/step - loss: 0.2297 - accuracy: 0.9441 - val_loss: 0.6424 - val_accuracy: 0.8630\n",
      "Epoch 270/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9409\n",
      "Epoch 270: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2403 - accuracy: 0.9409 - val_loss: 0.5851 - val_accuracy: 0.8577\n",
      "Epoch 271/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9416\n",
      "Epoch 271: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 754s 503ms/step - loss: 0.2358 - accuracy: 0.9416 - val_loss: 0.6622 - val_accuracy: 0.8568\n",
      "Epoch 272/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9418\n",
      "Epoch 272: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 755s 504ms/step - loss: 0.2347 - accuracy: 0.9418 - val_loss: 0.6168 - val_accuracy: 0.8580\n",
      "Epoch 273/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9417\n",
      "Epoch 273: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2390 - accuracy: 0.9417 - val_loss: 0.6668 - val_accuracy: 0.8625\n",
      "Epoch 274/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9428\n",
      "Epoch 274: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 756s 504ms/step - loss: 0.2316 - accuracy: 0.9428 - val_loss: 0.6522 - val_accuracy: 0.8577\n",
      "Epoch 275/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9436\n",
      "Epoch 275: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 757s 505ms/step - loss: 0.2355 - accuracy: 0.9436 - val_loss: 0.6290 - val_accuracy: 0.8685\n",
      "Epoch 276/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9435\n",
      "Epoch 276: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 760s 506ms/step - loss: 0.2308 - accuracy: 0.9435 - val_loss: 0.7292 - val_accuracy: 0.8605\n",
      "Epoch 277/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9444\n",
      "Epoch 277: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 764s 509ms/step - loss: 0.2352 - accuracy: 0.9444 - val_loss: 0.6814 - val_accuracy: 0.8622\n",
      "Epoch 278/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9442\n",
      "Epoch 278: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 766s 511ms/step - loss: 0.2315 - accuracy: 0.9442 - val_loss: 0.6063 - val_accuracy: 0.8670\n",
      "Epoch 279/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9460\n",
      "Epoch 279: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 475s 316ms/step - loss: 0.2325 - accuracy: 0.9460 - val_loss: 0.7050 - val_accuracy: 0.8555\n",
      "Epoch 280/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9497\n",
      "Epoch 280: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2248 - accuracy: 0.9497 - val_loss: 0.6594 - val_accuracy: 0.8568\n",
      "Epoch 281/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9445\n",
      "Epoch 281: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2353 - accuracy: 0.9445 - val_loss: 0.6153 - val_accuracy: 0.8577\n",
      "Epoch 282/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9488\n",
      "Epoch 282: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2301 - accuracy: 0.9488 - val_loss: 0.5963 - val_accuracy: 0.8630\n",
      "Epoch 283/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9476\n",
      "Epoch 283: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2242 - accuracy: 0.9476 - val_loss: 0.6223 - val_accuracy: 0.8662\n",
      "Epoch 284/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9479\n",
      "Epoch 284: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2287 - accuracy: 0.9479 - val_loss: 0.6908 - val_accuracy: 0.8530\n",
      "Epoch 285/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9498\n",
      "Epoch 285: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2207 - accuracy: 0.9498 - val_loss: 0.7239 - val_accuracy: 0.8313\n",
      "Epoch 286/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9478\n",
      "Epoch 286: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2315 - accuracy: 0.9478 - val_loss: 0.5781 - val_accuracy: 0.8640\n",
      "Epoch 287/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9499\n",
      "Epoch 287: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2252 - accuracy: 0.9499 - val_loss: 0.6050 - val_accuracy: 0.8583\n",
      "Epoch 288/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9463\n",
      "Epoch 288: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2291 - accuracy: 0.9463 - val_loss: 0.6327 - val_accuracy: 0.8612\n",
      "Epoch 289/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9474\n",
      "Epoch 289: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2318 - accuracy: 0.9474 - val_loss: 0.6607 - val_accuracy: 0.8615\n",
      "Epoch 290/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9465\n",
      "Epoch 290: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2305 - accuracy: 0.9465 - val_loss: 0.7476 - val_accuracy: 0.8382\n",
      "Epoch 291/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9483\n",
      "Epoch 291: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2278 - accuracy: 0.9483 - val_loss: 0.5564 - val_accuracy: 0.8508\n",
      "Epoch 292/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9440\n",
      "Epoch 292: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2308 - accuracy: 0.9440 - val_loss: 0.5942 - val_accuracy: 0.8652\n",
      "Epoch 293/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9472\n",
      "Epoch 293: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2289 - accuracy: 0.9472 - val_loss: 0.6966 - val_accuracy: 0.8462\n",
      "Epoch 294/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9463\n",
      "Epoch 294: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2313 - accuracy: 0.9463 - val_loss: 0.6454 - val_accuracy: 0.8640\n",
      "Epoch 295/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9487\n",
      "Epoch 295: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2268 - accuracy: 0.9487 - val_loss: 0.6411 - val_accuracy: 0.8680\n",
      "Epoch 296/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9461\n",
      "Epoch 296: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2313 - accuracy: 0.9461 - val_loss: 0.6701 - val_accuracy: 0.8645\n",
      "Epoch 297/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9489\n",
      "Epoch 297: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2244 - accuracy: 0.9489 - val_loss: 0.5881 - val_accuracy: 0.8395\n",
      "Epoch 298/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9478\n",
      "Epoch 298: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2260 - accuracy: 0.9478 - val_loss: 0.5785 - val_accuracy: 0.8635\n",
      "Epoch 299/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9498\n",
      "Epoch 299: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2251 - accuracy: 0.9498 - val_loss: 0.7466 - val_accuracy: 0.8275\n",
      "Epoch 300/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9492\n",
      "Epoch 300: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2236 - accuracy: 0.9492 - val_loss: 0.5978 - val_accuracy: 0.8618\n",
      "Epoch 301/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9466\n",
      "Epoch 301: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2300 - accuracy: 0.9466 - val_loss: 0.7446 - val_accuracy: 0.8475\n",
      "Epoch 302/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9503\n",
      "Epoch 302: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2260 - accuracy: 0.9503 - val_loss: 0.7470 - val_accuracy: 0.8325\n",
      "Epoch 303/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9495\n",
      "Epoch 303: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2207 - accuracy: 0.9495 - val_loss: 0.5932 - val_accuracy: 0.8643\n",
      "Epoch 304/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9499\n",
      "Epoch 304: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2254 - accuracy: 0.9499 - val_loss: 0.5709 - val_accuracy: 0.8590\n",
      "Epoch 305/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9487\n",
      "Epoch 305: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2236 - accuracy: 0.9487 - val_loss: 0.6491 - val_accuracy: 0.8597\n",
      "Epoch 306/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9525\n",
      "Epoch 306: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2205 - accuracy: 0.9525 - val_loss: 0.6897 - val_accuracy: 0.8590\n",
      "Epoch 307/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9475\n",
      "Epoch 307: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2334 - accuracy: 0.9475 - val_loss: 0.6425 - val_accuracy: 0.8543\n",
      "Epoch 308/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9519\n",
      "Epoch 308: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2240 - accuracy: 0.9519 - val_loss: 0.7446 - val_accuracy: 0.8445\n",
      "Epoch 309/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9517\n",
      "Epoch 309: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2235 - accuracy: 0.9517 - val_loss: 0.5834 - val_accuracy: 0.8595\n",
      "Epoch 310/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9483\n",
      "Epoch 310: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2302 - accuracy: 0.9483 - val_loss: 0.5817 - val_accuracy: 0.8637\n",
      "Epoch 311/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9542\n",
      "Epoch 311: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2190 - accuracy: 0.9542 - val_loss: 0.7788 - val_accuracy: 0.8518\n",
      "Epoch 312/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9506\n",
      "Epoch 312: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2235 - accuracy: 0.9506 - val_loss: 0.5687 - val_accuracy: 0.8560\n",
      "Epoch 313/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9523\n",
      "Epoch 313: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2226 - accuracy: 0.9523 - val_loss: 0.7361 - val_accuracy: 0.8350\n",
      "Epoch 314/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9517\n",
      "Epoch 314: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2228 - accuracy: 0.9517 - val_loss: 0.6135 - val_accuracy: 0.8572\n",
      "Epoch 315/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9460\n",
      "Epoch 315: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2288 - accuracy: 0.9460 - val_loss: 0.6539 - val_accuracy: 0.8553\n",
      "Epoch 316/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9524\n",
      "Epoch 316: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2205 - accuracy: 0.9524 - val_loss: 0.7876 - val_accuracy: 0.8213\n",
      "Epoch 317/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9519\n",
      "Epoch 317: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2230 - accuracy: 0.9519 - val_loss: 0.6560 - val_accuracy: 0.8692\n",
      "Epoch 318/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9521\n",
      "Epoch 318: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2226 - accuracy: 0.9521 - val_loss: 0.6279 - val_accuracy: 0.8612\n",
      "Epoch 319/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9528\n",
      "Epoch 319: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2189 - accuracy: 0.9528 - val_loss: 0.6340 - val_accuracy: 0.8608\n",
      "Epoch 320/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9552\n",
      "Epoch 320: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.2155 - accuracy: 0.9552 - val_loss: 0.6556 - val_accuracy: 0.8635\n",
      "Epoch 321/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9569\n",
      "Epoch 321: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2156 - accuracy: 0.9569 - val_loss: 0.6511 - val_accuracy: 0.8565\n",
      "Epoch 322/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9554\n",
      "Epoch 322: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 278ms/step - loss: 0.2169 - accuracy: 0.9554 - val_loss: 0.6862 - val_accuracy: 0.8485\n",
      "Epoch 323/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9562\n",
      "Epoch 323: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2147 - accuracy: 0.9562 - val_loss: 0.6550 - val_accuracy: 0.8570\n",
      "Epoch 324/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9538\n",
      "Epoch 324: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2215 - accuracy: 0.9538 - val_loss: 0.6195 - val_accuracy: 0.8585\n",
      "Epoch 325/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9499\n",
      "Epoch 325: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2244 - accuracy: 0.9499 - val_loss: 0.6150 - val_accuracy: 0.8675\n",
      "Epoch 326/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9582\n",
      "Epoch 326: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2070 - accuracy: 0.9582 - val_loss: 0.6141 - val_accuracy: 0.8565\n",
      "Epoch 327/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9551\n",
      "Epoch 327: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2184 - accuracy: 0.9551 - val_loss: 0.7015 - val_accuracy: 0.8540\n",
      "Epoch 328/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9523\n",
      "Epoch 328: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.2241 - accuracy: 0.9523 - val_loss: 0.6424 - val_accuracy: 0.8522\n",
      "Epoch 329/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9553\n",
      "Epoch 329: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2176 - accuracy: 0.9553 - val_loss: 0.6905 - val_accuracy: 0.8508\n",
      "Epoch 330/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9500\n",
      "Epoch 330: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2304 - accuracy: 0.9500 - val_loss: 0.6293 - val_accuracy: 0.8528\n",
      "Epoch 331/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9556\n",
      "Epoch 331: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2165 - accuracy: 0.9556 - val_loss: 0.6780 - val_accuracy: 0.8472\n",
      "Epoch 332/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9542\n",
      "Epoch 332: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2197 - accuracy: 0.9542 - val_loss: 0.6535 - val_accuracy: 0.8510\n",
      "Epoch 333/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9532\n",
      "Epoch 333: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2249 - accuracy: 0.9532 - val_loss: 0.6284 - val_accuracy: 0.8633\n",
      "Epoch 334/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9554\n",
      "Epoch 334: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2156 - accuracy: 0.9554 - val_loss: 0.6597 - val_accuracy: 0.8570\n",
      "Epoch 335/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9594\n",
      "Epoch 335: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2046 - accuracy: 0.9594 - val_loss: 0.7393 - val_accuracy: 0.8608\n",
      "Epoch 336/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9543\n",
      "Epoch 336: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2181 - accuracy: 0.9543 - val_loss: 0.6529 - val_accuracy: 0.8605\n",
      "Epoch 337/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9540\n",
      "Epoch 337: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2135 - accuracy: 0.9540 - val_loss: 0.6651 - val_accuracy: 0.8543\n",
      "Epoch 338/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9593\n",
      "Epoch 338: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2092 - accuracy: 0.9593 - val_loss: 0.9330 - val_accuracy: 0.8198\n",
      "Epoch 339/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9571\n",
      "Epoch 339: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2106 - accuracy: 0.9571 - val_loss: 0.6834 - val_accuracy: 0.8670\n",
      "Epoch 340/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9581\n",
      "Epoch 340: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2098 - accuracy: 0.9581 - val_loss: 0.5584 - val_accuracy: 0.8545\n",
      "Epoch 341/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9586\n",
      "Epoch 341: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2099 - accuracy: 0.9586 - val_loss: 0.6978 - val_accuracy: 0.8612\n",
      "Epoch 342/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9607\n",
      "Epoch 342: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2016 - accuracy: 0.9607 - val_loss: 0.8146 - val_accuracy: 0.8420\n",
      "Epoch 343/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9564\n",
      "Epoch 343: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2133 - accuracy: 0.9564 - val_loss: 0.5890 - val_accuracy: 0.8705\n",
      "Epoch 344/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9598\n",
      "Epoch 344: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2117 - accuracy: 0.9598 - val_loss: 0.7101 - val_accuracy: 0.8518\n",
      "Epoch 345/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9609\n",
      "Epoch 345: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2074 - accuracy: 0.9609 - val_loss: 0.7003 - val_accuracy: 0.8430\n",
      "Epoch 346/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9606\n",
      "Epoch 346: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2037 - accuracy: 0.9606 - val_loss: 0.6913 - val_accuracy: 0.8435\n",
      "Epoch 347/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9613\n",
      "Epoch 347: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2059 - accuracy: 0.9613 - val_loss: 0.6385 - val_accuracy: 0.8608\n",
      "Epoch 348/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9635\n",
      "Epoch 348: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1988 - accuracy: 0.9635 - val_loss: 0.7071 - val_accuracy: 0.8577\n",
      "Epoch 349/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9611\n",
      "Epoch 349: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2079 - accuracy: 0.9611 - val_loss: 0.7630 - val_accuracy: 0.8545\n",
      "Epoch 350/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9607\n",
      "Epoch 350: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2086 - accuracy: 0.9607 - val_loss: 0.6665 - val_accuracy: 0.8627\n",
      "Epoch 351/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9600\n",
      "Epoch 351: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2085 - accuracy: 0.9600 - val_loss: 0.6834 - val_accuracy: 0.8465\n",
      "Epoch 352/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9605\n",
      "Epoch 352: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2044 - accuracy: 0.9605 - val_loss: 0.6209 - val_accuracy: 0.8560\n",
      "Epoch 353/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9548\n",
      "Epoch 353: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2218 - accuracy: 0.9548 - val_loss: 0.6682 - val_accuracy: 0.8595\n",
      "Epoch 354/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9624\n",
      "Epoch 354: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2001 - accuracy: 0.9624 - val_loss: 0.6548 - val_accuracy: 0.8615\n",
      "Epoch 355/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9559\n",
      "Epoch 355: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2198 - accuracy: 0.9559 - val_loss: 0.6273 - val_accuracy: 0.8618\n",
      "Epoch 356/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9627\n",
      "Epoch 356: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2033 - accuracy: 0.9627 - val_loss: 0.7658 - val_accuracy: 0.8533\n",
      "Epoch 357/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9583\n",
      "Epoch 357: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2123 - accuracy: 0.9583 - val_loss: 0.6463 - val_accuracy: 0.8597\n",
      "Epoch 358/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9624\n",
      "Epoch 358: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2065 - accuracy: 0.9624 - val_loss: 0.6027 - val_accuracy: 0.8615\n",
      "Epoch 359/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9623\n",
      "Epoch 359: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2000 - accuracy: 0.9623 - val_loss: 0.7061 - val_accuracy: 0.8630\n",
      "Epoch 360/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9617\n",
      "Epoch 360: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2050 - accuracy: 0.9617 - val_loss: 0.7112 - val_accuracy: 0.8660\n",
      "Epoch 361/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9661\n",
      "Epoch 361: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1995 - accuracy: 0.9661 - val_loss: 0.6883 - val_accuracy: 0.8470\n",
      "Epoch 362/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9632\n",
      "Epoch 362: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2053 - accuracy: 0.9632 - val_loss: 0.6321 - val_accuracy: 0.8608\n",
      "Epoch 363/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9581\n",
      "Epoch 363: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2086 - accuracy: 0.9581 - val_loss: 0.6516 - val_accuracy: 0.8530\n",
      "Epoch 364/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9641\n",
      "Epoch 364: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1985 - accuracy: 0.9641 - val_loss: 0.8099 - val_accuracy: 0.8485\n",
      "Epoch 365/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9583\n",
      "Epoch 365: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.2134 - accuracy: 0.9583 - val_loss: 0.7601 - val_accuracy: 0.8405\n",
      "Epoch 366/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9647\n",
      "Epoch 366: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2031 - accuracy: 0.9647 - val_loss: 0.6578 - val_accuracy: 0.8622\n",
      "Epoch 367/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9605\n",
      "Epoch 367: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2051 - accuracy: 0.9605 - val_loss: 0.7548 - val_accuracy: 0.8568\n",
      "Epoch 368/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9625\n",
      "Epoch 368: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2071 - accuracy: 0.9625 - val_loss: 0.7283 - val_accuracy: 0.8545\n",
      "Epoch 369/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9638\n",
      "Epoch 369: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1993 - accuracy: 0.9638 - val_loss: 0.6977 - val_accuracy: 0.8395\n",
      "Epoch 370/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9637\n",
      "Epoch 370: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2039 - accuracy: 0.9637 - val_loss: 0.6594 - val_accuracy: 0.8618\n",
      "Epoch 371/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9620\n",
      "Epoch 371: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2065 - accuracy: 0.9620 - val_loss: 0.6435 - val_accuracy: 0.8580\n",
      "Epoch 372/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9671\n",
      "Epoch 372: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1961 - accuracy: 0.9671 - val_loss: 0.7080 - val_accuracy: 0.8508\n",
      "Epoch 373/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9644\n",
      "Epoch 373: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2022 - accuracy: 0.9644 - val_loss: 0.6164 - val_accuracy: 0.8650\n",
      "Epoch 374/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9677\n",
      "Epoch 374: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1962 - accuracy: 0.9677 - val_loss: 0.8550 - val_accuracy: 0.8495\n",
      "Epoch 375/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9637\n",
      "Epoch 375: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2032 - accuracy: 0.9637 - val_loss: 0.7570 - val_accuracy: 0.8395\n",
      "Epoch 376/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9679\n",
      "Epoch 376: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1973 - accuracy: 0.9679 - val_loss: 0.7066 - val_accuracy: 0.8612\n",
      "Epoch 377/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9675\n",
      "Epoch 377: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1987 - accuracy: 0.9675 - val_loss: 0.7355 - val_accuracy: 0.8432\n",
      "Epoch 378/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9638\n",
      "Epoch 378: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2048 - accuracy: 0.9638 - val_loss: 0.6593 - val_accuracy: 0.8710\n",
      "Epoch 379/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9663\n",
      "Epoch 379: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1943 - accuracy: 0.9663 - val_loss: 0.7567 - val_accuracy: 0.8435\n",
      "Epoch 380/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9643\n",
      "Epoch 380: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2007 - accuracy: 0.9643 - val_loss: 0.6572 - val_accuracy: 0.8577\n",
      "Epoch 381/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9679\n",
      "Epoch 381: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1943 - accuracy: 0.9679 - val_loss: 0.7304 - val_accuracy: 0.8510\n",
      "Epoch 382/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9607\n",
      "Epoch 382: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2113 - accuracy: 0.9607 - val_loss: 0.6726 - val_accuracy: 0.8593\n",
      "Epoch 383/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9642\n",
      "Epoch 383: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2043 - accuracy: 0.9642 - val_loss: 0.6979 - val_accuracy: 0.8415\n",
      "Epoch 384/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9639\n",
      "Epoch 384: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2035 - accuracy: 0.9639 - val_loss: 0.7544 - val_accuracy: 0.8443\n",
      "Epoch 385/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9649\n",
      "Epoch 385: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2035 - accuracy: 0.9649 - val_loss: 0.6590 - val_accuracy: 0.8640\n",
      "Epoch 386/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9629\n",
      "Epoch 386: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2064 - accuracy: 0.9629 - val_loss: 0.7110 - val_accuracy: 0.8558\n",
      "Epoch 387/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9677\n",
      "Epoch 387: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1970 - accuracy: 0.9677 - val_loss: 0.6913 - val_accuracy: 0.8612\n",
      "Epoch 388/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9648\n",
      "Epoch 388: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2046 - accuracy: 0.9648 - val_loss: 0.6834 - val_accuracy: 0.8583\n",
      "Epoch 389/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9665\n",
      "Epoch 389: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1983 - accuracy: 0.9665 - val_loss: 0.6353 - val_accuracy: 0.8460\n",
      "Epoch 390/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9635\n",
      "Epoch 390: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2041 - accuracy: 0.9635 - val_loss: 0.7509 - val_accuracy: 0.8415\n",
      "Epoch 391/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9613\n",
      "Epoch 391: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.2102 - accuracy: 0.9613 - val_loss: 0.6635 - val_accuracy: 0.8505\n",
      "Epoch 392/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9682\n",
      "Epoch 392: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1970 - accuracy: 0.9682 - val_loss: 0.7228 - val_accuracy: 0.8568\n",
      "Epoch 393/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9658\n",
      "Epoch 393: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2036 - accuracy: 0.9658 - val_loss: 0.8013 - val_accuracy: 0.8407\n",
      "Epoch 394/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9664\n",
      "Epoch 394: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1996 - accuracy: 0.9664 - val_loss: 0.6770 - val_accuracy: 0.8577\n",
      "Epoch 395/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9684\n",
      "Epoch 395: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1926 - accuracy: 0.9684 - val_loss: 0.8773 - val_accuracy: 0.8288\n",
      "Epoch 396/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9651\n",
      "Epoch 396: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2055 - accuracy: 0.9651 - val_loss: 0.6900 - val_accuracy: 0.8533\n",
      "Epoch 397/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9682\n",
      "Epoch 397: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1986 - accuracy: 0.9682 - val_loss: 0.7551 - val_accuracy: 0.8515\n",
      "Epoch 398/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9673\n",
      "Epoch 398: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1990 - accuracy: 0.9673 - val_loss: 0.6737 - val_accuracy: 0.8585\n",
      "Epoch 399/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9688\n",
      "Epoch 399: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1965 - accuracy: 0.9688 - val_loss: 0.6944 - val_accuracy: 0.8545\n",
      "Epoch 400/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9722\n",
      "Epoch 400: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1894 - accuracy: 0.9722 - val_loss: 0.6979 - val_accuracy: 0.8562\n",
      "Epoch 401/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9694\n",
      "Epoch 401: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1972 - accuracy: 0.9694 - val_loss: 0.7740 - val_accuracy: 0.8525\n",
      "Epoch 402/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9674\n",
      "Epoch 402: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.2005 - accuracy: 0.9674 - val_loss: 0.6211 - val_accuracy: 0.8555\n",
      "Epoch 403/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9710\n",
      "Epoch 403: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1877 - accuracy: 0.9710 - val_loss: 0.7948 - val_accuracy: 0.8610\n",
      "Epoch 404/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9733\n",
      "Epoch 404: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1862 - accuracy: 0.9733 - val_loss: 0.8176 - val_accuracy: 0.8443\n",
      "Epoch 405/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9675\n",
      "Epoch 405: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.2000 - accuracy: 0.9675 - val_loss: 0.7016 - val_accuracy: 0.8568\n",
      "Epoch 406/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9708\n",
      "Epoch 406: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1926 - accuracy: 0.9708 - val_loss: 0.6267 - val_accuracy: 0.8608\n",
      "Epoch 407/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9681\n",
      "Epoch 407: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1983 - accuracy: 0.9681 - val_loss: 0.7468 - val_accuracy: 0.8457\n",
      "Epoch 408/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9704\n",
      "Epoch 408: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1929 - accuracy: 0.9704 - val_loss: 0.6864 - val_accuracy: 0.8575\n",
      "Epoch 409/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9735\n",
      "Epoch 409: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1835 - accuracy: 0.9735 - val_loss: 0.7609 - val_accuracy: 0.8503\n",
      "Epoch 410/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9713\n",
      "Epoch 410: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1926 - accuracy: 0.9713 - val_loss: 0.7085 - val_accuracy: 0.8593\n",
      "Epoch 411/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9709\n",
      "Epoch 411: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1891 - accuracy: 0.9709 - val_loss: 0.9292 - val_accuracy: 0.8142\n",
      "Epoch 412/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9699\n",
      "Epoch 412: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 429s 286ms/step - loss: 0.1930 - accuracy: 0.9699 - val_loss: 0.7233 - val_accuracy: 0.8550\n",
      "Epoch 413/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9707\n",
      "Epoch 413: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1912 - accuracy: 0.9707 - val_loss: 0.6810 - val_accuracy: 0.8677\n",
      "Epoch 414/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9730\n",
      "Epoch 414: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1898 - accuracy: 0.9730 - val_loss: 0.8072 - val_accuracy: 0.8472\n",
      "Epoch 415/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9703\n",
      "Epoch 415: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1927 - accuracy: 0.9703 - val_loss: 0.7218 - val_accuracy: 0.8593\n",
      "Epoch 416/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9707\n",
      "Epoch 416: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1933 - accuracy: 0.9707 - val_loss: 0.8059 - val_accuracy: 0.8447\n",
      "Epoch 417/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9747\n",
      "Epoch 417: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1807 - accuracy: 0.9747 - val_loss: 0.6920 - val_accuracy: 0.8528\n",
      "Epoch 418/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9738\n",
      "Epoch 418: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1887 - accuracy: 0.9738 - val_loss: 0.6773 - val_accuracy: 0.8625\n",
      "Epoch 419/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9704\n",
      "Epoch 419: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1940 - accuracy: 0.9704 - val_loss: 0.6958 - val_accuracy: 0.8593\n",
      "Epoch 420/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9758\n",
      "Epoch 420: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1836 - accuracy: 0.9758 - val_loss: 0.8324 - val_accuracy: 0.8322\n",
      "Epoch 421/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9704\n",
      "Epoch 421: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1901 - accuracy: 0.9704 - val_loss: 0.7821 - val_accuracy: 0.8543\n",
      "Epoch 422/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9718\n",
      "Epoch 422: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1922 - accuracy: 0.9718 - val_loss: 0.7436 - val_accuracy: 0.8508\n",
      "Epoch 423/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9712\n",
      "Epoch 423: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1918 - accuracy: 0.9712 - val_loss: 0.7627 - val_accuracy: 0.8495\n",
      "Epoch 424/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.9702\n",
      "Epoch 424: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1907 - accuracy: 0.9702 - val_loss: 0.8213 - val_accuracy: 0.8485\n",
      "Epoch 425/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9713\n",
      "Epoch 425: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1924 - accuracy: 0.9713 - val_loss: 0.7240 - val_accuracy: 0.8400\n",
      "Epoch 426/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9754\n",
      "Epoch 426: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1820 - accuracy: 0.9754 - val_loss: 0.7149 - val_accuracy: 0.8547\n",
      "Epoch 427/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9762\n",
      "Epoch 427: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1823 - accuracy: 0.9762 - val_loss: 0.8195 - val_accuracy: 0.8535\n",
      "Epoch 428/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9738\n",
      "Epoch 428: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1874 - accuracy: 0.9738 - val_loss: 0.7562 - val_accuracy: 0.8558\n",
      "Epoch 429/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9743\n",
      "Epoch 429: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1874 - accuracy: 0.9743 - val_loss: 0.7695 - val_accuracy: 0.8570\n",
      "Epoch 430/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9727\n",
      "Epoch 430: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1877 - accuracy: 0.9727 - val_loss: 0.6922 - val_accuracy: 0.8662\n",
      "Epoch 431/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9723\n",
      "Epoch 431: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1889 - accuracy: 0.9723 - val_loss: 0.6243 - val_accuracy: 0.8695\n",
      "Epoch 432/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9735\n",
      "Epoch 432: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1894 - accuracy: 0.9735 - val_loss: 0.6620 - val_accuracy: 0.8612\n",
      "Epoch 433/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9770\n",
      "Epoch 433: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1807 - accuracy: 0.9770 - val_loss: 0.7555 - val_accuracy: 0.8545\n",
      "Epoch 434/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9735\n",
      "Epoch 434: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1870 - accuracy: 0.9735 - val_loss: 0.7842 - val_accuracy: 0.8480\n",
      "Epoch 435/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9778\n",
      "Epoch 435: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1834 - accuracy: 0.9778 - val_loss: 0.7673 - val_accuracy: 0.8385\n",
      "Epoch 436/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9734\n",
      "Epoch 436: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1877 - accuracy: 0.9734 - val_loss: 0.6987 - val_accuracy: 0.8380\n",
      "Epoch 437/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9742\n",
      "Epoch 437: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1856 - accuracy: 0.9742 - val_loss: 0.7242 - val_accuracy: 0.8533\n",
      "Epoch 438/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9735\n",
      "Epoch 438: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1866 - accuracy: 0.9735 - val_loss: 0.6961 - val_accuracy: 0.8610\n",
      "Epoch 439/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9778\n",
      "Epoch 439: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1761 - accuracy: 0.9778 - val_loss: 0.8130 - val_accuracy: 0.8583\n",
      "Epoch 440/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9729\n",
      "Epoch 440: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1906 - accuracy: 0.9729 - val_loss: 0.6247 - val_accuracy: 0.8590\n",
      "Epoch 441/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9715\n",
      "Epoch 441: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1923 - accuracy: 0.9715 - val_loss: 0.6649 - val_accuracy: 0.8540\n",
      "Epoch 442/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9762\n",
      "Epoch 442: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1777 - accuracy: 0.9762 - val_loss: 0.7370 - val_accuracy: 0.8490\n",
      "Epoch 443/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9707\n",
      "Epoch 443: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1959 - accuracy: 0.9707 - val_loss: 0.6570 - val_accuracy: 0.8568\n",
      "Epoch 444/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9718\n",
      "Epoch 444: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1903 - accuracy: 0.9718 - val_loss: 0.7209 - val_accuracy: 0.8622\n",
      "Epoch 445/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9739\n",
      "Epoch 445: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1864 - accuracy: 0.9739 - val_loss: 0.6791 - val_accuracy: 0.8535\n",
      "Epoch 446/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9712\n",
      "Epoch 446: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1885 - accuracy: 0.9712 - val_loss: 0.6040 - val_accuracy: 0.8565\n",
      "Epoch 447/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9745\n",
      "Epoch 447: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1844 - accuracy: 0.9745 - val_loss: 0.6710 - val_accuracy: 0.8612\n",
      "Epoch 448/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9753\n",
      "Epoch 448: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1808 - accuracy: 0.9753 - val_loss: 0.7035 - val_accuracy: 0.8625\n",
      "Epoch 449/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9761\n",
      "Epoch 449: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1856 - accuracy: 0.9761 - val_loss: 0.6927 - val_accuracy: 0.8620\n",
      "Epoch 450/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9759\n",
      "Epoch 450: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1849 - accuracy: 0.9759 - val_loss: 0.6611 - val_accuracy: 0.8637\n",
      "Epoch 451/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9784\n",
      "Epoch 451: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1760 - accuracy: 0.9784 - val_loss: 0.7806 - val_accuracy: 0.8558\n",
      "Epoch 452/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9760\n",
      "Epoch 452: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1819 - accuracy: 0.9760 - val_loss: 0.6905 - val_accuracy: 0.8472\n",
      "Epoch 453/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9755\n",
      "Epoch 453: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1862 - accuracy: 0.9755 - val_loss: 0.7122 - val_accuracy: 0.8495\n",
      "Epoch 454/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9758\n",
      "Epoch 454: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1843 - accuracy: 0.9758 - val_loss: 0.7548 - val_accuracy: 0.8537\n",
      "Epoch 455/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9753\n",
      "Epoch 455: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1864 - accuracy: 0.9753 - val_loss: 0.7276 - val_accuracy: 0.8443\n",
      "Epoch 456/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9772\n",
      "Epoch 456: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1782 - accuracy: 0.9772 - val_loss: 0.7605 - val_accuracy: 0.8528\n",
      "Epoch 457/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9787\n",
      "Epoch 457: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1740 - accuracy: 0.9787 - val_loss: 0.7633 - val_accuracy: 0.8438\n",
      "Epoch 458/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9744\n",
      "Epoch 458: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1905 - accuracy: 0.9744 - val_loss: 0.6670 - val_accuracy: 0.8555\n",
      "Epoch 459/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9762\n",
      "Epoch 459: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1766 - accuracy: 0.9762 - val_loss: 0.6757 - val_accuracy: 0.8560\n",
      "Epoch 460/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9740\n",
      "Epoch 460: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1897 - accuracy: 0.9740 - val_loss: 0.6718 - val_accuracy: 0.8575\n",
      "Epoch 461/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9774\n",
      "Epoch 461: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 280ms/step - loss: 0.1822 - accuracy: 0.9774 - val_loss: 0.6352 - val_accuracy: 0.8570\n",
      "Epoch 462/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9739\n",
      "Epoch 462: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1870 - accuracy: 0.9739 - val_loss: 0.6473 - val_accuracy: 0.8590\n",
      "Epoch 463/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9739\n",
      "Epoch 463: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 272ms/step - loss: 0.1865 - accuracy: 0.9739 - val_loss: 0.6107 - val_accuracy: 0.8587\n",
      "Epoch 464/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9789\n",
      "Epoch 464: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1771 - accuracy: 0.9789 - val_loss: 0.6892 - val_accuracy: 0.8453\n",
      "Epoch 465/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9747\n",
      "Epoch 465: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1837 - accuracy: 0.9747 - val_loss: 0.7545 - val_accuracy: 0.8530\n",
      "Epoch 466/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9758\n",
      "Epoch 466: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1827 - accuracy: 0.9758 - val_loss: 0.6576 - val_accuracy: 0.8633\n",
      "Epoch 467/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9772\n",
      "Epoch 467: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1820 - accuracy: 0.9772 - val_loss: 0.7039 - val_accuracy: 0.8662\n",
      "Epoch 468/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9787\n",
      "Epoch 468: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 409s 273ms/step - loss: 0.1753 - accuracy: 0.9787 - val_loss: 0.7173 - val_accuracy: 0.8530\n",
      "Epoch 469/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9799\n",
      "Epoch 469: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1786 - accuracy: 0.9799 - val_loss: 0.7225 - val_accuracy: 0.8590\n",
      "Epoch 470/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9803\n",
      "Epoch 470: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1712 - accuracy: 0.9803 - val_loss: 0.7782 - val_accuracy: 0.8490\n",
      "Epoch 471/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9811\n",
      "Epoch 471: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1716 - accuracy: 0.9811 - val_loss: 0.7389 - val_accuracy: 0.8528\n",
      "Epoch 472/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9770\n",
      "Epoch 472: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1789 - accuracy: 0.9770 - val_loss: 0.7211 - val_accuracy: 0.8618\n",
      "Epoch 473/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9774\n",
      "Epoch 473: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1785 - accuracy: 0.9774 - val_loss: 0.7503 - val_accuracy: 0.8610\n",
      "Epoch 474/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9802\n",
      "Epoch 474: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1740 - accuracy: 0.9802 - val_loss: 0.9829 - val_accuracy: 0.8100\n",
      "Epoch 475/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9768\n",
      "Epoch 475: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1814 - accuracy: 0.9768 - val_loss: 0.7151 - val_accuracy: 0.8490\n",
      "Epoch 476/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9797\n",
      "Epoch 476: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1734 - accuracy: 0.9797 - val_loss: 0.7753 - val_accuracy: 0.8460\n",
      "Epoch 477/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9768\n",
      "Epoch 477: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1808 - accuracy: 0.9768 - val_loss: 0.7006 - val_accuracy: 0.8490\n",
      "Epoch 478/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9810\n",
      "Epoch 478: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1729 - accuracy: 0.9810 - val_loss: 0.7504 - val_accuracy: 0.8560\n",
      "Epoch 479/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9812\n",
      "Epoch 479: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1717 - accuracy: 0.9812 - val_loss: 0.8326 - val_accuracy: 0.8450\n",
      "Epoch 480/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9788\n",
      "Epoch 480: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1770 - accuracy: 0.9788 - val_loss: 0.7274 - val_accuracy: 0.8300\n",
      "Epoch 481/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9780\n",
      "Epoch 481: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1754 - accuracy: 0.9780 - val_loss: 1.1286 - val_accuracy: 0.7793\n",
      "Epoch 482/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9824\n",
      "Epoch 482: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1682 - accuracy: 0.9824 - val_loss: 0.7250 - val_accuracy: 0.8568\n",
      "Epoch 483/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9835\n",
      "Epoch 483: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1664 - accuracy: 0.9835 - val_loss: 0.6903 - val_accuracy: 0.8512\n",
      "Epoch 484/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9827\n",
      "Epoch 484: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1686 - accuracy: 0.9827 - val_loss: 0.7484 - val_accuracy: 0.8465\n",
      "Epoch 485/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9778\n",
      "Epoch 485: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1768 - accuracy: 0.9778 - val_loss: 0.7484 - val_accuracy: 0.8577\n",
      "Epoch 486/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9784\n",
      "Epoch 486: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1754 - accuracy: 0.9784 - val_loss: 0.7431 - val_accuracy: 0.8425\n",
      "Epoch 487/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9791\n",
      "Epoch 487: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1808 - accuracy: 0.9791 - val_loss: 0.6413 - val_accuracy: 0.8457\n",
      "Epoch 488/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9809\n",
      "Epoch 488: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1718 - accuracy: 0.9809 - val_loss: 0.7293 - val_accuracy: 0.8640\n",
      "Epoch 489/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9812\n",
      "Epoch 489: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1731 - accuracy: 0.9812 - val_loss: 0.7044 - val_accuracy: 0.8610\n",
      "Epoch 490/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9793\n",
      "Epoch 490: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1734 - accuracy: 0.9793 - val_loss: 0.6976 - val_accuracy: 0.8618\n",
      "Epoch 491/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9811\n",
      "Epoch 491: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1727 - accuracy: 0.9811 - val_loss: 0.7102 - val_accuracy: 0.8580\n",
      "Epoch 492/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9827\n",
      "Epoch 492: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1673 - accuracy: 0.9827 - val_loss: 0.8686 - val_accuracy: 0.8465\n",
      "Epoch 493/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9768\n",
      "Epoch 493: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 413s 275ms/step - loss: 0.1793 - accuracy: 0.9768 - val_loss: 0.6923 - val_accuracy: 0.8547\n",
      "Epoch 494/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9783\n",
      "Epoch 494: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1798 - accuracy: 0.9783 - val_loss: 0.6460 - val_accuracy: 0.8575\n",
      "Epoch 495/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9828\n",
      "Epoch 495: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1679 - accuracy: 0.9828 - val_loss: 0.7282 - val_accuracy: 0.8508\n",
      "Epoch 496/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9838\n",
      "Epoch 496: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1644 - accuracy: 0.9838 - val_loss: 0.7651 - val_accuracy: 0.8520\n",
      "Epoch 497/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9811\n",
      "Epoch 497: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1717 - accuracy: 0.9811 - val_loss: 0.7070 - val_accuracy: 0.8515\n",
      "Epoch 498/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9747\n",
      "Epoch 498: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1868 - accuracy: 0.9747 - val_loss: 0.7503 - val_accuracy: 0.8357\n",
      "Epoch 499/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9818\n",
      "Epoch 499: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1700 - accuracy: 0.9818 - val_loss: 0.7142 - val_accuracy: 0.8478\n",
      "Epoch 500/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9858\n",
      "Epoch 500: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1597 - accuracy: 0.9858 - val_loss: 0.7094 - val_accuracy: 0.8485\n",
      "Epoch 501/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9826\n",
      "Epoch 501: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1675 - accuracy: 0.9826 - val_loss: 0.7016 - val_accuracy: 0.8465\n",
      "Epoch 502/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9808\n",
      "Epoch 502: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1704 - accuracy: 0.9808 - val_loss: 0.6868 - val_accuracy: 0.8475\n",
      "Epoch 503/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9826\n",
      "Epoch 503: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1683 - accuracy: 0.9826 - val_loss: 0.7138 - val_accuracy: 0.8637\n",
      "Epoch 504/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9790\n",
      "Epoch 504: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1746 - accuracy: 0.9790 - val_loss: 0.7191 - val_accuracy: 0.8460\n",
      "Epoch 505/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9818\n",
      "Epoch 505: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1656 - accuracy: 0.9818 - val_loss: 0.8013 - val_accuracy: 0.8562\n",
      "Epoch 506/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9769\n",
      "Epoch 506: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1816 - accuracy: 0.9769 - val_loss: 0.6559 - val_accuracy: 0.8575\n",
      "Epoch 507/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9768\n",
      "Epoch 507: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1813 - accuracy: 0.9768 - val_loss: 0.7657 - val_accuracy: 0.8580\n",
      "Epoch 508/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9814\n",
      "Epoch 508: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1681 - accuracy: 0.9814 - val_loss: 0.6756 - val_accuracy: 0.8500\n",
      "Epoch 509/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9792\n",
      "Epoch 509: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1719 - accuracy: 0.9792 - val_loss: 0.7482 - val_accuracy: 0.8465\n",
      "Epoch 510/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1829 - accuracy: 0.9769\n",
      "Epoch 510: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1829 - accuracy: 0.9769 - val_loss: 0.7012 - val_accuracy: 0.8595\n",
      "Epoch 511/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9823\n",
      "Epoch 511: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1651 - accuracy: 0.9823 - val_loss: 0.7020 - val_accuracy: 0.8535\n",
      "Epoch 512/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9824\n",
      "Epoch 512: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1695 - accuracy: 0.9824 - val_loss: 0.6583 - val_accuracy: 0.8568\n",
      "Epoch 513/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9839\n",
      "Epoch 513: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1635 - accuracy: 0.9839 - val_loss: 0.7722 - val_accuracy: 0.8390\n",
      "Epoch 514/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9791\n",
      "Epoch 514: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1765 - accuracy: 0.9791 - val_loss: 0.7179 - val_accuracy: 0.8540\n",
      "Epoch 515/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9797\n",
      "Epoch 515: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1737 - accuracy: 0.9797 - val_loss: 0.7262 - val_accuracy: 0.8295\n",
      "Epoch 516/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9858\n",
      "Epoch 516: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1594 - accuracy: 0.9858 - val_loss: 0.8533 - val_accuracy: 0.8335\n",
      "Epoch 517/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9792\n",
      "Epoch 517: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1771 - accuracy: 0.9792 - val_loss: 0.6983 - val_accuracy: 0.8618\n",
      "Epoch 518/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9814\n",
      "Epoch 518: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 408s 272ms/step - loss: 0.1702 - accuracy: 0.9814 - val_loss: 0.7153 - val_accuracy: 0.8537\n",
      "Epoch 519/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9826\n",
      "Epoch 519: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 271ms/step - loss: 0.1679 - accuracy: 0.9826 - val_loss: 0.6712 - val_accuracy: 0.8545\n",
      "Epoch 520/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9818\n",
      "Epoch 520: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1679 - accuracy: 0.9818 - val_loss: 0.8289 - val_accuracy: 0.8205\n",
      "Epoch 521/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9769\n",
      "Epoch 521: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1819 - accuracy: 0.9769 - val_loss: 0.6313 - val_accuracy: 0.8535\n",
      "Epoch 522/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9829\n",
      "Epoch 522: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1658 - accuracy: 0.9829 - val_loss: 0.9441 - val_accuracy: 0.8070\n",
      "Epoch 523/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9831\n",
      "Epoch 523: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1665 - accuracy: 0.9831 - val_loss: 0.7636 - val_accuracy: 0.8468\n",
      "Epoch 524/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9803\n",
      "Epoch 524: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1724 - accuracy: 0.9803 - val_loss: 0.7515 - val_accuracy: 0.8547\n",
      "Epoch 525/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9793\n",
      "Epoch 525: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1733 - accuracy: 0.9793 - val_loss: 0.7146 - val_accuracy: 0.8420\n",
      "Epoch 526/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9803\n",
      "Epoch 526: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1730 - accuracy: 0.9803 - val_loss: 0.6999 - val_accuracy: 0.8537\n",
      "Epoch 527/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9859\n",
      "Epoch 527: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1577 - accuracy: 0.9859 - val_loss: 0.8528 - val_accuracy: 0.8390\n",
      "Epoch 528/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9848\n",
      "Epoch 528: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1589 - accuracy: 0.9848 - val_loss: 0.7390 - val_accuracy: 0.8468\n",
      "Epoch 529/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9825\n",
      "Epoch 529: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1680 - accuracy: 0.9825 - val_loss: 0.7316 - val_accuracy: 0.8505\n",
      "Epoch 530/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9804\n",
      "Epoch 530: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1713 - accuracy: 0.9804 - val_loss: 0.7554 - val_accuracy: 0.8508\n",
      "Epoch 531/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9824\n",
      "Epoch 531: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1673 - accuracy: 0.9824 - val_loss: 0.7100 - val_accuracy: 0.8460\n",
      "Epoch 532/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9818\n",
      "Epoch 532: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1674 - accuracy: 0.9818 - val_loss: 0.6600 - val_accuracy: 0.8580\n",
      "Epoch 533/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9821\n",
      "Epoch 533: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1692 - accuracy: 0.9821 - val_loss: 0.6807 - val_accuracy: 0.8462\n",
      "Epoch 534/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9842\n",
      "Epoch 534: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1637 - accuracy: 0.9842 - val_loss: 0.6892 - val_accuracy: 0.8577\n",
      "Epoch 535/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9880\n",
      "Epoch 535: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1553 - accuracy: 0.9880 - val_loss: 0.6787 - val_accuracy: 0.8720\n",
      "Epoch 536/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9840\n",
      "Epoch 536: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 407s 272ms/step - loss: 0.1622 - accuracy: 0.9840 - val_loss: 0.7008 - val_accuracy: 0.8655\n",
      "Epoch 537/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9858\n",
      "Epoch 537: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1559 - accuracy: 0.9858 - val_loss: 0.8515 - val_accuracy: 0.8465\n",
      "Epoch 538/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9849\n",
      "Epoch 538: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1611 - accuracy: 0.9849 - val_loss: 0.7133 - val_accuracy: 0.8465\n",
      "Epoch 539/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9867\n",
      "Epoch 539: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1557 - accuracy: 0.9867 - val_loss: 0.7769 - val_accuracy: 0.8535\n",
      "Epoch 540/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9855\n",
      "Epoch 540: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1586 - accuracy: 0.9855 - val_loss: 0.7233 - val_accuracy: 0.8520\n",
      "Epoch 541/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9850\n",
      "Epoch 541: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1601 - accuracy: 0.9850 - val_loss: 0.8944 - val_accuracy: 0.8300\n",
      "Epoch 542/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9825\n",
      "Epoch 542: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1646 - accuracy: 0.9825 - val_loss: 0.9421 - val_accuracy: 0.8303\n",
      "Epoch 543/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9832\n",
      "Epoch 543: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1642 - accuracy: 0.9832 - val_loss: 0.7192 - val_accuracy: 0.8440\n",
      "Epoch 544/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9842\n",
      "Epoch 544: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1619 - accuracy: 0.9842 - val_loss: 0.7877 - val_accuracy: 0.8482\n",
      "Epoch 545/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9829\n",
      "Epoch 545: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 271ms/step - loss: 0.1658 - accuracy: 0.9829 - val_loss: 0.6823 - val_accuracy: 0.8465\n",
      "Epoch 546/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9859\n",
      "Epoch 546: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 406s 270ms/step - loss: 0.1582 - accuracy: 0.9859 - val_loss: 0.7150 - val_accuracy: 0.8345\n",
      "Epoch 547/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9846\n",
      "Epoch 547: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1593 - accuracy: 0.9846 - val_loss: 0.7113 - val_accuracy: 0.8558\n",
      "Epoch 548/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9847\n",
      "Epoch 548: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1602 - accuracy: 0.9847 - val_loss: 0.7366 - val_accuracy: 0.8587\n",
      "Epoch 549/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9846\n",
      "Epoch 549: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1611 - accuracy: 0.9846 - val_loss: 0.8195 - val_accuracy: 0.8372\n",
      "Epoch 550/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9837\n",
      "Epoch 550: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1610 - accuracy: 0.9837 - val_loss: 0.6897 - val_accuracy: 0.8550\n",
      "Epoch 551/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9867\n",
      "Epoch 551: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1575 - accuracy: 0.9867 - val_loss: 0.7319 - val_accuracy: 0.8497\n",
      "Epoch 552/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9871\n",
      "Epoch 552: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1543 - accuracy: 0.9871 - val_loss: 0.9206 - val_accuracy: 0.8338\n",
      "Epoch 553/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9849\n",
      "Epoch 553: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1598 - accuracy: 0.9849 - val_loss: 0.6658 - val_accuracy: 0.8543\n",
      "Epoch 554/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9833\n",
      "Epoch 554: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1629 - accuracy: 0.9833 - val_loss: 0.8609 - val_accuracy: 0.8435\n",
      "Epoch 555/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9848\n",
      "Epoch 555: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1573 - accuracy: 0.9848 - val_loss: 0.7729 - val_accuracy: 0.8472\n",
      "Epoch 556/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9844\n",
      "Epoch 556: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1608 - accuracy: 0.9844 - val_loss: 0.7692 - val_accuracy: 0.8372\n",
      "Epoch 557/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9848\n",
      "Epoch 557: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1571 - accuracy: 0.9848 - val_loss: 0.7468 - val_accuracy: 0.8320\n",
      "Epoch 558/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9857\n",
      "Epoch 558: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1579 - accuracy: 0.9857 - val_loss: 0.7133 - val_accuracy: 0.8340\n",
      "Epoch 559/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9837\n",
      "Epoch 559: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1632 - accuracy: 0.9837 - val_loss: 0.6778 - val_accuracy: 0.8583\n",
      "Epoch 560/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9830\n",
      "Epoch 560: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1617 - accuracy: 0.9830 - val_loss: 0.8141 - val_accuracy: 0.8472\n",
      "Epoch 561/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9843\n",
      "Epoch 561: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1624 - accuracy: 0.9843 - val_loss: 0.7871 - val_accuracy: 0.8480\n",
      "Epoch 562/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9840\n",
      "Epoch 562: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1599 - accuracy: 0.9840 - val_loss: 0.7321 - val_accuracy: 0.8510\n",
      "Epoch 563/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9843\n",
      "Epoch 563: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1602 - accuracy: 0.9843 - val_loss: 0.7129 - val_accuracy: 0.8497\n",
      "Epoch 564/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9839\n",
      "Epoch 564: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1598 - accuracy: 0.9839 - val_loss: 0.6895 - val_accuracy: 0.8590\n",
      "Epoch 565/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9864\n",
      "Epoch 565: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1584 - accuracy: 0.9864 - val_loss: 0.7096 - val_accuracy: 0.8583\n",
      "Epoch 566/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9855\n",
      "Epoch 566: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1553 - accuracy: 0.9855 - val_loss: 0.6941 - val_accuracy: 0.8593\n",
      "Epoch 567/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9862\n",
      "Epoch 567: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1580 - accuracy: 0.9862 - val_loss: 0.7767 - val_accuracy: 0.8522\n",
      "Epoch 568/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9859\n",
      "Epoch 568: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1569 - accuracy: 0.9859 - val_loss: 0.7797 - val_accuracy: 0.8453\n",
      "Epoch 569/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9834\n",
      "Epoch 569: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1617 - accuracy: 0.9834 - val_loss: 0.7255 - val_accuracy: 0.8612\n",
      "Epoch 570/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9859\n",
      "Epoch 570: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1575 - accuracy: 0.9859 - val_loss: 0.7562 - val_accuracy: 0.8425\n",
      "Epoch 571/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9843\n",
      "Epoch 571: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1606 - accuracy: 0.9843 - val_loss: 0.9028 - val_accuracy: 0.8165\n",
      "Epoch 572/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9808\n",
      "Epoch 572: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1669 - accuracy: 0.9808 - val_loss: 0.6924 - val_accuracy: 0.8482\n",
      "Epoch 573/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9855\n",
      "Epoch 573: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1577 - accuracy: 0.9855 - val_loss: 0.9698 - val_accuracy: 0.7975\n",
      "Epoch 574/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9827\n",
      "Epoch 574: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1631 - accuracy: 0.9827 - val_loss: 0.8960 - val_accuracy: 0.8280\n",
      "Epoch 575/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9852\n",
      "Epoch 575: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1578 - accuracy: 0.9852 - val_loss: 0.7301 - val_accuracy: 0.8608\n",
      "Epoch 576/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9893\n",
      "Epoch 576: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1475 - accuracy: 0.9893 - val_loss: 0.7837 - val_accuracy: 0.8470\n",
      "Epoch 577/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9845\n",
      "Epoch 577: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1608 - accuracy: 0.9845 - val_loss: 0.7311 - val_accuracy: 0.8528\n",
      "Epoch 578/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9835\n",
      "Epoch 578: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1625 - accuracy: 0.9835 - val_loss: 0.7931 - val_accuracy: 0.8438\n",
      "Epoch 579/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9880\n",
      "Epoch 579: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1525 - accuracy: 0.9880 - val_loss: 0.7752 - val_accuracy: 0.8510\n",
      "Epoch 580/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9835\n",
      "Epoch 580: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1608 - accuracy: 0.9835 - val_loss: 0.6925 - val_accuracy: 0.8520\n",
      "Epoch 581/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9845\n",
      "Epoch 581: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1598 - accuracy: 0.9845 - val_loss: 0.7284 - val_accuracy: 0.8393\n",
      "Epoch 582/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9855\n",
      "Epoch 582: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1568 - accuracy: 0.9855 - val_loss: 0.6924 - val_accuracy: 0.8450\n",
      "Epoch 583/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9822\n",
      "Epoch 583: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1642 - accuracy: 0.9822 - val_loss: 0.7024 - val_accuracy: 0.8425\n",
      "Epoch 584/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9854\n",
      "Epoch 584: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1596 - accuracy: 0.9854 - val_loss: 0.6926 - val_accuracy: 0.8562\n",
      "Epoch 585/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9835\n",
      "Epoch 585: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1598 - accuracy: 0.9835 - val_loss: 0.6875 - val_accuracy: 0.8540\n",
      "Epoch 586/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9833\n",
      "Epoch 586: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1627 - accuracy: 0.9833 - val_loss: 0.6795 - val_accuracy: 0.8583\n",
      "Epoch 587/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9852\n",
      "Epoch 587: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1590 - accuracy: 0.9852 - val_loss: 0.7192 - val_accuracy: 0.8608\n",
      "Epoch 588/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9838\n",
      "Epoch 588: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1610 - accuracy: 0.9838 - val_loss: 0.6851 - val_accuracy: 0.8550\n",
      "Epoch 589/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9828\n",
      "Epoch 589: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1614 - accuracy: 0.9828 - val_loss: 0.7471 - val_accuracy: 0.8478\n",
      "Epoch 590/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9890\n",
      "Epoch 590: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1502 - accuracy: 0.9890 - val_loss: 0.8106 - val_accuracy: 0.8432\n",
      "Epoch 591/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9852\n",
      "Epoch 591: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1595 - accuracy: 0.9852 - val_loss: 0.6648 - val_accuracy: 0.8550\n",
      "Epoch 592/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9847\n",
      "Epoch 592: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1572 - accuracy: 0.9847 - val_loss: 0.6923 - val_accuracy: 0.8537\n",
      "Epoch 593/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9868\n",
      "Epoch 593: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1559 - accuracy: 0.9868 - val_loss: 0.8386 - val_accuracy: 0.8397\n",
      "Epoch 594/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9837\n",
      "Epoch 594: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1613 - accuracy: 0.9837 - val_loss: 0.7222 - val_accuracy: 0.8347\n",
      "Epoch 595/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9845\n",
      "Epoch 595: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1579 - accuracy: 0.9845 - val_loss: 0.6911 - val_accuracy: 0.8558\n",
      "Epoch 596/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9843\n",
      "Epoch 596: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1597 - accuracy: 0.9843 - val_loss: 0.6975 - val_accuracy: 0.8512\n",
      "Epoch 597/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9847\n",
      "Epoch 597: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1585 - accuracy: 0.9847 - val_loss: 0.6719 - val_accuracy: 0.8637\n",
      "Epoch 598/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9877\n",
      "Epoch 598: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1529 - accuracy: 0.9877 - val_loss: 0.7349 - val_accuracy: 0.8572\n",
      "Epoch 599/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9865\n",
      "Epoch 599: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1540 - accuracy: 0.9865 - val_loss: 0.7222 - val_accuracy: 0.8440\n",
      "Epoch 600/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9833\n",
      "Epoch 600: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1620 - accuracy: 0.9833 - val_loss: 0.6514 - val_accuracy: 0.8478\n",
      "Epoch 601/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9877\n",
      "Epoch 601: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1535 - accuracy: 0.9877 - val_loss: 0.6941 - val_accuracy: 0.8602\n",
      "Epoch 602/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9899\n",
      "Epoch 602: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1456 - accuracy: 0.9899 - val_loss: 0.6839 - val_accuracy: 0.8612\n",
      "Epoch 603/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9852\n",
      "Epoch 603: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1554 - accuracy: 0.9852 - val_loss: 0.7413 - val_accuracy: 0.8500\n",
      "Epoch 604/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9870\n",
      "Epoch 604: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1484 - accuracy: 0.9870 - val_loss: 0.6922 - val_accuracy: 0.8612\n",
      "Epoch 605/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9884\n",
      "Epoch 605: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1489 - accuracy: 0.9884 - val_loss: 0.8846 - val_accuracy: 0.8253\n",
      "Epoch 606/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9833\n",
      "Epoch 606: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1590 - accuracy: 0.9833 - val_loss: 0.7115 - val_accuracy: 0.8447\n",
      "Epoch 607/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9888\n",
      "Epoch 607: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1488 - accuracy: 0.9888 - val_loss: 0.7269 - val_accuracy: 0.8540\n",
      "Epoch 608/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9853\n",
      "Epoch 608: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1540 - accuracy: 0.9853 - val_loss: 0.7577 - val_accuracy: 0.8450\n",
      "Epoch 609/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9869\n",
      "Epoch 609: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1526 - accuracy: 0.9869 - val_loss: 0.7338 - val_accuracy: 0.8530\n",
      "Epoch 610/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9905\n",
      "Epoch 610: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1428 - accuracy: 0.9905 - val_loss: 0.8435 - val_accuracy: 0.8465\n",
      "Epoch 611/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1506 - accuracy: 0.9869\n",
      "Epoch 611: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 280ms/step - loss: 0.1506 - accuracy: 0.9869 - val_loss: 0.7998 - val_accuracy: 0.8503\n",
      "Epoch 612/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9845\n",
      "Epoch 612: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1551 - accuracy: 0.9845 - val_loss: 0.7721 - val_accuracy: 0.8485\n",
      "Epoch 613/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9893\n",
      "Epoch 613: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1439 - accuracy: 0.9893 - val_loss: 0.7711 - val_accuracy: 0.8543\n",
      "Epoch 614/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9863\n",
      "Epoch 614: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1553 - accuracy: 0.9863 - val_loss: 0.7028 - val_accuracy: 0.8655\n",
      "Epoch 615/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9893\n",
      "Epoch 615: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1469 - accuracy: 0.9893 - val_loss: 0.7246 - val_accuracy: 0.8645\n",
      "Epoch 616/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9854\n",
      "Epoch 616: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1536 - accuracy: 0.9854 - val_loss: 0.7340 - val_accuracy: 0.8518\n",
      "Epoch 617/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9858\n",
      "Epoch 617: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1511 - accuracy: 0.9858 - val_loss: 0.7032 - val_accuracy: 0.8490\n",
      "Epoch 618/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9883\n",
      "Epoch 618: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1466 - accuracy: 0.9883 - val_loss: 0.7479 - val_accuracy: 0.8568\n",
      "Epoch 619/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9875\n",
      "Epoch 619: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1500 - accuracy: 0.9875 - val_loss: 0.6690 - val_accuracy: 0.8555\n",
      "Epoch 620/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9903\n",
      "Epoch 620: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1378 - accuracy: 0.9903 - val_loss: 0.7734 - val_accuracy: 0.8562\n",
      "Epoch 621/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9812\n",
      "Epoch 621: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1631 - accuracy: 0.9812 - val_loss: 0.6571 - val_accuracy: 0.8600\n",
      "Epoch 622/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9899\n",
      "Epoch 622: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1433 - accuracy: 0.9899 - val_loss: 0.8152 - val_accuracy: 0.8453\n",
      "Epoch 623/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9837\n",
      "Epoch 623: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1600 - accuracy: 0.9837 - val_loss: 0.7668 - val_accuracy: 0.8355\n",
      "Epoch 624/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9845\n",
      "Epoch 624: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1573 - accuracy: 0.9845 - val_loss: 0.7134 - val_accuracy: 0.8522\n",
      "Epoch 625/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9905\n",
      "Epoch 625: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1397 - accuracy: 0.9905 - val_loss: 1.0442 - val_accuracy: 0.8155\n",
      "Epoch 626/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9853\n",
      "Epoch 626: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1547 - accuracy: 0.9853 - val_loss: 0.6858 - val_accuracy: 0.8482\n",
      "Epoch 627/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9885\n",
      "Epoch 627: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1474 - accuracy: 0.9885 - val_loss: 0.7373 - val_accuracy: 0.8547\n",
      "Epoch 628/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9841\n",
      "Epoch 628: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1567 - accuracy: 0.9841 - val_loss: 0.7215 - val_accuracy: 0.8572\n",
      "Epoch 629/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9848\n",
      "Epoch 629: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1556 - accuracy: 0.9848 - val_loss: 0.7755 - val_accuracy: 0.8430\n",
      "Epoch 630/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9866\n",
      "Epoch 630: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1508 - accuracy: 0.9866 - val_loss: 0.7496 - val_accuracy: 0.8610\n",
      "Epoch 631/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.9861\n",
      "Epoch 631: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1494 - accuracy: 0.9861 - val_loss: 0.7078 - val_accuracy: 0.8593\n",
      "Epoch 632/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9915\n",
      "Epoch 632: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1399 - accuracy: 0.9915 - val_loss: 0.7401 - val_accuracy: 0.8602\n",
      "Epoch 633/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9918\n",
      "Epoch 633: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1369 - accuracy: 0.9918 - val_loss: 0.7235 - val_accuracy: 0.8537\n",
      "Epoch 634/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9892\n",
      "Epoch 634: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1440 - accuracy: 0.9892 - val_loss: 0.7778 - val_accuracy: 0.8520\n",
      "Epoch 635/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9812\n",
      "Epoch 635: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1586 - accuracy: 0.9812 - val_loss: 0.6903 - val_accuracy: 0.8422\n",
      "Epoch 636/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9895\n",
      "Epoch 636: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1414 - accuracy: 0.9895 - val_loss: 0.7441 - val_accuracy: 0.8540\n",
      "Epoch 637/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9872\n",
      "Epoch 637: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1480 - accuracy: 0.9872 - val_loss: 0.7098 - val_accuracy: 0.8648\n",
      "Epoch 638/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9860\n",
      "Epoch 638: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1485 - accuracy: 0.9860 - val_loss: 0.6676 - val_accuracy: 0.8537\n",
      "Epoch 639/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9843\n",
      "Epoch 639: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1538 - accuracy: 0.9843 - val_loss: 0.7433 - val_accuracy: 0.8587\n",
      "Epoch 640/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9883\n",
      "Epoch 640: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1448 - accuracy: 0.9883 - val_loss: 0.7485 - val_accuracy: 0.8558\n",
      "Epoch 641/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9845\n",
      "Epoch 641: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1561 - accuracy: 0.9845 - val_loss: 0.7164 - val_accuracy: 0.8537\n",
      "Epoch 642/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9861\n",
      "Epoch 642: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1485 - accuracy: 0.9861 - val_loss: 0.7190 - val_accuracy: 0.8535\n",
      "Epoch 643/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9881\n",
      "Epoch 643: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1468 - accuracy: 0.9881 - val_loss: 0.7102 - val_accuracy: 0.8553\n",
      "Epoch 644/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9897\n",
      "Epoch 644: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1421 - accuracy: 0.9897 - val_loss: 0.7779 - val_accuracy: 0.8518\n",
      "Epoch 645/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9898\n",
      "Epoch 645: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1425 - accuracy: 0.9898 - val_loss: 0.6863 - val_accuracy: 0.8670\n",
      "Epoch 646/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9923\n",
      "Epoch 646: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1341 - accuracy: 0.9923 - val_loss: 1.0717 - val_accuracy: 0.8330\n",
      "Epoch 647/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9843\n",
      "Epoch 647: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1550 - accuracy: 0.9843 - val_loss: 0.7190 - val_accuracy: 0.8385\n",
      "Epoch 648/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9871\n",
      "Epoch 648: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1452 - accuracy: 0.9871 - val_loss: 0.7015 - val_accuracy: 0.8487\n",
      "Epoch 649/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9841\n",
      "Epoch 649: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1562 - accuracy: 0.9841 - val_loss: 0.7399 - val_accuracy: 0.8482\n",
      "Epoch 650/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9852\n",
      "Epoch 650: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1534 - accuracy: 0.9852 - val_loss: 0.6713 - val_accuracy: 0.8378\n",
      "Epoch 651/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9871\n",
      "Epoch 651: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1497 - accuracy: 0.9871 - val_loss: 0.7252 - val_accuracy: 0.8562\n",
      "Epoch 652/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9877\n",
      "Epoch 652: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1447 - accuracy: 0.9877 - val_loss: 0.8116 - val_accuracy: 0.8355\n",
      "Epoch 653/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9852\n",
      "Epoch 653: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1520 - accuracy: 0.9852 - val_loss: 0.7606 - val_accuracy: 0.8520\n",
      "Epoch 654/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9898\n",
      "Epoch 654: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 281ms/step - loss: 0.1413 - accuracy: 0.9898 - val_loss: 0.6309 - val_accuracy: 0.8630\n",
      "Epoch 655/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9899\n",
      "Epoch 655: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1398 - accuracy: 0.9899 - val_loss: 0.7595 - val_accuracy: 0.8495\n",
      "Epoch 656/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9889\n",
      "Epoch 656: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1455 - accuracy: 0.9889 - val_loss: 0.7162 - val_accuracy: 0.8525\n",
      "Epoch 657/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9912\n",
      "Epoch 657: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1368 - accuracy: 0.9912 - val_loss: 0.8476 - val_accuracy: 0.8367\n",
      "Epoch 658/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9898\n",
      "Epoch 658: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1405 - accuracy: 0.9898 - val_loss: 0.7292 - val_accuracy: 0.8553\n",
      "Epoch 659/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9926\n",
      "Epoch 659: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1322 - accuracy: 0.9926 - val_loss: 0.7746 - val_accuracy: 0.8568\n",
      "Epoch 660/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9882\n",
      "Epoch 660: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1415 - accuracy: 0.9882 - val_loss: 0.6681 - val_accuracy: 0.8522\n",
      "Epoch 661/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9877\n",
      "Epoch 661: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 422s 282ms/step - loss: 0.1482 - accuracy: 0.9877 - val_loss: 0.7979 - val_accuracy: 0.8465\n",
      "Epoch 662/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9880\n",
      "Epoch 662: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1411 - accuracy: 0.9880 - val_loss: 0.6942 - val_accuracy: 0.8665\n",
      "Epoch 663/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9860\n",
      "Epoch 663: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 432s 288ms/step - loss: 0.1460 - accuracy: 0.9860 - val_loss: 0.7424 - val_accuracy: 0.8583\n",
      "Epoch 664/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9839\n",
      "Epoch 664: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 438s 292ms/step - loss: 0.1557 - accuracy: 0.9839 - val_loss: 0.7195 - val_accuracy: 0.8482\n",
      "Epoch 665/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9823\n",
      "Epoch 665: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 438s 292ms/step - loss: 0.1576 - accuracy: 0.9823 - val_loss: 0.7259 - val_accuracy: 0.8430\n",
      "Epoch 666/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9877\n",
      "Epoch 666: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 435s 290ms/step - loss: 0.1444 - accuracy: 0.9877 - val_loss: 0.6841 - val_accuracy: 0.8570\n",
      "Epoch 667/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9867\n",
      "Epoch 667: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 439s 293ms/step - loss: 0.1447 - accuracy: 0.9867 - val_loss: 0.7963 - val_accuracy: 0.8245\n",
      "Epoch 668/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9908\n",
      "Epoch 668: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1392 - accuracy: 0.9908 - val_loss: 0.7581 - val_accuracy: 0.8575\n",
      "Epoch 669/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9923\n",
      "Epoch 669: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 419s 279ms/step - loss: 0.1329 - accuracy: 0.9923 - val_loss: 0.7518 - val_accuracy: 0.8443\n",
      "Epoch 670/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9884\n",
      "Epoch 670: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 429s 286ms/step - loss: 0.1406 - accuracy: 0.9884 - val_loss: 0.9131 - val_accuracy: 0.8403\n",
      "Epoch 671/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9871\n",
      "Epoch 671: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1414 - accuracy: 0.9871 - val_loss: 0.7404 - val_accuracy: 0.8475\n",
      "Epoch 672/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9872\n",
      "Epoch 672: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1453 - accuracy: 0.9872 - val_loss: 0.6848 - val_accuracy: 0.8512\n",
      "Epoch 673/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9888\n",
      "Epoch 673: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 277ms/step - loss: 0.1430 - accuracy: 0.9888 - val_loss: 0.6857 - val_accuracy: 0.8630\n",
      "Epoch 674/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9918\n",
      "Epoch 674: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1360 - accuracy: 0.9918 - val_loss: 0.8137 - val_accuracy: 0.8422\n",
      "Epoch 675/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9949\n",
      "Epoch 675: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 413s 275ms/step - loss: 0.1251 - accuracy: 0.9949 - val_loss: 0.8830 - val_accuracy: 0.8320\n",
      "Epoch 676/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9895\n",
      "Epoch 676: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 414s 276ms/step - loss: 0.1410 - accuracy: 0.9895 - val_loss: 0.7739 - val_accuracy: 0.8522\n",
      "Epoch 677/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9882\n",
      "Epoch 677: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 411s 274ms/step - loss: 0.1414 - accuracy: 0.9882 - val_loss: 0.6986 - val_accuracy: 0.8518\n",
      "Epoch 678/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9887\n",
      "Epoch 678: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 411s 274ms/step - loss: 0.1439 - accuracy: 0.9887 - val_loss: 0.7116 - val_accuracy: 0.8550\n",
      "Epoch 679/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9862\n",
      "Epoch 679: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 410s 273ms/step - loss: 0.1454 - accuracy: 0.9862 - val_loss: 0.7409 - val_accuracy: 0.8418\n",
      "Epoch 680/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9859\n",
      "Epoch 680: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 416s 278ms/step - loss: 0.1461 - accuracy: 0.9859 - val_loss: 0.6941 - val_accuracy: 0.8595\n",
      "Epoch 681/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9865\n",
      "Epoch 681: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 418s 279ms/step - loss: 0.1440 - accuracy: 0.9865 - val_loss: 0.7958 - val_accuracy: 0.8278\n",
      "Epoch 682/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9883\n",
      "Epoch 682: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 415s 277ms/step - loss: 0.1434 - accuracy: 0.9883 - val_loss: 0.6962 - val_accuracy: 0.8518\n",
      "Epoch 683/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9908\n",
      "Epoch 683: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 421s 281ms/step - loss: 0.1343 - accuracy: 0.9908 - val_loss: 0.7177 - val_accuracy: 0.8570\n",
      "Epoch 684/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9882\n",
      "Epoch 684: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 420s 280ms/step - loss: 0.1419 - accuracy: 0.9882 - val_loss: 0.7370 - val_accuracy: 0.8590\n",
      "Epoch 685/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9844\n",
      "Epoch 685: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 417s 278ms/step - loss: 0.1480 - accuracy: 0.9844 - val_loss: 0.7435 - val_accuracy: 0.8400\n",
      "Epoch 686/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9883\n",
      "Epoch 686: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1406 - accuracy: 0.9883 - val_loss: 0.7646 - val_accuracy: 0.8425\n",
      "Epoch 687/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9874\n",
      "Epoch 687: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 426s 284ms/step - loss: 0.1434 - accuracy: 0.9874 - val_loss: 0.7216 - val_accuracy: 0.8602\n",
      "Epoch 688/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9877\n",
      "Epoch 688: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1426 - accuracy: 0.9877 - val_loss: 0.7560 - val_accuracy: 0.8490\n",
      "Epoch 689/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9862\n",
      "Epoch 689: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1483 - accuracy: 0.9862 - val_loss: 0.6659 - val_accuracy: 0.8558\n",
      "Epoch 690/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9906\n",
      "Epoch 690: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1371 - accuracy: 0.9906 - val_loss: 0.8165 - val_accuracy: 0.8530\n",
      "Epoch 691/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9886\n",
      "Epoch 691: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1421 - accuracy: 0.9886 - val_loss: 0.7864 - val_accuracy: 0.8522\n",
      "Epoch 692/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9849\n",
      "Epoch 692: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1468 - accuracy: 0.9849 - val_loss: 0.7908 - val_accuracy: 0.8390\n",
      "Epoch 693/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9862\n",
      "Epoch 693: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1470 - accuracy: 0.9862 - val_loss: 0.8256 - val_accuracy: 0.8375\n",
      "Epoch 694/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9894\n",
      "Epoch 694: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1362 - accuracy: 0.9894 - val_loss: 0.7693 - val_accuracy: 0.8558\n",
      "Epoch 695/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9877\n",
      "Epoch 695: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1430 - accuracy: 0.9877 - val_loss: 0.7986 - val_accuracy: 0.8375\n",
      "Epoch 696/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9854\n",
      "Epoch 696: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1472 - accuracy: 0.9854 - val_loss: 0.7055 - val_accuracy: 0.8525\n",
      "Epoch 697/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9855\n",
      "Epoch 697: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1472 - accuracy: 0.9855 - val_loss: 0.7261 - val_accuracy: 0.8495\n",
      "Epoch 698/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9884\n",
      "Epoch 698: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1402 - accuracy: 0.9884 - val_loss: 0.7881 - val_accuracy: 0.8453\n",
      "Epoch 699/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9897\n",
      "Epoch 699: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1387 - accuracy: 0.9897 - val_loss: 0.7231 - val_accuracy: 0.8558\n",
      "Epoch 700/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9890\n",
      "Epoch 700: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1392 - accuracy: 0.9890 - val_loss: 0.6950 - val_accuracy: 0.8505\n",
      "Epoch 701/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9891\n",
      "Epoch 701: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1382 - accuracy: 0.9891 - val_loss: 0.7477 - val_accuracy: 0.8510\n",
      "Epoch 702/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9902\n",
      "Epoch 702: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1336 - accuracy: 0.9902 - val_loss: 0.8149 - val_accuracy: 0.8405\n",
      "Epoch 703/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9887\n",
      "Epoch 703: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9887 - val_loss: 0.9238 - val_accuracy: 0.8163\n",
      "Epoch 704/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9870\n",
      "Epoch 704: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1433 - accuracy: 0.9870 - val_loss: 0.7347 - val_accuracy: 0.8512\n",
      "Epoch 705/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9860\n",
      "Epoch 705: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1467 - accuracy: 0.9860 - val_loss: 0.6777 - val_accuracy: 0.8533\n",
      "Epoch 706/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9878\n",
      "Epoch 706: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1406 - accuracy: 0.9878 - val_loss: 0.8143 - val_accuracy: 0.8440\n",
      "Epoch 707/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9877\n",
      "Epoch 707: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1407 - accuracy: 0.9877 - val_loss: 0.6718 - val_accuracy: 0.8668\n",
      "Epoch 708/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9904\n",
      "Epoch 708: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1349 - accuracy: 0.9904 - val_loss: 0.7232 - val_accuracy: 0.8635\n",
      "Epoch 709/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9891\n",
      "Epoch 709: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1362 - accuracy: 0.9891 - val_loss: 0.7729 - val_accuracy: 0.8363\n",
      "Epoch 710/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9868\n",
      "Epoch 710: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1440 - accuracy: 0.9868 - val_loss: 0.7576 - val_accuracy: 0.8570\n",
      "Epoch 711/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9916\n",
      "Epoch 711: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1318 - accuracy: 0.9916 - val_loss: 0.9236 - val_accuracy: 0.8400\n",
      "Epoch 712/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9877\n",
      "Epoch 712: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1432 - accuracy: 0.9877 - val_loss: 0.7622 - val_accuracy: 0.8400\n",
      "Epoch 713/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9870\n",
      "Epoch 713: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1442 - accuracy: 0.9870 - val_loss: 0.7225 - val_accuracy: 0.8485\n",
      "Epoch 714/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9923\n",
      "Epoch 714: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1316 - accuracy: 0.9923 - val_loss: 0.7398 - val_accuracy: 0.8495\n",
      "Epoch 715/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9893\n",
      "Epoch 715: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1367 - accuracy: 0.9893 - val_loss: 0.7546 - val_accuracy: 0.8605\n",
      "Epoch 716/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9823\n",
      "Epoch 716: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1559 - accuracy: 0.9823 - val_loss: 0.6297 - val_accuracy: 0.8522\n",
      "Epoch 717/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9876\n",
      "Epoch 717: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1399 - accuracy: 0.9876 - val_loss: 0.7073 - val_accuracy: 0.8528\n",
      "Epoch 718/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9863\n",
      "Epoch 718: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1444 - accuracy: 0.9863 - val_loss: 0.6902 - val_accuracy: 0.8587\n",
      "Epoch 719/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9876\n",
      "Epoch 719: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1431 - accuracy: 0.9876 - val_loss: 0.7891 - val_accuracy: 0.8480\n",
      "Epoch 720/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9866\n",
      "Epoch 720: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1439 - accuracy: 0.9866 - val_loss: 0.7395 - val_accuracy: 0.8322\n",
      "Epoch 721/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9879\n",
      "Epoch 721: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1383 - accuracy: 0.9879 - val_loss: 0.7335 - val_accuracy: 0.8525\n",
      "Epoch 722/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9883\n",
      "Epoch 722: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1420 - accuracy: 0.9883 - val_loss: 0.7317 - val_accuracy: 0.8420\n",
      "Epoch 723/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9877\n",
      "Epoch 723: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1452 - accuracy: 0.9877 - val_loss: 0.6799 - val_accuracy: 0.8595\n",
      "Epoch 724/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9872\n",
      "Epoch 724: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1411 - accuracy: 0.9872 - val_loss: 0.7854 - val_accuracy: 0.8347\n",
      "Epoch 725/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9872\n",
      "Epoch 725: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1422 - accuracy: 0.9872 - val_loss: 0.7360 - val_accuracy: 0.8540\n",
      "Epoch 726/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9833\n",
      "Epoch 726: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1527 - accuracy: 0.9833 - val_loss: 0.7249 - val_accuracy: 0.8363\n",
      "Epoch 727/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9881\n",
      "Epoch 727: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 425s 283ms/step - loss: 0.1400 - accuracy: 0.9881 - val_loss: 0.7344 - val_accuracy: 0.8533\n",
      "Epoch 728/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9898\n",
      "Epoch 728: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9898 - val_loss: 0.7354 - val_accuracy: 0.8490\n",
      "Epoch 729/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9905\n",
      "Epoch 729: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1331 - accuracy: 0.9905 - val_loss: 0.7654 - val_accuracy: 0.8315\n",
      "Epoch 730/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9884\n",
      "Epoch 730: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1402 - accuracy: 0.9884 - val_loss: 0.8508 - val_accuracy: 0.8407\n",
      "Epoch 731/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9882\n",
      "Epoch 731: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9882 - val_loss: 0.7166 - val_accuracy: 0.8605\n",
      "Epoch 732/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9859\n",
      "Epoch 732: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1484 - accuracy: 0.9859 - val_loss: 0.7211 - val_accuracy: 0.8453\n",
      "Epoch 733/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9890\n",
      "Epoch 733: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9890 - val_loss: 0.6876 - val_accuracy: 0.8562\n",
      "Epoch 734/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9861\n",
      "Epoch 734: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1452 - accuracy: 0.9861 - val_loss: 0.7420 - val_accuracy: 0.8443\n",
      "Epoch 735/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9908\n",
      "Epoch 735: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1304 - accuracy: 0.9908 - val_loss: 0.7381 - val_accuracy: 0.8558\n",
      "Epoch 736/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9898\n",
      "Epoch 736: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1365 - accuracy: 0.9898 - val_loss: 0.7308 - val_accuracy: 0.8558\n",
      "Epoch 737/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9888\n",
      "Epoch 737: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1370 - accuracy: 0.9888 - val_loss: 0.7776 - val_accuracy: 0.8447\n",
      "Epoch 738/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9907\n",
      "Epoch 738: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1313 - accuracy: 0.9907 - val_loss: 0.8214 - val_accuracy: 0.8378\n",
      "Epoch 739/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9899\n",
      "Epoch 739: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1375 - accuracy: 0.9899 - val_loss: 0.7200 - val_accuracy: 0.8522\n",
      "Epoch 740/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9912\n",
      "Epoch 740: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1342 - accuracy: 0.9912 - val_loss: 0.7446 - val_accuracy: 0.8570\n",
      "Epoch 741/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9911\n",
      "Epoch 741: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1323 - accuracy: 0.9911 - val_loss: 0.8283 - val_accuracy: 0.8265\n",
      "Epoch 742/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9884\n",
      "Epoch 742: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1397 - accuracy: 0.9884 - val_loss: 0.8797 - val_accuracy: 0.8300\n",
      "Epoch 743/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9884\n",
      "Epoch 743: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1385 - accuracy: 0.9884 - val_loss: 0.8520 - val_accuracy: 0.8317\n",
      "Epoch 744/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9903\n",
      "Epoch 744: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1320 - accuracy: 0.9903 - val_loss: 0.7697 - val_accuracy: 0.8457\n",
      "Epoch 745/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9928\n",
      "Epoch 745: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1262 - accuracy: 0.9928 - val_loss: 0.8139 - val_accuracy: 0.8455\n",
      "Epoch 746/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9879\n",
      "Epoch 746: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1388 - accuracy: 0.9879 - val_loss: 0.7490 - val_accuracy: 0.8590\n",
      "Epoch 747/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9881\n",
      "Epoch 747: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1390 - accuracy: 0.9881 - val_loss: 0.7455 - val_accuracy: 0.8540\n",
      "Epoch 748/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9896\n",
      "Epoch 748: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1335 - accuracy: 0.9896 - val_loss: 0.6918 - val_accuracy: 0.8577\n",
      "Epoch 749/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9919\n",
      "Epoch 749: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1282 - accuracy: 0.9919 - val_loss: 0.8083 - val_accuracy: 0.8420\n",
      "Epoch 750/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9896\n",
      "Epoch 750: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1354 - accuracy: 0.9896 - val_loss: 0.7425 - val_accuracy: 0.8543\n",
      "Epoch 751/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9891\n",
      "Epoch 751: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1385 - accuracy: 0.9891 - val_loss: 0.6564 - val_accuracy: 0.8640\n",
      "Epoch 752/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9909\n",
      "Epoch 752: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1331 - accuracy: 0.9909 - val_loss: 0.7609 - val_accuracy: 0.8355\n",
      "Epoch 753/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9903\n",
      "Epoch 753: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1346 - accuracy: 0.9903 - val_loss: 0.6912 - val_accuracy: 0.8580\n",
      "Epoch 754/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9914\n",
      "Epoch 754: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1306 - accuracy: 0.9914 - val_loss: 0.8074 - val_accuracy: 0.8313\n",
      "Epoch 755/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9907\n",
      "Epoch 755: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1335 - accuracy: 0.9907 - val_loss: 0.6893 - val_accuracy: 0.8530\n",
      "Epoch 756/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9875\n",
      "Epoch 756: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1388 - accuracy: 0.9875 - val_loss: 0.6987 - val_accuracy: 0.8648\n",
      "Epoch 757/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9906\n",
      "Epoch 757: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1333 - accuracy: 0.9906 - val_loss: 0.8827 - val_accuracy: 0.8347\n",
      "Epoch 758/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9847\n",
      "Epoch 758: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1490 - accuracy: 0.9847 - val_loss: 0.6560 - val_accuracy: 0.8533\n",
      "Epoch 759/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9900\n",
      "Epoch 759: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1337 - accuracy: 0.9900 - val_loss: 0.7372 - val_accuracy: 0.8462\n",
      "Epoch 760/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9889\n",
      "Epoch 760: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1353 - accuracy: 0.9889 - val_loss: 0.7817 - val_accuracy: 0.8475\n",
      "Epoch 761/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9887\n",
      "Epoch 761: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1351 - accuracy: 0.9887 - val_loss: 0.8869 - val_accuracy: 0.8338\n",
      "Epoch 762/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9869\n",
      "Epoch 762: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1398 - accuracy: 0.9869 - val_loss: 0.6899 - val_accuracy: 0.8487\n",
      "Epoch 763/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9880\n",
      "Epoch 763: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1367 - accuracy: 0.9880 - val_loss: 0.7510 - val_accuracy: 0.8583\n",
      "Epoch 764/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9899\n",
      "Epoch 764: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1356 - accuracy: 0.9899 - val_loss: 0.7138 - val_accuracy: 0.8528\n",
      "Epoch 765/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9937\n",
      "Epoch 765: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1228 - accuracy: 0.9937 - val_loss: 0.7454 - val_accuracy: 0.8568\n",
      "Epoch 766/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9892\n",
      "Epoch 766: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1365 - accuracy: 0.9892 - val_loss: 0.7035 - val_accuracy: 0.8447\n",
      "Epoch 767/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9901\n",
      "Epoch 767: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1337 - accuracy: 0.9901 - val_loss: 0.7366 - val_accuracy: 0.8543\n",
      "Epoch 768/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9908\n",
      "Epoch 768: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1289 - accuracy: 0.9908 - val_loss: 0.9892 - val_accuracy: 0.8170\n",
      "Epoch 769/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9882\n",
      "Epoch 769: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1381 - accuracy: 0.9882 - val_loss: 0.7683 - val_accuracy: 0.8487\n",
      "Epoch 770/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9877\n",
      "Epoch 770: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1392 - accuracy: 0.9877 - val_loss: 0.6755 - val_accuracy: 0.8600\n",
      "Epoch 771/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9926\n",
      "Epoch 771: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1249 - accuracy: 0.9926 - val_loss: 0.7187 - val_accuracy: 0.8610\n",
      "Epoch 772/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9934\n",
      "Epoch 772: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1240 - accuracy: 0.9934 - val_loss: 0.7731 - val_accuracy: 0.8393\n",
      "Epoch 773/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9916\n",
      "Epoch 773: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1293 - accuracy: 0.9916 - val_loss: 0.8094 - val_accuracy: 0.8482\n",
      "Epoch 774/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9889\n",
      "Epoch 774: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1370 - accuracy: 0.9889 - val_loss: 0.7241 - val_accuracy: 0.8547\n",
      "Epoch 775/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9931\n",
      "Epoch 775: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1233 - accuracy: 0.9931 - val_loss: 0.7918 - val_accuracy: 0.8360\n",
      "Epoch 776/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9887\n",
      "Epoch 776: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1327 - accuracy: 0.9887 - val_loss: 0.7241 - val_accuracy: 0.8515\n",
      "Epoch 777/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9897\n",
      "Epoch 777: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1315 - accuracy: 0.9897 - val_loss: 0.6792 - val_accuracy: 0.8518\n",
      "Epoch 778/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9925\n",
      "Epoch 778: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1274 - accuracy: 0.9925 - val_loss: 0.7743 - val_accuracy: 0.8465\n",
      "Epoch 779/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9901\n",
      "Epoch 779: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1319 - accuracy: 0.9901 - val_loss: 0.7550 - val_accuracy: 0.8605\n",
      "Epoch 780/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9868\n",
      "Epoch 780: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1427 - accuracy: 0.9868 - val_loss: 0.6658 - val_accuracy: 0.8520\n",
      "Epoch 781/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9902\n",
      "Epoch 781: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1328 - accuracy: 0.9902 - val_loss: 0.7571 - val_accuracy: 0.8320\n",
      "Epoch 782/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9887\n",
      "Epoch 782: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1351 - accuracy: 0.9887 - val_loss: 0.7844 - val_accuracy: 0.8363\n",
      "Epoch 783/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9922\n",
      "Epoch 783: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1278 - accuracy: 0.9922 - val_loss: 0.7473 - val_accuracy: 0.8620\n",
      "Epoch 784/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9912\n",
      "Epoch 784: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1300 - accuracy: 0.9912 - val_loss: 0.7155 - val_accuracy: 0.8627\n",
      "Epoch 785/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9887\n",
      "Epoch 785: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1326 - accuracy: 0.9887 - val_loss: 0.7613 - val_accuracy: 0.8572\n",
      "Epoch 786/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9868\n",
      "Epoch 786: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1393 - accuracy: 0.9868 - val_loss: 0.7015 - val_accuracy: 0.8572\n",
      "Epoch 787/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9913\n",
      "Epoch 787: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1276 - accuracy: 0.9913 - val_loss: 0.7988 - val_accuracy: 0.8372\n",
      "Epoch 788/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9864\n",
      "Epoch 788: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1407 - accuracy: 0.9864 - val_loss: 0.7371 - val_accuracy: 0.8447\n",
      "Epoch 789/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9896\n",
      "Epoch 789: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1333 - accuracy: 0.9896 - val_loss: 0.8675 - val_accuracy: 0.8227\n",
      "Epoch 790/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9880\n",
      "Epoch 790: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1330 - accuracy: 0.9880 - val_loss: 0.7816 - val_accuracy: 0.8545\n",
      "Epoch 791/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9857\n",
      "Epoch 791: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1400 - accuracy: 0.9857 - val_loss: 0.7870 - val_accuracy: 0.8420\n",
      "Epoch 792/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9897\n",
      "Epoch 792: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1345 - accuracy: 0.9897 - val_loss: 0.7232 - val_accuracy: 0.8595\n",
      "Epoch 793/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9913\n",
      "Epoch 793: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 282ms/step - loss: 0.1260 - accuracy: 0.9913 - val_loss: 0.8365 - val_accuracy: 0.8407\n",
      "Epoch 794/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9864\n",
      "Epoch 794: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1391 - accuracy: 0.9864 - val_loss: 0.8136 - val_accuracy: 0.8350\n",
      "Epoch 795/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9866\n",
      "Epoch 795: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1411 - accuracy: 0.9866 - val_loss: 0.7095 - val_accuracy: 0.8605\n",
      "Epoch 796/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9885\n",
      "Epoch 796: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1323 - accuracy: 0.9885 - val_loss: 0.7467 - val_accuracy: 0.8487\n",
      "Epoch 797/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9845\n",
      "Epoch 797: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1468 - accuracy: 0.9845 - val_loss: 0.6984 - val_accuracy: 0.8622\n",
      "Epoch 798/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9877\n",
      "Epoch 798: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1346 - accuracy: 0.9877 - val_loss: 0.7324 - val_accuracy: 0.8440\n",
      "Epoch 799/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9910\n",
      "Epoch 799: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 423s 282ms/step - loss: 0.1317 - accuracy: 0.9910 - val_loss: 0.7011 - val_accuracy: 0.8455\n",
      "Epoch 800/800\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9923\n",
      "Epoch 800: val_accuracy did not improve from 0.87350\n",
      "1500/1500 [==============================] - 424s 283ms/step - loss: 0.1285 - accuracy: 0.9923 - val_loss: 0.7863 - val_accuracy: 0.8512\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 6000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m base_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m04S-UNIWARD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVT_prueba1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mbase_name\n\u001b[1;32m----> 3\u001b[0m _, history  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_cvt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch, model_name, num_test)\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m     14\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[0;32m     15\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[tensorboard,  checkpoint], \n\u001b[0;32m     16\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     17\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid),\n\u001b[0;32m     18\u001b[0m                     initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch)\n\u001b[1;32m---> 20\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m results_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/testing_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mnum_test\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_dir):\n",
      "File \u001b[1;32mc:\\Users\\sergioa.holguin\\.conda\\envs\\machine\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\sergioa.holguin\\.conda\\envs\\machine\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 6000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "base_name=\"04S-UNIWARD\"\n",
    "name=\"Model_\"+'CVT_prueba1'+\"_\"+base_name\n",
    "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=8, epochs=800, model_name=name, num_test='1_cvt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan_create\n"
     ]
    }
   ],
   "source": [
    "def new_arch_kan():\n",
    "  tf.keras.backend.clear_session()\n",
    "  inputs = tf.keras.Input(shape=(256,256,1), name=\"input_1\")\n",
    "  #Layer 1\n",
    "  layers_ty = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "  layers_tn = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=True, activation=Tanh3, use_bias=True)(inputs)\n",
    "\n",
    "  layers = tf.keras.layers.add([layers_ty, layers_tn])\n",
    "  layers1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "  #Layer 2\n",
    "  \n",
    "  # L1\n",
    "  layers = Block_1(layers1,64)\n",
    "\n",
    "  # L2\n",
    "  layers = Block_1(layers,64)\n",
    "\n",
    "  # L3 - L7\n",
    "  for i in range(5):\n",
    "    layers = Block_2(layers,64)\n",
    "\n",
    "  # L8 - L11\n",
    "  for i in [64, 64, 128, 256]:\n",
    "    layers = Block_3(layers,i)\n",
    "\n",
    "  # L12\n",
    "  layers = Block_4(layers,512)\n",
    "  #CVT=Transform_sh_1(layers)\n",
    "\n",
    "  representation = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(layers)\n",
    "  representation = tf.keras.layers.GlobalAvgPool2D()(representation)\n",
    "  #---------------------------------------------------Fin de Transformer 2------------------------------------------------------------------------#\n",
    "  # Classify outputs.\n",
    "      #FC\n",
    "  layers = DenseKAN(64)(representation)\n",
    "  layers = DenseKAN(32)(layers)\n",
    "  layers = DenseKAN(16)(layers)\n",
    "\n",
    "  #Softmax\n",
    "  layers = DenseKAN(2)(layers)\n",
    "  predictions = tf.keras.layers.Softmax(axis=1)(layers)\n",
    "  model =tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "\n",
    "  # Compile the model if the compile flag is set\n",
    "  if compile:\n",
    "      model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "      print(\"kan_create\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "model2 = new_arch_kan()  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.5032\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50200, saving model to D:/testing_2_kan/Model_CVkan_prueba1_04S-UNIWARD_1729988110.3461075\\saved-model.hdf5\n",
      "3000/3000 [==============================] - 466s 153ms/step - loss: 0.7960 - accuracy: 0.5032 - val_loss: 0.7640 - val_accuracy: 0.5020\n",
      "Epoch 2/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.4967\n",
      "Epoch 2: val_accuracy did not improve from 0.50200\n",
      "3000/3000 [==============================] - 444s 148ms/step - loss: 0.7195 - accuracy: 0.4967 - val_loss: 0.7247 - val_accuracy: 0.5008\n",
      "Epoch 3/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.5008\n",
      "Epoch 3: val_accuracy improved from 0.50200 to 0.50300, saving model to D:/testing_2_kan/Model_CVkan_prueba1_04S-UNIWARD_1729988110.3461075\\saved-model.hdf5\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.7190 - accuracy: 0.5008 - val_loss: 0.7159 - val_accuracy: 0.5030\n",
      "Epoch 4/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.4928\n",
      "Epoch 4: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 464s 155ms/step - loss: 0.7101 - accuracy: 0.4928 - val_loss: 0.7032 - val_accuracy: 0.5013\n",
      "Epoch 5/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7208 - accuracy: 0.5039\n",
      "Epoch 5: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 463s 154ms/step - loss: 0.7208 - accuracy: 0.5039 - val_loss: 0.7231 - val_accuracy: 0.5000\n",
      "Epoch 6/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.5063\n",
      "Epoch 6: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 464s 155ms/step - loss: 0.7248 - accuracy: 0.5063 - val_loss: 0.7353 - val_accuracy: 0.4988\n",
      "Epoch 7/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7216 - accuracy: 0.4979\n",
      "Epoch 7: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 470s 157ms/step - loss: 0.7216 - accuracy: 0.4979 - val_loss: 0.7225 - val_accuracy: 0.4978\n",
      "Epoch 8/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.4958\n",
      "Epoch 8: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 458s 153ms/step - loss: 0.7090 - accuracy: 0.4958 - val_loss: 0.7227 - val_accuracy: 0.5010\n",
      "Epoch 9/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.4971\n",
      "Epoch 9: val_accuracy did not improve from 0.50300\n",
      "3000/3000 [==============================] - 454s 151ms/step - loss: 0.7101 - accuracy: 0.4971 - val_loss: 0.7473 - val_accuracy: 0.4992\n",
      "Epoch 10/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.4992\n",
      "Epoch 10: val_accuracy improved from 0.50300 to 0.51075, saving model to D:/testing_2_kan/Model_CVkan_prueba1_04S-UNIWARD_1729988110.3461075\\saved-model.hdf5\n",
      "3000/3000 [==============================] - 479s 160ms/step - loss: 0.7017 - accuracy: 0.4992 - val_loss: 0.7386 - val_accuracy: 0.5107\n",
      "Epoch 11/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7173 - accuracy: 0.4974\n",
      "Epoch 11: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 463s 154ms/step - loss: 0.7173 - accuracy: 0.4974 - val_loss: 0.7051 - val_accuracy: 0.5005\n",
      "Epoch 12/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.4942\n",
      "Epoch 12: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 464s 155ms/step - loss: 0.7109 - accuracy: 0.4942 - val_loss: 0.7084 - val_accuracy: 0.5000\n",
      "Epoch 13/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.4936\n",
      "Epoch 13: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 463s 154ms/step - loss: 0.7191 - accuracy: 0.4936 - val_loss: 0.7575 - val_accuracy: 0.5000\n",
      "Epoch 14/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.4973\n",
      "Epoch 14: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 476s 159ms/step - loss: 0.7230 - accuracy: 0.4973 - val_loss: 0.7116 - val_accuracy: 0.5000\n",
      "Epoch 15/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7156 - accuracy: 0.4980\n",
      "Epoch 15: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 463s 154ms/step - loss: 0.7156 - accuracy: 0.4980 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 16/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7084 - accuracy: 0.4962\n",
      "Epoch 16: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 464s 155ms/step - loss: 0.7084 - accuracy: 0.4962 - val_loss: 0.6978 - val_accuracy: 0.5000\n",
      "Epoch 17/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.4997\n",
      "Epoch 17: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 464s 155ms/step - loss: 0.7255 - accuracy: 0.4997 - val_loss: 0.7210 - val_accuracy: 0.5000\n",
      "Epoch 18/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.7044 - accuracy: 0.4952\n",
      "Epoch 18: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 474s 158ms/step - loss: 0.7044 - accuracy: 0.4952 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 19/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.4923\n",
      "Epoch 19: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 467s 156ms/step - loss: 0.6995 - accuracy: 0.4923 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 20/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4980\n",
      "Epoch 20: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 465s 155ms/step - loss: 0.6943 - accuracy: 0.4980 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 21/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5020\n",
      "Epoch 21: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 453s 151ms/step - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 22/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5025\n",
      "Epoch 22: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 449s 150ms/step - loss: 0.6933 - accuracy: 0.5025 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 23/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4942\n",
      "Epoch 23: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 448s 149ms/step - loss: 0.6934 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 24/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5008\n",
      "Epoch 24: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 460s 153ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 25/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4903\n",
      "Epoch 25: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.6933 - accuracy: 0.4903 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 26/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 26: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 459s 153ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 27/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4942\n",
      "Epoch 27: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 477s 159ms/step - loss: 0.6933 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 28/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4925\n",
      "Epoch 28: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.6934 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 29/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4913\n",
      "Epoch 29: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 451s 150ms/step - loss: 0.6933 - accuracy: 0.4913 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 30/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5075\n",
      "Epoch 30: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.6931 - accuracy: 0.5075 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 31/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4918\n",
      "Epoch 31: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 452s 151ms/step - loss: 0.6934 - accuracy: 0.4918 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 32/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 32: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 459s 153ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 33/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 33: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 491s 164ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 34/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 34: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 451s 150ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 35/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4937\n",
      "Epoch 35: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.6933 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 36/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4885\n",
      "Epoch 36: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 450s 150ms/step - loss: 0.6934 - accuracy: 0.4885 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 37/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920\n",
      "Epoch 37: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 444s 148ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 38/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4902\n",
      "Epoch 38: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6934 - accuracy: 0.4902 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 39/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5003\n",
      "Epoch 39: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 40/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 40: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 41/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4918\n",
      "Epoch 41: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4918 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 42/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4883\n",
      "Epoch 42: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4883 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 43/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4943\n",
      "Epoch 43: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4943 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 44/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4997\n",
      "Epoch 44: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 45/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4903\n",
      "Epoch 45: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6934 - accuracy: 0.4903 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 46/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5022\n",
      "Epoch 46: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 47/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4932\n",
      "Epoch 47: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 144ms/step - loss: 0.6934 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 48/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5028\n",
      "Epoch 48: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6932 - accuracy: 0.5028 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 49/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4942\n",
      "Epoch 49: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6934 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 50/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4995\n",
      "Epoch 50: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 51/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4832\n",
      "Epoch 51: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6934 - accuracy: 0.4832 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 52/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4943\n",
      "Epoch 52: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4943 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 53/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5017\n",
      "Epoch 53: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 54/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5045\n",
      "Epoch 54: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 55/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4965\n",
      "Epoch 55: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 56/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4948\n",
      "Epoch 56: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 57/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 57: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 58/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4975\n",
      "Epoch 58: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 59/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4948\n",
      "Epoch 59: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 60/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4967\n",
      "Epoch 60: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 434s 145ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 61/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 61: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 62/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5002\n",
      "Epoch 62: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 63/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4965\n",
      "Epoch 63: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 64/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4998\n",
      "Epoch 64: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 65/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 65: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 66/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5067\n",
      "Epoch 66: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6931 - accuracy: 0.5067 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 67/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5030\n",
      "Epoch 67: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 68/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5017\n",
      "Epoch 68: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 69/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 69: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 70/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4913\n",
      "Epoch 70: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4913 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 71/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4975\n",
      "Epoch 71: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 72/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4973\n",
      "Epoch 72: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 73/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4917\n",
      "Epoch 73: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4917 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 74/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5027\n",
      "Epoch 74: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.5027 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 75/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5005\n",
      "Epoch 75: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 76/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4923\n",
      "Epoch 76: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4923 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 77/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4905\n",
      "Epoch 77: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 78/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4923\n",
      "Epoch 78: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4923 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 79/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4980\n",
      "Epoch 79: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 80/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5007\n",
      "Epoch 80: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 81/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5068\n",
      "Epoch 81: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6932 - accuracy: 0.5068 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 82/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5045\n",
      "Epoch 82: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 83/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 83: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 84/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4915\n",
      "Epoch 84: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6934 - accuracy: 0.4915 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 85/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4962\n",
      "Epoch 85: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4962 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 86/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 86: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 87/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5082\n",
      "Epoch 87: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.5082 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 88/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5002\n",
      "Epoch 88: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6934 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 89/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 89: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 90/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4957\n",
      "Epoch 90: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 91/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 91: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 92/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5058\n",
      "Epoch 92: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 93/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 93: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 94/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4940\n",
      "Epoch 94: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 144ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 95/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030\n",
      "Epoch 95: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 96/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 96: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 97/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4963\n",
      "Epoch 97: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 98/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4905\n",
      "Epoch 98: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 99/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4957\n",
      "Epoch 99: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 100/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 100: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 101/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5010\n",
      "Epoch 101: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 102/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4993\n",
      "Epoch 102: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 103/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4958\n",
      "Epoch 103: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 104/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 104: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 105/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5013\n",
      "Epoch 105: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 106/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4917\n",
      "Epoch 106: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4917 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 107/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 107: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 436s 145ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 108/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5003\n",
      "Epoch 108: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.5003 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 109/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5027\n",
      "Epoch 109: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 110/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5013\n",
      "Epoch 110: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 111/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4883\n",
      "Epoch 111: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6934 - accuracy: 0.4883 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 112/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5005\n",
      "Epoch 112: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 113/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 113: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 114/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5010\n",
      "Epoch 114: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 115/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5045\n",
      "Epoch 115: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 116/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 116: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 117/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4943\n",
      "Epoch 117: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4943 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 118/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 118: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 119/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4958\n",
      "Epoch 119: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 120/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4987\n",
      "Epoch 120: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 121/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 121: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 122/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4953\n",
      "Epoch 122: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4953 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 123/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5020\n",
      "Epoch 123: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 124/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 124: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 125/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 125: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 126/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 126: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 127/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 127: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 128/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 128: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 129/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 129: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 130/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4953\n",
      "Epoch 130: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 435s 145ms/step - loss: 0.6933 - accuracy: 0.4953 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 131/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 131: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 443s 148ms/step - loss: 0.6933 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 132/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 132: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 133/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4995\n",
      "Epoch 133: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4995 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 134/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4908\n",
      "Epoch 134: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 435s 145ms/step - loss: 0.6933 - accuracy: 0.4908 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 135/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 135: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 136/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4912\n",
      "Epoch 136: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4912 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 137/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4905\n",
      "Epoch 137: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 463s 154ms/step - loss: 0.6933 - accuracy: 0.4905 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 138/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4940\n",
      "Epoch 138: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 436s 145ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 139/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4937\n",
      "Epoch 139: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 140/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4878\n",
      "Epoch 140: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4878 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 141/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4922\n",
      "Epoch 141: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 502s 167ms/step - loss: 0.6933 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 142/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4968\n",
      "Epoch 142: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 493s 164ms/step - loss: 0.6933 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 143/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4905\n",
      "Epoch 143: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 459s 153ms/step - loss: 0.6933 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 144/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 144: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 436s 145ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 145/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 145: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 435s 145ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 146/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4923\n",
      "Epoch 146: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4923 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 147/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5073\n",
      "Epoch 147: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 444s 148ms/step - loss: 0.6932 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 148/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 148: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 149/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4953\n",
      "Epoch 149: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4953 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 150/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5040\n",
      "Epoch 150: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 151/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4972\n",
      "Epoch 151: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4972 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 152/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4968\n",
      "Epoch 152: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 153/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5012\n",
      "Epoch 153: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 154/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4987\n",
      "Epoch 154: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 155/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 155: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 156/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4998\n",
      "Epoch 156: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 157/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5002\n",
      "Epoch 157: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 158/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5015\n",
      "Epoch 158: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 159/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4928\n",
      "Epoch 159: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4928 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 160/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 160: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 161/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 161: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 162/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 162: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 163/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4975\n",
      "Epoch 163: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 164/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4993\n",
      "Epoch 164: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 165/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5058\n",
      "Epoch 165: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5058 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 166/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4973\n",
      "Epoch 166: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 167/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5038\n",
      "Epoch 167: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 168/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 168: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 169/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4927\n",
      "Epoch 169: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 170/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 170: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 171/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4932\n",
      "Epoch 171: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 172/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 172: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 173/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5028\n",
      "Epoch 173: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 444s 148ms/step - loss: 0.6932 - accuracy: 0.5028 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 174/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 174: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 175/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4960\n",
      "Epoch 175: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 427s 142ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 176/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4948\n",
      "Epoch 176: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 447s 149ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 177/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 177: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 447s 149ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 178/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 178: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 179/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4887\n",
      "Epoch 179: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 434s 145ms/step - loss: 0.6933 - accuracy: 0.4887 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 180/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5057\n",
      "Epoch 180: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 181/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 181: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 182/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 182: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 183/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4987\n",
      "Epoch 183: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 184/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5007\n",
      "Epoch 184: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 185/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 185: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 186/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 186: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 187/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5040\n",
      "Epoch 187: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 188/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4975\n",
      "Epoch 188: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 189/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4990\n",
      "Epoch 189: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 190/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5045\n",
      "Epoch 190: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 191/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5033\n",
      "Epoch 191: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 192/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 192: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 193/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 193: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 194/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4973\n",
      "Epoch 194: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 195/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4950\n",
      "Epoch 195: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 196/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5042\n",
      "Epoch 196: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 197/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5012\n",
      "Epoch 197: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 198/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4898\n",
      "Epoch 198: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4898 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 199/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4968\n",
      "Epoch 199: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 200/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4862\n",
      "Epoch 200: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4862 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 201/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 201: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 202/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5008\n",
      "Epoch 202: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 203/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5025\n",
      "Epoch 203: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 204/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5022\n",
      "Epoch 204: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 205/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 205: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 206/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4962\n",
      "Epoch 206: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.4962 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 207/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5010\n",
      "Epoch 207: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 208/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4950\n",
      "Epoch 208: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 209/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4907\n",
      "Epoch 209: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4907 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 210/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4933\n",
      "Epoch 210: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 211/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4985\n",
      "Epoch 211: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 212/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4888\n",
      "Epoch 212: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4888 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 213/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5015\n",
      "Epoch 213: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 214/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 214: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 215/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5010\n",
      "Epoch 215: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 216/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4890\n",
      "Epoch 216: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4890 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 217/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5015\n",
      "Epoch 217: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 218/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 218: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 219/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4973\n",
      "Epoch 219: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 220/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5027\n",
      "Epoch 220: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5027 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 221/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4948\n",
      "Epoch 221: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 222/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5063\n",
      "Epoch 222: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5063 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 223/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 223: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4975 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 224/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5083\n",
      "Epoch 224: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6931 - accuracy: 0.5083 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 225/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 225: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 226/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4950\n",
      "Epoch 226: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 227/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5042\n",
      "Epoch 227: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 228/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4935\n",
      "Epoch 228: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4935 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 229/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 229: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 230/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 230: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 231/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5017\n",
      "Epoch 231: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 232/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 232: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 233/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 233: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 234/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 234: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 235/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4990\n",
      "Epoch 235: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 236/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4998\n",
      "Epoch 236: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 237/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 237: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 238/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5015\n",
      "Epoch 238: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 239/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5002\n",
      "Epoch 239: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 240/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4925\n",
      "Epoch 240: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6934 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 241/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 241: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 242/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5063\n",
      "Epoch 242: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5063 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 243/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5032\n",
      "Epoch 243: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 244/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 244: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 245/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 245: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 147ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 246/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4900\n",
      "Epoch 246: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 247/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4937\n",
      "Epoch 247: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 248/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4995\n",
      "Epoch 248: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 249/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 249: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 250/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 250: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 251/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4963\n",
      "Epoch 251: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 252/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4943\n",
      "Epoch 252: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4943 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 253/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5013\n",
      "Epoch 253: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 254/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987\n",
      "Epoch 254: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 255/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 255: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 256/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030\n",
      "Epoch 256: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 257/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4967\n",
      "Epoch 257: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 258/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 258: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 259/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4927\n",
      "Epoch 259: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 260/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4942\n",
      "Epoch 260: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 261/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4942\n",
      "Epoch 261: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.4942 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 262/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 262: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 263/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5032\n",
      "Epoch 263: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 264/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4908\n",
      "Epoch 264: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4908 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 265/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 265: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 266/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4958\n",
      "Epoch 266: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 267/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 267: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 268/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4965\n",
      "Epoch 268: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 269/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4902\n",
      "Epoch 269: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4902 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 270/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5028\n",
      "Epoch 270: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5028 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 271/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5032\n",
      "Epoch 271: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 272/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4985\n",
      "Epoch 272: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 273/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4978\n",
      "Epoch 273: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 274/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5073\n",
      "Epoch 274: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 275/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 275: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 276/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4960\n",
      "Epoch 276: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 277/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 277: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 278/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4967\n",
      "Epoch 278: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 279/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5033\n",
      "Epoch 279: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 280/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5038\n",
      "Epoch 280: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.5038 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 281/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 281: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 282/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "Epoch 282: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 283/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030\n",
      "Epoch 283: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 284/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4967\n",
      "Epoch 284: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 285/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4958\n",
      "Epoch 285: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 286/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 286: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 287/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4968\n",
      "Epoch 287: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 147ms/step - loss: 0.6933 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 288/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4992\n",
      "Epoch 288: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 289/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4872\n",
      "Epoch 289: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4872 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 290/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4993\n",
      "Epoch 290: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 291/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4908\n",
      "Epoch 291: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4908 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 292/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5068\n",
      "Epoch 292: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 293/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5050\n",
      "Epoch 293: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6932 - accuracy: 0.5050 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 294/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5070\n",
      "Epoch 294: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6931 - accuracy: 0.5070 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 295/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5078\n",
      "Epoch 295: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6931 - accuracy: 0.5078 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 296/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4993\n",
      "Epoch 296: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 297/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 297: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 298/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4960\n",
      "Epoch 298: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 299/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5010\n",
      "Epoch 299: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 300/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 300: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 301/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 301: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 302/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4948\n",
      "Epoch 302: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 303/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4965\n",
      "Epoch 303: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 304/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4988\n",
      "Epoch 304: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 305/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4952\n",
      "Epoch 305: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 306/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 306: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 307/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4960\n",
      "Epoch 307: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 308/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4940\n",
      "Epoch 308: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 309/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4975\n",
      "Epoch 309: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 310/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5040\n",
      "Epoch 310: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 311/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4982\n",
      "Epoch 311: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 312/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4945\n",
      "Epoch 312: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4945 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 313/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4973\n",
      "Epoch 313: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 314/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4953\n",
      "Epoch 314: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4953 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 315/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5010\n",
      "Epoch 315: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 316/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4905\n",
      "Epoch 316: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 317/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5005\n",
      "Epoch 317: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 318/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4950\n",
      "Epoch 318: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 319/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 319: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 443s 148ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 320/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5015\n",
      "Epoch 320: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 321/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5037\n",
      "Epoch 321: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6932 - accuracy: 0.5037 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 322/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4990\n",
      "Epoch 322: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 323/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4988\n",
      "Epoch 323: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 324/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920\n",
      "Epoch 324: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 429s 143ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 325/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4932\n",
      "Epoch 325: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 326/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4922\n",
      "Epoch 326: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 427s 142ms/step - loss: 0.6933 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 327/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 327: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 328/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4942\n",
      "Epoch 328: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6932 - accuracy: 0.4942 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 329/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4915\n",
      "Epoch 329: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4915 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 330/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030\n",
      "Epoch 330: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 331/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4998\n",
      "Epoch 331: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 332/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4913\n",
      "Epoch 332: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4913 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 333/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920\n",
      "Epoch 333: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 334/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 334: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 335/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4967\n",
      "Epoch 335: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 442s 147ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 336/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5045\n",
      "Epoch 336: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 443s 148ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 337/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5045\n",
      "Epoch 337: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 444s 148ms/step - loss: 0.6931 - accuracy: 0.5045 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 338/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5003\n",
      "Epoch 338: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6933 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 339/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4982\n",
      "Epoch 339: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 340/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5027\n",
      "Epoch 340: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 441s 147ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 341/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4972\n",
      "Epoch 341: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 342/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4922\n",
      "Epoch 342: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 343/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 343: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 344/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4953\n",
      "Epoch 344: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4953 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 345/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4978\n",
      "Epoch 345: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 346/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4977\n",
      "Epoch 346: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 347/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4968\n",
      "Epoch 347: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 348/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4955\n",
      "Epoch 348: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 349/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4892\n",
      "Epoch 349: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4892 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 350/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5047\n",
      "Epoch 350: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5047 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 351/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4912\n",
      "Epoch 351: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 147ms/step - loss: 0.6933 - accuracy: 0.4912 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 352/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4892\n",
      "Epoch 352: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4892 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 353/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 353: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 354/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 354: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 355/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5023\n",
      "Epoch 355: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 356/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4965\n",
      "Epoch 356: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 357/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920\n",
      "Epoch 357: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 427s 142ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 358/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4950\n",
      "Epoch 358: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 359/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4927\n",
      "Epoch 359: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4927 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 360/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4935\n",
      "Epoch 360: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4935 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 361/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5032\n",
      "Epoch 361: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 144ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 362/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4898\n",
      "Epoch 362: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4898 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 363/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 363: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 364/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 364: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 424s 141ms/step - loss: 0.6933 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 365/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 365: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 366/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4972\n",
      "Epoch 366: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6933 - accuracy: 0.4972 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 367/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4940\n",
      "Epoch 367: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 368/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4927\n",
      "Epoch 368: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 369/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 369: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 370/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4928\n",
      "Epoch 370: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 422s 141ms/step - loss: 0.6933 - accuracy: 0.4928 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 371/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4998\n",
      "Epoch 371: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 423s 141ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 372/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5007\n",
      "Epoch 372: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 373/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 373: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 426s 142ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 374/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4995\n",
      "Epoch 374: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4995 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 375/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 375: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 427s 142ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 376/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4908\n",
      "Epoch 376: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4908 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 377/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4977\n",
      "Epoch 377: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 422s 141ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 378/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5013\n",
      "Epoch 378: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 379/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 379: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 380/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4980\n",
      "Epoch 380: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 381/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4938\n",
      "Epoch 381: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 428s 143ms/step - loss: 0.6933 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 382/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4933\n",
      "Epoch 382: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 425s 142ms/step - loss: 0.6933 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 383/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4995\n",
      "Epoch 383: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 430s 143ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 384/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 384: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 431s 144ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 385/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5013\n",
      "Epoch 385: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 386/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968\n",
      "Epoch 386: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 435s 145ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 387/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4940\n",
      "Epoch 387: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 388/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 388: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 433s 144ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 389/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4922\n",
      "Epoch 389: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 438s 146ms/step - loss: 0.6933 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 390/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5050\n",
      "Epoch 390: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5050 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 391/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4997\n",
      "Epoch 391: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 392/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4922\n",
      "Epoch 392: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 393/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4990\n",
      "Epoch 393: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 394/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4917\n",
      "Epoch 394: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 440s 147ms/step - loss: 0.6933 - accuracy: 0.4917 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 395/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5013\n",
      "Epoch 395: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 396/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5017\n",
      "Epoch 396: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 397/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5020\n",
      "Epoch 397: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 432s 144ms/step - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 398/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5033\n",
      "Epoch 398: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 437s 146ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 399/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4945\n",
      "Epoch 399: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6933 - accuracy: 0.4945 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 400/400\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 400: val_accuracy did not improve from 0.51075\n",
      "3000/3000 [==============================] - 439s 146ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Model_CVkan_prueba1_04S-UNIWARD = 175502.6486272812 [seconds]\n"
     ]
    }
   ],
   "source": [
    "base_name=\"04S-UNIWARD\"\n",
    "name=\"Model_\"+'CVkan_prueba1'+\"_\"+base_name\n",
    "_, history  = train(model2, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=4, epochs=400, model_name=name, num_test='2_kan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
